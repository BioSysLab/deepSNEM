{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import Set2Set\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from deepSNEM.models.graph_transformer.euclidean_graph_transformer import GraphTransformerEncoder\n",
    "from models.deep_graph_infomax.infomax import SNInfomax\n",
    "from utils.data_gen import load_prot_embs, load_prot_embs_go, ucsv2graph_infomax, SNDatasetInfomax\n",
    "\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from captum.attr import IntegratedGradients, DeepLift, LayerIntegratedGradients, Saliency\n",
    "from captum.attr import configure_interpretable_embedding_layer,remove_interpretable_embedding_layer\n",
    "\n",
    "dev = torch.device('cuda')\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_embs, global_dict = load_prot_embs_go(512, norm=False)\n",
    "\n",
    "def load_infomax_model(emb_type = 'seqveq'):\n",
    "    if emb_type=='GO':\n",
    "        trained_model_path = 'embeddings/deep_graph_infomax/unsupervised/DGI_JSD_512_GO_uniform_un.pt'\n",
    "    elif emb_type=='seqveq':\n",
    "        trained_model_path = 'embeddings/deep_graph_infomax/unsupervised/DGI_JSD_512_seq_uniform_un.pt'\n",
    "    elif emb_type=='random':\n",
    "        trained_model_path = 'embeddings/deep_graph_infomax/unsupervised/DGI_JSD_512_random_uniform_un.pt'\n",
    "    else:\n",
    "        raise AttributeError\n",
    "\n",
    "    summarizer = summarizer = Set2Set(512, 3)\n",
    "    encoder = GraphTransformerEncoder(n_layers=1, n_heads=4, n_hid=512, pretrained_weights=prot_embs, \n",
    "                                    summarizer=None).to(dev)\n",
    "    model = SNInfomax(hidden_channels=512, encoder=encoder,\n",
    "                                        summary=summarizer, semi=True).to(dev)\n",
    "    model.load_state_dict(torch.load(trained_model_path))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def drug_group(drug_name ,smoothing=0.1):\n",
    "    full_dataset = pd.read_csv('data/graph_info_df/full_dataset.csv')\n",
    "    full_dataset = full_dataset[full_dataset.pert_iname == drug_name]\n",
    "    #full_dataset = full_dataset[full_dataset.moa_v1 != 'Unknown']\n",
    "    \n",
    "    u_path_list = full_dataset.files_combined.values\n",
    "    all_moas = full_dataset.moa_v1.values\n",
    "    moa_v1 = full_dataset.moa_v1.values\n",
    "    labels = full_dataset.sigs_g.to_numpy().reshape(-1,1)\n",
    "\n",
    "    oh = OneHotEncoder()\n",
    "    labels = oh.fit_transform(labels).toarray()\n",
    "    dataset = SNDatasetInfomax(u_path_list, global_dict, labels)\n",
    "    return full_dataset, dataset\n",
    "\n",
    "def moa_group(moa):\n",
    "    df = pd.read_csv('data/graph_info_df/kris.csv')\n",
    "    full_dataset = df[df.moa_v1 == moa].groupby('sig_id').apply(lambda df: df.sample(1))\n",
    "    \n",
    "    u_path_list = full_dataset.files_combined.values\n",
    "    all_moas = full_dataset.moa_v1.values\n",
    "    moa_v1 = full_dataset.moa_v1.values\n",
    "\n",
    "    dataset = SNDatasetInfomax(u_path_list, global_dict)\n",
    "    return full_dataset, dataset\n",
    "\n",
    "def custom_forward(x, data):\n",
    "    act = model.encoder.pe(data)\n",
    "    x = torch.add(x, act)\n",
    "    for t in model.encoder.transformers:\n",
    "        x = t(x, data)\n",
    "    \n",
    "    summary = model.summary(x, data.batch)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def cni_all_models(model, dataset, topk=5):\n",
    "    sal = Saliency(custom_forward)\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=1, num_workers=12, shuffle=False)\n",
    "        \n",
    "    important_nodes = []\n",
    "    attribs = []\n",
    "    \n",
    "    for tb in tqdm(data_loader):\n",
    "        tb = tb.to(dev)\n",
    "        for tar in range(1024):\n",
    "            model.zero_grad()\n",
    "            x = model.encoder.emb_layer(tb.global_idx)\n",
    "            attributions = sal.attribute(x, additional_forward_args=(tb), target=tar, abs=True)\n",
    "            attributions_a = attributions.sum(1).cpu().sort(descending=True)[0].numpy().reshape(-1,1)\n",
    "            \n",
    "            minmax = MinMaxScaler()\n",
    "            attributions_a = minmax.fit_transform(attributions_a)\n",
    "            attribs.append(attributions_a.reshape(-1))\n",
    "            \n",
    "            att_keys = attributions.sum(1).cpu().sort(descending=True)[1]\n",
    "            important_nodes.append(tb.global_idx[att_keys].cpu().numpy())\n",
    "            \n",
    "    return np.array(important_nodes), np.array(attribs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "full, dataset = moa_group('HSP inhibitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_infomax_model('seqveq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_nodes, all_attrs = cni_all_models(model, dataset, topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_global_dict = {v:k for k,v in global_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d4225b0310d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_attrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_nodes' is not defined"
     ]
    }
   ],
   "source": [
    "all_nodes = np.concatenate(all_nodes).ravel()\n",
    "all_attrs = np.concatenate(all_attrs).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = np.array([reverse_global_dict[key] for key in all_nodes.reshape(-1)])\n",
    "attribs = all_attrs.reshape(-1)\n",
    "df = pd.DataFrame({'N':nodes,'A':attribs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = df.groupby('N').sum().reset_index().sort_values(by='A', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.merge(df, df_filt, on='N')\n",
    "order = df1.groupby('N').mean().sort_values(by='A_x', ascending=False).reset_index().N.values\n",
    "df2 = df1.groupby('N').mean().sort_values(by='A_x', ascending=False).reset_index().drop('A_y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_hek = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([seqveq_hek, go_hek, random_hek], axis=1)\n",
    "df3.columns = ['N_s', 'A_s', 'N_G', 'A_G', 'N_r', 'A_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_new = df3.iloc[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vis Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../snac_data/' + df_atp.files_combined.values[5])\n",
    "G = nx.from_pandas_edgelist(sample, source='node1', target='node2', \n",
    "                            edge_attr=['sign'], create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.nan_to_num(np.array(colors, dtype=np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('torch': conda)",
   "language": "python",
   "name": "python37164bittorchconda400f3b5524f54409b045df0fcc1fa418"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
