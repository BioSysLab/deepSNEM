{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import functools\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, roc_auc_score, \\\n",
    "    auc, average_precision_score, pairwise_distances\n",
    "import scikitplot as skplt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from functools import partial\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read gene features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = pd.read_csv(\"data/gene_data/gene_features_all.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the training,val,test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"data/gene_data/allsigs.csv\",index_col=0)\n",
    "val1_df = pd.read_csv(\"data/gene_data/splits/csv/val_set_1.csv\",index_col=0)\n",
    "val2_df = pd.read_csv(\"data/gene_data/splits/csv/val_set_2.csv\",index_col=0)\n",
    "val3_df = pd.read_csv(\"data/gene_data/splits/csv/val_set_3.csv\",index_col=0)\n",
    "val4_df = pd.read_csv(\"data/gene_data/splits/csv/val_set_4.csv\",index_col=0)\n",
    "test_df = pd.read_csv(\"data/gene_data/splits/csv/test_set.csv\",index_col=0)\n",
    "\n",
    "valsets = [val1_df,val2_df,val3_df,val4_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only keep labels with more than 3 examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df[all_df[\"moa_count\"]>3]\n",
    "genes = genes.loc[all_df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(all_df['moa_v1'])\n",
    "#encoded_Y = encoder.transform(all_df['moa_v1'])\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "#dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first remove the test set from all_df and all genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genes = genes.loc[test_df[\"sig_id\"]]\n",
    "test_sigs = all_df.loc[test_df[\"sig_id\"]]\n",
    "all_df = all_df.drop(test_df[\"sig_id\"])\n",
    "genes = genes.drop(test_df[\"sig_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the space\n",
    "fspace = {\n",
    "    'dense_size_1' : hp.quniform('dense_size_1', 512,1024,64), #+\n",
    "    'dense_size_2' : hp.quniform('dense_size_2', 256,512,64), #+\n",
    "    'dense_size_3' : hp.quniform('dense_size_3', 128,256,64), #+\n",
    "    'lr' : hp.uniform('lr', 0.0005, 0.01),\n",
    "    'dropout' : hp.uniform('dropout',0.1,0.5),\n",
    "    'epochs' : hp.quniform('epochs',5,50,5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Dropout, Input, Lambda\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import History, ReduceLROnPlateau,EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(fspace, all_df, genes, valsets, label_encoder):\n",
    "    accs = []\n",
    "    model_params = {\n",
    "        'dense_size' : [int(fspace['dense_size_1']),int(fspace['dense_size_2']),int(fspace['dense_size_3'])],\n",
    "        'dropout' : [fspace['dropout'],fspace['dropout'],fspace['dropout']],\n",
    "        'lr' : fspace['lr']\n",
    "    }\n",
    "    for i in range(len(valsets)):\n",
    "        #split val and train\n",
    "        val_genes = genes.loc[valsets[i][\"sig_id\"]]\n",
    "        val_sigs = all_df.loc[valsets[i][\"sig_id\"]]\n",
    "        train_genes = genes.drop(valsets[i][\"sig_id\"])\n",
    "        train_sigs = all_df.drop(valsets[i][\"sig_id\"])\n",
    "        #one hot encode labels\n",
    "        encoded_Y_train = label_encoder.transform(train_sigs['moa_v1'])\n",
    "        encoded_Y_val = label_encoder.transform(val_sigs['moa_v1'])\n",
    "        train_y = np_utils.to_categorical(encoded_Y_train)\n",
    "        val_y = np_utils.to_categorical(encoded_Y_val)\n",
    "        # define the mlp model\n",
    "        \n",
    "        gene_input = Input(name = 'gene_input',shape = (978,), dtype = 'float32')\n",
    "\n",
    "        fc1 = Dense(model_params['dense_size'][0], activation = 'relu', kernel_initializer='random_normal')(gene_input)\n",
    "        fc1 = Dropout(model_params['dropout'][0])(fc1)\n",
    "        fc2 = Dense(model_params['dense_size'][1], activation = 'relu', kernel_initializer='random_normal')(fc1)\n",
    "        fc2 = Dropout(model_params['dropout'][1])(fc2)\n",
    "        fc3 = Dense(model_params['dense_size'][2], activation = 'relu', kernel_initializer='random_normal')(fc2)\n",
    "        fc3 = Dropout(model_params['dropout'][2])(fc3)\n",
    "\n",
    "        prediction = Dense(all_df['moa_v1'].nunique(), activation = 'softmax')(fc3)\n",
    "        adam = keras.optimizers.Adam(lr=model_params[\"lr\"], beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False)\n",
    "        mlp_model = Model(inputs = gene_input, outputs = prediction)\n",
    "        mlp_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "        \n",
    "        mlp_model.fit(np.array(train_genes),train_y, batch_size = 64, \n",
    "                      epochs = int(fspace['epochs']), \n",
    "                      verbose = 0, shuffle = True, validation_data = None)\n",
    "       \n",
    "        pred = mlp_model.predict(np.array(val_genes))\n",
    "        \n",
    "        accs.append(accuracy_score(np.argmax(val_y,axis=1), np.argmax(pred,axis=1)))\n",
    "    ave_acc = np.mean(accs,axis = 0)\n",
    "    return {'loss': -ave_acc ,  'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective, all_df = all_df, genes = genes, valsets = valsets, label_encoder = label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 100  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"MLP_genes.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"MLP_genes.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 1 trials to 101 (+100) trials\n",
      "100%|██████████████████████████████████████████| 101/101 [4:42:00<00:00, 167.53s/trial, best loss: -0.6265860190229938]\n",
      "Best: {'dense_size_1': 576.0, 'dense_size_2': 256.0, 'dense_size_3': 128.0, 'dropout': 0.2733924105790248, 'epochs': 25.0, 'lr': 0.000981699073272445}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "        'dense_size' : [int(896),int(384),int(128)],\n",
    "        'dropout' : [0.45,0.45,0.45],\n",
    "        'lr' : 0.00127\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2087/2087 [==============================] - 1s 554us/step - loss: 5.4621 - acc: 0.0738\n",
      "Epoch 2/100\n",
      "2087/2087 [==============================] - 0s 163us/step - loss: 4.3158 - acc: 0.1433\n",
      "Epoch 3/100\n",
      "2087/2087 [==============================] - 0s 187us/step - loss: 4.0477 - acc: 0.1773\n",
      "Epoch 4/100\n",
      "2087/2087 [==============================] - 0s 169us/step - loss: 3.8741 - acc: 0.2080\n",
      "Epoch 5/100\n",
      "2087/2087 [==============================] - 0s 174us/step - loss: 3.6897 - acc: 0.2362\n",
      "Epoch 6/100\n",
      "2087/2087 [==============================] - 0s 193us/step - loss: 3.5622 - acc: 0.2487\n",
      "Epoch 7/100\n",
      "2087/2087 [==============================] - 0s 192us/step - loss: 3.3846 - acc: 0.2750\n",
      "Epoch 8/100\n",
      "2087/2087 [==============================] - 0s 170us/step - loss: 3.2615 - acc: 0.2899\n",
      "Epoch 9/100\n",
      "2087/2087 [==============================] - 0s 164us/step - loss: 3.1259 - acc: 0.3138\n",
      "Epoch 10/100\n",
      "2087/2087 [==============================] - 0s 185us/step - loss: 2.9534 - acc: 0.3340\n",
      "Epoch 11/100\n",
      "2087/2087 [==============================] - 0s 175us/step - loss: 2.8576 - acc: 0.3517\n",
      "Epoch 12/100\n",
      "2087/2087 [==============================] - 0s 170us/step - loss: 2.7491 - acc: 0.3819\n",
      "Epoch 13/100\n",
      "2087/2087 [==============================] - 0s 179us/step - loss: 2.6795 - acc: 0.3900\n",
      "Epoch 14/100\n",
      "2087/2087 [==============================] - 0s 179us/step - loss: 2.5967 - acc: 0.4097\n",
      "Epoch 15/100\n",
      "2087/2087 [==============================] - 0s 182us/step - loss: 2.4650 - acc: 0.4312\n",
      "Epoch 16/100\n",
      "2087/2087 [==============================] - 0s 165us/step - loss: 2.4057 - acc: 0.4327\n",
      "Epoch 17/100\n",
      "2087/2087 [==============================] - 0s 162us/step - loss: 2.3234 - acc: 0.4533\n",
      "Epoch 18/100\n",
      "2087/2087 [==============================] - 0s 192us/step - loss: 2.2794 - acc: 0.4638\n",
      "Epoch 19/100\n",
      "2087/2087 [==============================] - 0s 182us/step - loss: 2.1853 - acc: 0.4753\n",
      "Epoch 20/100\n",
      "2087/2087 [==============================] - 0s 169us/step - loss: 2.1367 - acc: 0.4748\n",
      "Epoch 21/100\n",
      "2087/2087 [==============================] - 0s 191us/step - loss: 2.0392 - acc: 0.4998\n",
      "Epoch 22/100\n",
      "2087/2087 [==============================] - 0s 183us/step - loss: 2.0004 - acc: 0.5117\n",
      "Epoch 23/100\n",
      "2087/2087 [==============================] - 0s 173us/step - loss: 1.9511 - acc: 0.5079\n",
      "Epoch 24/100\n",
      "2087/2087 [==============================] - 0s 173us/step - loss: 1.8677 - acc: 0.5314\n",
      "Epoch 25/100\n",
      "2087/2087 [==============================] - 0s 168us/step - loss: 1.8628 - acc: 0.5410\n",
      "Epoch 26/100\n",
      "2087/2087 [==============================] - 0s 178us/step - loss: 1.8020 - acc: 0.5577\n",
      "Epoch 27/100\n",
      "2087/2087 [==============================] - 0s 188us/step - loss: 1.7659 - acc: 0.5649\n",
      "Epoch 28/100\n",
      "2087/2087 [==============================] - 0s 171us/step - loss: 1.6759 - acc: 0.5851\n",
      "Epoch 29/100\n",
      "2087/2087 [==============================] - 0s 175us/step - loss: 1.7142 - acc: 0.5678\n",
      "Epoch 30/100\n",
      "2087/2087 [==============================] - 0s 191us/step - loss: 1.5796 - acc: 0.6013\n",
      "Epoch 31/100\n",
      "2087/2087 [==============================] - 0s 171us/step - loss: 1.5577 - acc: 0.6066\n",
      "Epoch 32/100\n",
      "2087/2087 [==============================] - 0s 183us/step - loss: 1.5367 - acc: 0.6176\n",
      "Epoch 33/100\n",
      "2087/2087 [==============================] - 0s 156us/step - loss: 1.4417 - acc: 0.6301\n",
      "Epoch 34/100\n",
      "2087/2087 [==============================] - 0s 154us/step - loss: 1.4450 - acc: 0.6239\n",
      "Epoch 35/100\n",
      "2087/2087 [==============================] - 0s 159us/step - loss: 1.4482 - acc: 0.6272\n",
      "Epoch 36/100\n",
      "2087/2087 [==============================] - 0s 162us/step - loss: 1.3960 - acc: 0.6464\n",
      "Epoch 37/100\n",
      "2087/2087 [==============================] - 0s 159us/step - loss: 1.3860 - acc: 0.6411\n",
      "Epoch 38/100\n",
      "2087/2087 [==============================] - 0s 145us/step - loss: 1.3207 - acc: 0.6584\n",
      "Epoch 39/100\n",
      "2087/2087 [==============================] - 0s 167us/step - loss: 1.3079 - acc: 0.6747\n",
      "Epoch 40/100\n",
      "2087/2087 [==============================] - 0s 136us/step - loss: 1.2379 - acc: 0.6847\n",
      "Epoch 41/100\n",
      "2087/2087 [==============================] - 0s 147us/step - loss: 1.1951 - acc: 0.6818\n",
      "Epoch 42/100\n",
      "2087/2087 [==============================] - 0s 147us/step - loss: 1.1907 - acc: 0.6943\n",
      "Epoch 43/100\n",
      "2087/2087 [==============================] - 0s 154us/step - loss: 1.1927 - acc: 0.6895\n",
      "Epoch 44/100\n",
      "2087/2087 [==============================] - 0s 161us/step - loss: 1.1430 - acc: 0.6991\n",
      "Epoch 45/100\n",
      "2087/2087 [==============================] - 0s 163us/step - loss: 1.1092 - acc: 0.7087\n",
      "Epoch 46/100\n",
      "2087/2087 [==============================] - 0s 141us/step - loss: 1.1150 - acc: 0.7063\n",
      "Epoch 47/100\n",
      "2087/2087 [==============================] - 0s 165us/step - loss: 1.0564 - acc: 0.7187\n",
      "Epoch 48/100\n",
      "2087/2087 [==============================] - 0s 156us/step - loss: 1.0711 - acc: 0.7245\n",
      "Epoch 49/100\n",
      "2087/2087 [==============================] - 0s 137us/step - loss: 1.0376 - acc: 0.7269\n",
      "Epoch 50/100\n",
      "2087/2087 [==============================] - 0s 150us/step - loss: 1.0453 - acc: 0.7264\n",
      "Epoch 51/100\n",
      "2087/2087 [==============================] - 0s 150us/step - loss: 1.0720 - acc: 0.7278\n",
      "Epoch 52/100\n",
      "2087/2087 [==============================] - 0s 150us/step - loss: 1.0204 - acc: 0.7355\n",
      "Epoch 53/100\n",
      "2087/2087 [==============================] - 0s 153us/step - loss: 0.9973 - acc: 0.7384\n",
      "Epoch 54/100\n",
      "2087/2087 [==============================] - 0s 154us/step - loss: 0.9731 - acc: 0.7523\n",
      "Epoch 55/100\n",
      "2087/2087 [==============================] - 0s 159us/step - loss: 0.9337 - acc: 0.7456\n",
      "Epoch 56/100\n",
      "2087/2087 [==============================] - 0s 149us/step - loss: 0.9022 - acc: 0.7417\n",
      "Epoch 57/100\n",
      "2087/2087 [==============================] - 0s 162us/step - loss: 0.9621 - acc: 0.7417\n",
      "Epoch 58/100\n",
      "2087/2087 [==============================] - 0s 151us/step - loss: 0.9455 - acc: 0.7494\n",
      "Epoch 59/100\n",
      "2087/2087 [==============================] - 0s 148us/step - loss: 0.9178 - acc: 0.7561\n",
      "Epoch 60/100\n",
      "2087/2087 [==============================] - 0s 156us/step - loss: 0.8687 - acc: 0.7762\n",
      "Epoch 61/100\n",
      "2087/2087 [==============================] - 0s 133us/step - loss: 0.8482 - acc: 0.7671\n",
      "Epoch 62/100\n",
      "2087/2087 [==============================] - 0s 143us/step - loss: 0.9004 - acc: 0.7628\n",
      "Epoch 63/100\n",
      "2087/2087 [==============================] - 0s 154us/step - loss: 0.8744 - acc: 0.7619\n",
      "Epoch 64/100\n",
      "2087/2087 [==============================] - 0s 153us/step - loss: 0.8499 - acc: 0.7901\n",
      "Epoch 65/100\n",
      "2087/2087 [==============================] - 0s 142us/step - loss: 0.7912 - acc: 0.7983\n",
      "Epoch 66/100\n",
      "2087/2087 [==============================] - 0s 155us/step - loss: 0.7972 - acc: 0.7887\n",
      "Epoch 67/100\n",
      "2087/2087 [==============================] - 0s 169us/step - loss: 0.8336 - acc: 0.7858\n",
      "Epoch 68/100\n",
      "2087/2087 [==============================] - 0s 144us/step - loss: 0.7391 - acc: 0.8059\n",
      "Epoch 69/100\n",
      "2087/2087 [==============================] - ETA: 0s - loss: 0.7890 - acc: 0.792 - 0s 153us/step - loss: 0.7966 - acc: 0.7916\n",
      "Epoch 70/100\n",
      "2087/2087 [==============================] - 0s 152us/step - loss: 0.7616 - acc: 0.7968\n",
      "Epoch 71/100\n",
      "2087/2087 [==============================] - 0s 151us/step - loss: 0.7737 - acc: 0.8035\n",
      "Epoch 72/100\n",
      "2087/2087 [==============================] - 0s 142us/step - loss: 0.7024 - acc: 0.8150\n",
      "Epoch 73/100\n",
      "2087/2087 [==============================] - 0s 159us/step - loss: 0.7039 - acc: 0.8007\n",
      "Epoch 74/100\n",
      "2087/2087 [==============================] - 0s 173us/step - loss: 0.7174 - acc: 0.8088\n",
      "Epoch 75/100\n",
      "2087/2087 [==============================] - 0s 149us/step - loss: 0.7368 - acc: 0.8069\n",
      "Epoch 76/100\n",
      "2087/2087 [==============================] - 0s 153us/step - loss: 0.7189 - acc: 0.8146\n",
      "Epoch 77/100\n",
      "2087/2087 [==============================] - 0s 155us/step - loss: 0.6631 - acc: 0.8213\n",
      "Epoch 78/100\n",
      "2087/2087 [==============================] - 0s 151us/step - loss: 0.6639 - acc: 0.8256\n",
      "Epoch 79/100\n",
      "2087/2087 [==============================] - 0s 163us/step - loss: 0.6991 - acc: 0.8194\n",
      "Epoch 80/100\n",
      "2087/2087 [==============================] - 0s 155us/step - loss: 0.7075 - acc: 0.8146\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2087/2087 [==============================] - 0s 146us/step - loss: 0.7359 - acc: 0.8232\n",
      "Epoch 82/100\n",
      "2087/2087 [==============================] - 0s 152us/step - loss: 0.7281 - acc: 0.8088\n",
      "Epoch 83/100\n",
      "2087/2087 [==============================] - 0s 169us/step - loss: 0.6517 - acc: 0.8347\n",
      "Epoch 84/100\n",
      "2087/2087 [==============================] - 0s 143us/step - loss: 0.5804 - acc: 0.8380\n",
      "Epoch 85/100\n",
      "2087/2087 [==============================] - 0s 158us/step - loss: 0.6912 - acc: 0.8227\n",
      "Epoch 86/100\n",
      "2087/2087 [==============================] - 0s 172us/step - loss: 0.6313 - acc: 0.8380\n",
      "Epoch 87/100\n",
      "2087/2087 [==============================] - 0s 159us/step - loss: 0.6370 - acc: 0.8352\n",
      "Epoch 88/100\n",
      "2087/2087 [==============================] - 0s 167us/step - loss: 0.6162 - acc: 0.8347\n",
      "Epoch 89/100\n",
      "2087/2087 [==============================] - 0s 172us/step - loss: 0.6740 - acc: 0.8376\n",
      "Epoch 90/100\n",
      "2087/2087 [==============================] - 0s 176us/step - loss: 0.6237 - acc: 0.8366\n",
      "Epoch 91/100\n",
      "2087/2087 [==============================] - 0s 178us/step - loss: 0.6185 - acc: 0.8433\n",
      "Epoch 92/100\n",
      "2087/2087 [==============================] - 0s 160us/step - loss: 0.5747 - acc: 0.8529\n",
      "Epoch 93/100\n",
      "2087/2087 [==============================] - 0s 184us/step - loss: 0.5575 - acc: 0.8625\n",
      "Epoch 94/100\n",
      "2087/2087 [==============================] - 0s 178us/step - loss: 0.5521 - acc: 0.8563\n",
      "Epoch 95/100\n",
      "2087/2087 [==============================] - 0s 177us/step - loss: 0.5245 - acc: 0.8610\n",
      "Epoch 96/100\n",
      "2087/2087 [==============================] - 0s 177us/step - loss: 0.5213 - acc: 0.8596\n",
      "Epoch 97/100\n",
      "2087/2087 [==============================] - 0s 172us/step - loss: 0.5346 - acc: 0.8678\n",
      "Epoch 98/100\n",
      "2087/2087 [==============================] - 0s 161us/step - loss: 0.5282 - acc: 0.8606\n",
      "Epoch 99/100\n",
      "2087/2087 [==============================] - 0s 179us/step - loss: 0.5439 - acc: 0.8558\n",
      "Epoch 100/100\n",
      "2087/2087 [==============================] - 0s 178us/step - loss: 0.6211 - acc: 0.8452\n",
      "Epoch 1/100\n",
      "2141/2141 [==============================] - 1s 570us/step - loss: 5.1823 - acc: 0.0850\n",
      "Epoch 2/100\n",
      "2141/2141 [==============================] - 0s 158us/step - loss: 4.2260 - acc: 0.1504\n",
      "Epoch 3/100\n",
      "2141/2141 [==============================] - 0s 184us/step - loss: 3.9938 - acc: 0.1924\n",
      "Epoch 4/100\n",
      "2141/2141 [==============================] - 0s 182us/step - loss: 3.7149 - acc: 0.2251\n",
      "Epoch 5/100\n",
      "2141/2141 [==============================] - 0s 169us/step - loss: 3.5549 - acc: 0.2499\n",
      "Epoch 6/100\n",
      "2141/2141 [==============================] - 0s 186us/step - loss: 3.3951 - acc: 0.2723\n",
      "Epoch 7/100\n",
      "2141/2141 [==============================] - 0s 175us/step - loss: 3.3169 - acc: 0.2840\n",
      "Epoch 8/100\n",
      "2141/2141 [==============================] - 0s 176us/step - loss: 3.1720 - acc: 0.2994\n",
      "Epoch 9/100\n",
      "2141/2141 [==============================] - 0s 162us/step - loss: 3.0807 - acc: 0.3255\n",
      "Epoch 10/100\n",
      "2141/2141 [==============================] - 0s 155us/step - loss: 2.9239 - acc: 0.3494\n",
      "Epoch 11/100\n",
      "2141/2141 [==============================] - 0s 179us/step - loss: 2.8560 - acc: 0.3545\n",
      "Epoch 12/100\n",
      "2141/2141 [==============================] - 0s 182us/step - loss: 2.6911 - acc: 0.3881\n",
      "Epoch 13/100\n",
      "2141/2141 [==============================] - 0s 170us/step - loss: 2.6528 - acc: 0.3956\n",
      "Epoch 14/100\n",
      "2141/2141 [==============================] - 0s 170us/step - loss: 2.5772 - acc: 0.4031\n",
      "Epoch 15/100\n",
      "2141/2141 [==============================] - 0s 175us/step - loss: 2.4901 - acc: 0.4194\n",
      "Epoch 16/100\n",
      "2141/2141 [==============================] - 0s 176us/step - loss: 2.3992 - acc: 0.4423\n",
      "Epoch 17/100\n",
      "2141/2141 [==============================] - 0s 179us/step - loss: 2.2654 - acc: 0.4643\n",
      "Epoch 18/100\n",
      "2141/2141 [==============================] - 0s 180us/step - loss: 2.2323 - acc: 0.4736\n",
      "Epoch 19/100\n",
      "2141/2141 [==============================] - 0s 176us/step - loss: 2.1888 - acc: 0.4741\n",
      "Epoch 20/100\n",
      "2141/2141 [==============================] - 0s 180us/step - loss: 2.1419 - acc: 0.4806\n",
      "Epoch 21/100\n",
      "2141/2141 [==============================] - 0s 183us/step - loss: 2.0783 - acc: 0.4984\n",
      "Epoch 22/100\n",
      "2141/2141 [==============================] - 0s 170us/step - loss: 1.9944 - acc: 0.5100\n",
      "Epoch 23/100\n",
      "2141/2141 [==============================] - 0s 186us/step - loss: 1.9964 - acc: 0.5100\n",
      "Epoch 24/100\n",
      "2141/2141 [==============================] - 0s 186us/step - loss: 1.8489 - acc: 0.5516\n",
      "Epoch 25/100\n",
      "2141/2141 [==============================] - 0s 177us/step - loss: 1.8372 - acc: 0.5605\n",
      "Epoch 26/100\n",
      "2141/2141 [==============================] - 0s 175us/step - loss: 1.7485 - acc: 0.5619\n",
      "Epoch 27/100\n",
      "2141/2141 [==============================] - 0s 181us/step - loss: 1.7337 - acc: 0.5726\n",
      "Epoch 28/100\n",
      "2141/2141 [==============================] - 0s 170us/step - loss: 1.7103 - acc: 0.5806\n",
      "Epoch 29/100\n",
      "2141/2141 [==============================] - 0s 135us/step - loss: 1.7049 - acc: 0.5834\n",
      "Epoch 30/100\n",
      "2141/2141 [==============================] - 0s 182us/step - loss: 1.6340 - acc: 0.5969\n",
      "Epoch 31/100\n",
      "2141/2141 [==============================] - 0s 170us/step - loss: 1.6039 - acc: 0.5965\n",
      "Epoch 32/100\n",
      "2141/2141 [==============================] - 0s 196us/step - loss: 1.5827 - acc: 0.6016\n",
      "Epoch 33/100\n",
      "2141/2141 [==============================] - 0s 187us/step - loss: 1.5316 - acc: 0.6189\n",
      "Epoch 34/100\n",
      "2141/2141 [==============================] - 0s 167us/step - loss: 1.5584 - acc: 0.6133\n",
      "Epoch 35/100\n",
      "2141/2141 [==============================] - 0s 198us/step - loss: 1.4875 - acc: 0.6291\n",
      "Epoch 36/100\n",
      "2141/2141 [==============================] - 0s 172us/step - loss: 1.4606 - acc: 0.6273\n",
      "Epoch 37/100\n",
      "2141/2141 [==============================] - 0s 162us/step - loss: 1.3978 - acc: 0.6464\n",
      "Epoch 38/100\n",
      "2141/2141 [==============================] - 0s 182us/step - loss: 1.3936 - acc: 0.6404\n",
      "Epoch 39/100\n",
      "2141/2141 [==============================] - 0s 148us/step - loss: 1.3585 - acc: 0.6534\n",
      "Epoch 40/100\n",
      "2141/2141 [==============================] - 0s 188us/step - loss: 1.3664 - acc: 0.6464\n",
      "Epoch 41/100\n",
      "2141/2141 [==============================] - 0s 184us/step - loss: 1.3818 - acc: 0.6623\n",
      "Epoch 42/100\n",
      "2141/2141 [==============================] - 0s 147us/step - loss: 1.3054 - acc: 0.6684\n",
      "Epoch 43/100\n",
      "2141/2141 [==============================] - 0s 185us/step - loss: 1.3144 - acc: 0.6581\n",
      "Epoch 44/100\n",
      "2141/2141 [==============================] - 0s 193us/step - loss: 1.3212 - acc: 0.6595\n",
      "Epoch 45/100\n",
      "2141/2141 [==============================] - 0s 154us/step - loss: 1.1695 - acc: 0.6978\n",
      "Epoch 46/100\n",
      "2141/2141 [==============================] - 0s 186us/step - loss: 1.1956 - acc: 0.6852\n",
      "Epoch 47/100\n",
      "2141/2141 [==============================] - 0s 187us/step - loss: 1.1445 - acc: 0.6997\n",
      "Epoch 48/100\n",
      "2141/2141 [==============================] - 0s 175us/step - loss: 1.1455 - acc: 0.6992\n",
      "Epoch 49/100\n",
      "2141/2141 [==============================] - 0s 188us/step - loss: 1.1290 - acc: 0.7090\n",
      "Epoch 50/100\n",
      "2141/2141 [==============================] - 0s 174us/step - loss: 1.1290 - acc: 0.7170\n",
      "Epoch 51/100\n",
      "2141/2141 [==============================] - 0s 183us/step - loss: 1.0683 - acc: 0.7221\n",
      "Epoch 52/100\n",
      "2141/2141 [==============================] - 0s 183us/step - loss: 1.0971 - acc: 0.7188\n",
      "Epoch 53/100\n",
      "2141/2141 [==============================] - 0s 180us/step - loss: 1.0678 - acc: 0.7249\n",
      "Epoch 54/100\n",
      "2141/2141 [==============================] - 0s 187us/step - loss: 1.0218 - acc: 0.7352\n",
      "Epoch 55/100\n",
      "2141/2141 [==============================] - 0s 173us/step - loss: 1.0060 - acc: 0.7468\n",
      "Epoch 56/100\n",
      "2141/2141 [==============================] - 0s 181us/step - loss: 0.9678 - acc: 0.7384\n",
      "Epoch 57/100\n",
      "2141/2141 [==============================] - 0s 184us/step - loss: 0.9663 - acc: 0.7454\n",
      "Epoch 58/100\n",
      "2141/2141 [==============================] - 0s 186us/step - loss: 0.9892 - acc: 0.7520\n",
      "Epoch 59/100\n",
      "2141/2141 [==============================] - 0s 181us/step - loss: 0.9627 - acc: 0.7576\n",
      "Epoch 60/100\n",
      "2141/2141 [==============================] - 0s 163us/step - loss: 0.9593 - acc: 0.7539\n",
      "Epoch 61/100\n",
      "2141/2141 [==============================] - 0s 184us/step - loss: 0.9859 - acc: 0.7380\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2141/2141 [==============================] - 0s 167us/step - loss: 0.9629 - acc: 0.7567\n",
      "Epoch 63/100\n",
      "2141/2141 [==============================] - 0s 172us/step - loss: 0.9735 - acc: 0.7450\n",
      "Epoch 64/100\n",
      "2141/2141 [==============================] - 0s 191us/step - loss: 0.9251 - acc: 0.7609\n",
      "Epoch 65/100\n",
      "2141/2141 [==============================] - 0s 166us/step - loss: 0.9501 - acc: 0.7571\n",
      "Epoch 66/100\n",
      "2141/2141 [==============================] - 0s 172us/step - loss: 0.9479 - acc: 0.7511\n",
      "Epoch 67/100\n",
      "2141/2141 [==============================] - 0s 184us/step - loss: 0.8961 - acc: 0.7613\n",
      "Epoch 68/100\n",
      "2141/2141 [==============================] - 0s 154us/step - loss: 0.8897 - acc: 0.7702\n",
      "Epoch 69/100\n",
      "2141/2141 [==============================] - 0s 171us/step - loss: 0.8215 - acc: 0.7842\n",
      "Epoch 70/100\n",
      "2141/2141 [==============================] - 0s 176us/step - loss: 0.8765 - acc: 0.7707\n",
      "Epoch 71/100\n",
      "2141/2141 [==============================] - 0s 133us/step - loss: 0.8413 - acc: 0.7711\n",
      "Epoch 72/100\n",
      "2141/2141 [==============================] - 0s 179us/step - loss: 0.8476 - acc: 0.7828\n",
      "Epoch 73/100\n",
      "2141/2141 [==============================] - 0s 184us/step - loss: 0.8533 - acc: 0.7763\n",
      "Epoch 74/100\n",
      "2141/2141 [==============================] - 0s 132us/step - loss: 0.7830 - acc: 0.7936\n",
      "Epoch 75/100\n",
      "2141/2141 [==============================] - 0s 172us/step - loss: 0.8873 - acc: 0.7781\n",
      "Epoch 76/100\n",
      "2141/2141 [==============================] - 0s 180us/step - loss: 0.8340 - acc: 0.7823\n",
      "Epoch 77/100\n",
      "2141/2141 [==============================] - 0s 146us/step - loss: 0.8495 - acc: 0.7842\n",
      "Epoch 78/100\n",
      "2141/2141 [==============================] - 0s 189us/step - loss: 0.8148 - acc: 0.7964\n",
      "Epoch 79/100\n",
      "2141/2141 [==============================] - 0s 166us/step - loss: 0.7933 - acc: 0.7908\n",
      "Epoch 80/100\n",
      "2141/2141 [==============================] - 0s 110us/step - loss: 0.7403 - acc: 0.7978\n",
      "Epoch 81/100\n",
      "2141/2141 [==============================] - 0s 156us/step - loss: 0.7958 - acc: 0.7996\n",
      "Epoch 82/100\n",
      "2141/2141 [==============================] - 0s 182us/step - loss: 0.7674 - acc: 0.8001\n",
      "Epoch 83/100\n",
      "2141/2141 [==============================] - 0s 137us/step - loss: 0.7321 - acc: 0.8048\n",
      "Epoch 84/100\n",
      "2141/2141 [==============================] - 0s 188us/step - loss: 0.6765 - acc: 0.8141\n",
      "Epoch 85/100\n",
      "2141/2141 [==============================] - 0s 180us/step - loss: 0.7695 - acc: 0.8020\n",
      "Epoch 86/100\n",
      "2141/2141 [==============================] - 0s 173us/step - loss: 0.6770 - acc: 0.8169\n",
      "Epoch 87/100\n",
      "2141/2141 [==============================] - 0s 194us/step - loss: 0.6877 - acc: 0.8258\n",
      "Epoch 88/100\n",
      "2141/2141 [==============================] - 0s 174us/step - loss: 0.6800 - acc: 0.8206\n",
      "Epoch 89/100\n",
      "2141/2141 [==============================] - 0s 168us/step - loss: 0.6538 - acc: 0.8220\n",
      "Epoch 90/100\n",
      "2141/2141 [==============================] - 0s 180us/step - loss: 0.6121 - acc: 0.8351 0s - loss: 0.6119 - acc: 0.8\n",
      "Epoch 91/100\n",
      "2141/2141 [==============================] - 0s 152us/step - loss: 0.6211 - acc: 0.8309\n",
      "Epoch 92/100\n",
      "2141/2141 [==============================] - 0s 177us/step - loss: 0.6417 - acc: 0.8365\n",
      "Epoch 93/100\n",
      "2141/2141 [==============================] - 0s 169us/step - loss: 0.7034 - acc: 0.8197\n",
      "Epoch 94/100\n",
      "2141/2141 [==============================] - 0s 175us/step - loss: 0.6474 - acc: 0.8328\n",
      "Epoch 95/100\n",
      "2141/2141 [==============================] - 0s 177us/step - loss: 0.6573 - acc: 0.8244\n",
      "Epoch 96/100\n",
      "2141/2141 [==============================] - 0s 184us/step - loss: 0.6188 - acc: 0.8375\n",
      "Epoch 97/100\n",
      "2141/2141 [==============================] - 0s 187us/step - loss: 0.6659 - acc: 0.8281\n",
      "Epoch 98/100\n",
      "2141/2141 [==============================] - 0s 179us/step - loss: 0.6814 - acc: 0.8375\n",
      "Epoch 99/100\n",
      "2141/2141 [==============================] - 0s 171us/step - loss: 0.6416 - acc: 0.8370\n",
      "Epoch 100/100\n",
      "2141/2141 [==============================] - 0s 185us/step - loss: 0.6575 - acc: 0.8389\n",
      "Epoch 1/100\n",
      "2108/2108 [==============================] - 1s 591us/step - loss: 5.3702 - acc: 0.0778\n",
      "Epoch 2/100\n",
      "2108/2108 [==============================] - 0s 160us/step - loss: 4.2497 - acc: 0.1561\n",
      "Epoch 3/100\n",
      "2108/2108 [==============================] - 0s 177us/step - loss: 4.0013 - acc: 0.1964\n",
      "Epoch 4/100\n",
      "2108/2108 [==============================] - 0s 169us/step - loss: 3.7523 - acc: 0.2239\n",
      "Epoch 5/100\n",
      "2108/2108 [==============================] - 0s 161us/step - loss: 3.5652 - acc: 0.2590\n",
      "Epoch 6/100\n",
      "2108/2108 [==============================] - 0s 172us/step - loss: 3.4507 - acc: 0.2609\n",
      "Epoch 7/100\n",
      "2108/2108 [==============================] - 0s 160us/step - loss: 3.2522 - acc: 0.3031\n",
      "Epoch 8/100\n",
      "2108/2108 [==============================] - 0s 164us/step - loss: 3.1529 - acc: 0.3046\n",
      "Epoch 9/100\n",
      "2108/2108 [==============================] - 0s 169us/step - loss: 3.0947 - acc: 0.3150\n",
      "Epoch 10/100\n",
      "2108/2108 [==============================] - 0s 157us/step - loss: 2.9469 - acc: 0.3378\n",
      "Epoch 11/100\n",
      "2108/2108 [==============================] - 0s 165us/step - loss: 2.8098 - acc: 0.3705\n",
      "Epoch 12/100\n",
      "2108/2108 [==============================] - 0s 181us/step - loss: 2.6990 - acc: 0.4004\n",
      "Epoch 13/100\n",
      "2108/2108 [==============================] - 0s 163us/step - loss: 2.6146 - acc: 0.3999\n",
      "Epoch 14/100\n",
      "2108/2108 [==============================] - 0s 169us/step - loss: 2.5793 - acc: 0.4118\n",
      "Epoch 15/100\n",
      "2108/2108 [==============================] - 0s 176us/step - loss: 2.4739 - acc: 0.4194\n",
      "Epoch 16/100\n",
      "2108/2108 [==============================] - 0s 163us/step - loss: 2.3720 - acc: 0.4364\n",
      "Epoch 17/100\n",
      "2108/2108 [==============================] - 0s 146us/step - loss: 2.3551 - acc: 0.4478\n",
      "Epoch 18/100\n",
      "2108/2108 [==============================] - 0s 171us/step - loss: 2.2476 - acc: 0.4668\n",
      "Epoch 19/100\n",
      "2108/2108 [==============================] - 0s 162us/step - loss: 2.1990 - acc: 0.4810\n",
      "Epoch 20/100\n",
      "2108/2108 [==============================] - 0s 164us/step - loss: 2.0478 - acc: 0.4919\n",
      "Epoch 21/100\n",
      "2108/2108 [==============================] - 0s 151us/step - loss: 2.0349 - acc: 0.5128\n",
      "Epoch 22/100\n",
      "2108/2108 [==============================] - 0s 174us/step - loss: 2.0211 - acc: 0.5100\n",
      "Epoch 23/100\n",
      "2108/2108 [==============================] - 0s 156us/step - loss: 1.9387 - acc: 0.5266\n",
      "Epoch 24/100\n",
      "2108/2108 [==============================] - 0s 184us/step - loss: 1.8430 - acc: 0.5380\n",
      "Epoch 25/100\n",
      "2108/2108 [==============================] - 0s 159us/step - loss: 1.8251 - acc: 0.5522\n",
      "Epoch 26/100\n",
      "2108/2108 [==============================] - 0s 171us/step - loss: 1.7748 - acc: 0.5626\n",
      "Epoch 27/100\n",
      "2108/2108 [==============================] - 0s 172us/step - loss: 1.7064 - acc: 0.5783\n",
      "Epoch 28/100\n",
      "2108/2108 [==============================] - 0s 165us/step - loss: 1.6990 - acc: 0.5721\n",
      "Epoch 29/100\n",
      "2108/2108 [==============================] - 0s 171us/step - loss: 1.6454 - acc: 0.5854\n",
      "Epoch 30/100\n",
      "2108/2108 [==============================] - 0s 162us/step - loss: 1.6125 - acc: 0.5863\n",
      "Epoch 31/100\n",
      "2108/2108 [==============================] - 0s 123us/step - loss: 1.5954 - acc: 0.6086\n",
      "Epoch 32/100\n",
      "2108/2108 [==============================] - 0s 160us/step - loss: 1.5315 - acc: 0.6195\n",
      "Epoch 33/100\n",
      "2108/2108 [==============================] - 0s 172us/step - loss: 1.5248 - acc: 0.6153\n",
      "Epoch 34/100\n",
      "2108/2108 [==============================] - 0s 155us/step - loss: 1.5251 - acc: 0.6181\n",
      "Epoch 35/100\n",
      "2108/2108 [==============================] - 0s 169us/step - loss: 1.3927 - acc: 0.6409\n",
      "Epoch 36/100\n",
      "2108/2108 [==============================] - 0s 167us/step - loss: 1.3982 - acc: 0.6414\n",
      "Epoch 37/100\n",
      "2108/2108 [==============================] - 0s 179us/step - loss: 1.3919 - acc: 0.6584\n",
      "Epoch 38/100\n",
      "2108/2108 [==============================] - 0s 179us/step - loss: 1.3300 - acc: 0.6632\n",
      "Epoch 39/100\n",
      "2108/2108 [==============================] - 0s 150us/step - loss: 1.3068 - acc: 0.6632\n",
      "Epoch 40/100\n",
      "2108/2108 [==============================] - 0s 178us/step - loss: 1.2457 - acc: 0.6750\n",
      "Epoch 41/100\n",
      "2108/2108 [==============================] - 0s 169us/step - loss: 1.2735 - acc: 0.6679\n",
      "Epoch 42/100\n",
      "2108/2108 [==============================] - 0s 179us/step - loss: 1.2471 - acc: 0.6765\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - 0s 139us/step - loss: 1.2375 - acc: 0.6769\n",
      "Epoch 44/100\n",
      "2108/2108 [==============================] - 0s 173us/step - loss: 1.1836 - acc: 0.6988\n",
      "Epoch 45/100\n",
      "2108/2108 [==============================] - 0s 163us/step - loss: 1.1850 - acc: 0.6788\n",
      "Epoch 46/100\n",
      "2108/2108 [==============================] - 0s 183us/step - loss: 1.1598 - acc: 0.7002\n",
      "Epoch 47/100\n",
      "2108/2108 [==============================] - 0s 111us/step - loss: 1.1232 - acc: 0.7021\n",
      "Epoch 48/100\n",
      "2108/2108 [==============================] - 0s 147us/step - loss: 1.0758 - acc: 0.7116\n",
      "Epoch 49/100\n",
      "2108/2108 [==============================] - 0s 153us/step - loss: 1.0528 - acc: 0.7201\n",
      "Epoch 50/100\n",
      "2108/2108 [==============================] - 0s 178us/step - loss: 1.0179 - acc: 0.7225\n",
      "Epoch 51/100\n",
      "2108/2108 [==============================] - 0s 144us/step - loss: 1.0383 - acc: 0.7310\n",
      "Epoch 52/100\n",
      "2108/2108 [==============================] - 0s 175us/step - loss: 1.0622 - acc: 0.7206\n",
      "Epoch 53/100\n",
      "2108/2108 [==============================] - 0s 170us/step - loss: 1.0372 - acc: 0.7211\n",
      "Epoch 54/100\n",
      "2108/2108 [==============================] - 0s 150us/step - loss: 0.9259 - acc: 0.7552\n",
      "Epoch 55/100\n",
      "2108/2108 [==============================] - 0s 170us/step - loss: 1.0387 - acc: 0.7343\n",
      "Epoch 56/100\n",
      "2108/2108 [==============================] - 0s 174us/step - loss: 0.9270 - acc: 0.7538\n",
      "Epoch 57/100\n",
      "2108/2108 [==============================] - 0s 154us/step - loss: 0.9803 - acc: 0.7571\n",
      "Epoch 58/100\n",
      "2108/2108 [==============================] - 0s 179us/step - loss: 0.9442 - acc: 0.7505\n",
      "Epoch 59/100\n",
      "2108/2108 [==============================] - 0s 170us/step - loss: 0.8923 - acc: 0.7642\n",
      "Epoch 60/100\n",
      "2108/2108 [==============================] - 0s 158us/step - loss: 0.8665 - acc: 0.7713\n",
      "Epoch 61/100\n",
      "2108/2108 [==============================] - 0s 176us/step - loss: 0.8838 - acc: 0.7728\n",
      "Epoch 62/100\n",
      "2108/2108 [==============================] - 0s 181us/step - loss: 0.8571 - acc: 0.7723\n",
      "Epoch 63/100\n",
      "2108/2108 [==============================] - 0s 177us/step - loss: 0.9045 - acc: 0.7661\n",
      "Epoch 64/100\n",
      "2108/2108 [==============================] - 0s 185us/step - loss: 0.8533 - acc: 0.7837\n",
      "Epoch 65/100\n",
      "2108/2108 [==============================] - 0s 165us/step - loss: 0.8079 - acc: 0.7908\n",
      "Epoch 66/100\n",
      "2108/2108 [==============================] - 0s 181us/step - loss: 0.8232 - acc: 0.7842\n",
      "Epoch 67/100\n",
      "2108/2108 [==============================] - 0s 183us/step - loss: 0.7954 - acc: 0.7941\n",
      "Epoch 68/100\n",
      "2108/2108 [==============================] - 0s 187us/step - loss: 0.8051 - acc: 0.7951\n",
      "Epoch 69/100\n",
      "2108/2108 [==============================] - 0s 153us/step - loss: 0.7781 - acc: 0.7974\n",
      "Epoch 70/100\n",
      "2108/2108 [==============================] - 0s 163us/step - loss: 0.8150 - acc: 0.7984\n",
      "Epoch 71/100\n",
      "2108/2108 [==============================] - 0s 180us/step - loss: 0.7560 - acc: 0.8093\n",
      "Epoch 72/100\n",
      "2108/2108 [==============================] - 0s 168us/step - loss: 0.7299 - acc: 0.8060\n",
      "Epoch 73/100\n",
      "2108/2108 [==============================] - 0s 177us/step - loss: 0.7716 - acc: 0.7917\n",
      "Epoch 74/100\n",
      "2108/2108 [==============================] - 0s 176us/step - loss: 0.7573 - acc: 0.7970\n",
      "Epoch 75/100\n",
      "2108/2108 [==============================] - 0s 166us/step - loss: 0.7352 - acc: 0.7970\n",
      "Epoch 76/100\n",
      "2108/2108 [==============================] - 0s 180us/step - loss: 0.7151 - acc: 0.8155\n",
      "Epoch 77/100\n",
      "2108/2108 [==============================] - 0s 171us/step - loss: 0.7643 - acc: 0.8031\n",
      "Epoch 78/100\n",
      "2108/2108 [==============================] - 0s 176us/step - loss: 0.7303 - acc: 0.8164\n",
      "Epoch 79/100\n",
      "2108/2108 [==============================] - 0s 189us/step - loss: 0.6993 - acc: 0.8131\n",
      "Epoch 80/100\n",
      "2108/2108 [==============================] - 0s 174us/step - loss: 0.6836 - acc: 0.8145\n",
      "Epoch 81/100\n",
      "2108/2108 [==============================] - 0s 165us/step - loss: 0.6944 - acc: 0.8159\n",
      "Epoch 82/100\n",
      "2108/2108 [==============================] - 0s 183us/step - loss: 0.6968 - acc: 0.8193\n",
      "Epoch 83/100\n",
      "2108/2108 [==============================] - 0s 184us/step - loss: 0.6763 - acc: 0.8216\n",
      "Epoch 84/100\n",
      "2108/2108 [==============================] - 0s 173us/step - loss: 0.7103 - acc: 0.8207\n",
      "Epoch 85/100\n",
      "2108/2108 [==============================] - 0s 193us/step - loss: 0.7050 - acc: 0.8216\n",
      "Epoch 86/100\n",
      "2108/2108 [==============================] - 0s 181us/step - loss: 0.6407 - acc: 0.8335\n",
      "Epoch 87/100\n",
      "2108/2108 [==============================] - 0s 177us/step - loss: 0.6006 - acc: 0.8397\n",
      "Epoch 88/100\n",
      "2108/2108 [==============================] - 0s 183us/step - loss: 0.5829 - acc: 0.8491\n",
      "Epoch 89/100\n",
      "2108/2108 [==============================] - 0s 165us/step - loss: 0.6274 - acc: 0.8430\n",
      "Epoch 90/100\n",
      "2108/2108 [==============================] - 0s 179us/step - loss: 0.6229 - acc: 0.8392\n",
      "Epoch 91/100\n",
      "2108/2108 [==============================] - 0s 192us/step - loss: 0.6639 - acc: 0.8378\n",
      "Epoch 92/100\n",
      "2108/2108 [==============================] - 0s 171us/step - loss: 0.6169 - acc: 0.8425\n",
      "Epoch 93/100\n",
      "2108/2108 [==============================] - 0s 183us/step - loss: 0.6335 - acc: 0.8273\n",
      "Epoch 94/100\n",
      "2108/2108 [==============================] - 0s 181us/step - loss: 0.6072 - acc: 0.8420\n",
      "Epoch 95/100\n",
      "2108/2108 [==============================] - 0s 149us/step - loss: 0.5921 - acc: 0.8515\n",
      "Epoch 96/100\n",
      "2108/2108 [==============================] - 0s 115us/step - loss: 0.6380 - acc: 0.8439\n",
      "Epoch 97/100\n",
      "2108/2108 [==============================] - 0s 146us/step - loss: 0.5570 - acc: 0.8468\n",
      "Epoch 98/100\n",
      "2108/2108 [==============================] - 0s 161us/step - loss: 0.5808 - acc: 0.8482\n",
      "Epoch 99/100\n",
      "2108/2108 [==============================] - 0s 118us/step - loss: 0.5664 - acc: 0.8496\n",
      "Epoch 100/100\n",
      "2108/2108 [==============================] - 0s 135us/step - loss: 0.6311 - acc: 0.8340\n",
      "Epoch 1/100\n",
      "2141/2141 [==============================] - 1s 591us/step - loss: 5.3343 - acc: 0.0696\n",
      "Epoch 2/100\n",
      "2141/2141 [==============================] - 0s 170us/step - loss: 4.2650 - acc: 0.1485\n",
      "Epoch 3/100\n",
      "2141/2141 [==============================] - 0s 179us/step - loss: 3.9785 - acc: 0.1845\n",
      "Epoch 4/100\n",
      "2141/2141 [==============================] - 0s 184us/step - loss: 3.7448 - acc: 0.2223\n",
      "Epoch 5/100\n",
      "2141/2141 [==============================] - 0s 187us/step - loss: 3.5486 - acc: 0.2508\n",
      "Epoch 6/100\n",
      "2141/2141 [==============================] - 0s 179us/step - loss: 3.4229 - acc: 0.2714\n",
      "Epoch 7/100\n",
      "2141/2141 [==============================] - 0s 175us/step - loss: 3.3037 - acc: 0.2844\n",
      "Epoch 8/100\n",
      "2141/2141 [==============================] - 0s 188us/step - loss: 3.1670 - acc: 0.3041\n",
      "Epoch 9/100\n",
      "2141/2141 [==============================] - 0s 170us/step - loss: 3.0574 - acc: 0.3204\n",
      "Epoch 10/100\n",
      "2141/2141 [==============================] - 0s 175us/step - loss: 2.9364 - acc: 0.3517\n",
      "Epoch 11/100\n",
      "2141/2141 [==============================] - 0s 180us/step - loss: 2.8216 - acc: 0.3610\n",
      "Epoch 12/100\n",
      "2141/2141 [==============================] - 0s 169us/step - loss: 2.7167 - acc: 0.3783\n",
      "Epoch 13/100\n",
      "2141/2141 [==============================] - 0s 171us/step - loss: 2.6059 - acc: 0.3919\n",
      "Epoch 14/100\n",
      "2141/2141 [==============================] - 0s 123us/step - loss: 2.5310 - acc: 0.4017\n",
      "Epoch 15/100\n",
      "2141/2141 [==============================] - 0s 129us/step - loss: 2.4583 - acc: 0.4278\n",
      "Epoch 16/100\n",
      "2141/2141 [==============================] - 0s 142us/step - loss: 2.3770 - acc: 0.4353\n",
      "Epoch 17/100\n",
      "2141/2141 [==============================] - 0s 159us/step - loss: 2.2914 - acc: 0.4666\n",
      "Epoch 18/100\n",
      "2141/2141 [==============================] - 0s 154us/step - loss: 2.2541 - acc: 0.4605\n",
      "Epoch 19/100\n",
      "2141/2141 [==============================] - 0s 140us/step - loss: 2.1820 - acc: 0.4689\n",
      "Epoch 20/100\n",
      "2141/2141 [==============================] - 0s 154us/step - loss: 2.1021 - acc: 0.4867\n",
      "Epoch 21/100\n",
      "2141/2141 [==============================] - 0s 95us/step - loss: 2.0387 - acc: 0.4974\n",
      "Epoch 22/100\n",
      "2141/2141 [==============================] - 0s 132us/step - loss: 1.9992 - acc: 0.5152\n",
      "Epoch 23/100\n",
      "2141/2141 [==============================] - 0s 140us/step - loss: 1.8877 - acc: 0.5259\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2141/2141 [==============================] - 0s 161us/step - loss: 1.8734 - acc: 0.5427 0s - loss: 1.8353 - acc: 0\n",
      "Epoch 25/100\n",
      "2141/2141 [==============================] - 0s 172us/step - loss: 1.8812 - acc: 0.5353\n",
      "Epoch 26/100\n",
      "2141/2141 [==============================] - 0s 150us/step - loss: 1.7515 - acc: 0.5647\n",
      "Epoch 27/100\n",
      "2141/2141 [==============================] - 0s 178us/step - loss: 1.7291 - acc: 0.5787\n",
      "Epoch 28/100\n",
      "2141/2141 [==============================] - 0s 179us/step - loss: 1.6886 - acc: 0.5736\n",
      "Epoch 29/100\n",
      "2141/2141 [==============================] - 0s 166us/step - loss: 1.6471 - acc: 0.5838\n",
      "Epoch 30/100\n",
      "2141/2141 [==============================] - 0s 172us/step - loss: 1.5517 - acc: 0.6044\n",
      "Epoch 31/100\n",
      "2141/2141 [==============================] - 0s 172us/step - loss: 1.5906 - acc: 0.5993\n",
      "Epoch 32/100\n",
      "2141/2141 [==============================] - 0s 177us/step - loss: 1.4841 - acc: 0.6207\n",
      "Epoch 33/100\n",
      "2141/2141 [==============================] - 0s 164us/step - loss: 1.5015 - acc: 0.6142\n",
      "Epoch 34/100\n",
      "2141/2141 [==============================] - 0s 181us/step - loss: 1.4298 - acc: 0.6455\n",
      "Epoch 35/100\n",
      "2141/2141 [==============================] - 0s 182us/step - loss: 1.4138 - acc: 0.6464\n",
      "Epoch 36/100\n",
      "2141/2141 [==============================] - 0s 173us/step - loss: 1.4311 - acc: 0.6338\n",
      "Epoch 37/100\n",
      "2141/2141 [==============================] - 0s 173us/step - loss: 1.3635 - acc: 0.6446\n",
      "Epoch 38/100\n",
      "2141/2141 [==============================] - 0s 180us/step - loss: 1.3584 - acc: 0.6520\n",
      "Epoch 39/100\n",
      "2141/2141 [==============================] - 0s 158us/step - loss: 1.3667 - acc: 0.6595\n",
      "Epoch 40/100\n",
      "2141/2141 [==============================] - 0s 179us/step - loss: 1.3536 - acc: 0.6562\n",
      "Epoch 41/100\n",
      "2141/2141 [==============================] - 0s 181us/step - loss: 1.2652 - acc: 0.6702\n",
      "Epoch 42/100\n",
      "2141/2141 [==============================] - 0s 199us/step - loss: 1.2419 - acc: 0.6665\n",
      "Epoch 43/100\n",
      "2141/2141 [==============================] - 0s 178us/step - loss: 1.2554 - acc: 0.6796\n",
      "Epoch 44/100\n",
      "2141/2141 [==============================] - 0s 166us/step - loss: 1.2129 - acc: 0.6838\n",
      "Epoch 45/100\n",
      "2141/2141 [==============================] - 0s 194us/step - loss: 1.1579 - acc: 0.7053\n",
      "Epoch 46/100\n",
      "2141/2141 [==============================] - 0s 169us/step - loss: 1.2018 - acc: 0.6955\n",
      "Epoch 47/100\n",
      "2141/2141 [==============================] - 0s 193us/step - loss: 1.1214 - acc: 0.7076\n",
      "Epoch 48/100\n",
      "2141/2141 [==============================] - 0s 194us/step - loss: 1.1050 - acc: 0.7160\n",
      "Epoch 49/100\n",
      "2141/2141 [==============================] - 0s 174us/step - loss: 1.1201 - acc: 0.7109\n",
      "Epoch 50/100\n",
      "2141/2141 [==============================] - 0s 170us/step - loss: 1.0831 - acc: 0.7235\n",
      "Epoch 51/100\n",
      "2141/2141 [==============================] - 0s 177us/step - loss: 1.0711 - acc: 0.7244\n",
      "Epoch 52/100\n",
      "2141/2141 [==============================] - 0s 181us/step - loss: 0.9894 - acc: 0.7468\n",
      "Epoch 53/100\n",
      "2141/2141 [==============================] - 0s 186us/step - loss: 0.9893 - acc: 0.7408\n",
      "Epoch 54/100\n",
      "2141/2141 [==============================] - 0s 181us/step - loss: 1.0181 - acc: 0.7305\n",
      "Epoch 55/100\n",
      "2141/2141 [==============================] - 0s 186us/step - loss: 1.0103 - acc: 0.7492\n",
      "Epoch 56/100\n",
      "2141/2141 [==============================] - 0s 191us/step - loss: 0.8794 - acc: 0.7590\n",
      "Epoch 57/100\n",
      "2141/2141 [==============================] - 0s 173us/step - loss: 0.9121 - acc: 0.7511\n",
      "Epoch 58/100\n",
      "2141/2141 [==============================] - 0s 191us/step - loss: 0.9657 - acc: 0.7492\n",
      "Epoch 59/100\n",
      "2141/2141 [==============================] - 0s 179us/step - loss: 0.9573 - acc: 0.7492\n",
      "Epoch 60/100\n",
      "2141/2141 [==============================] - 0s 152us/step - loss: 0.8970 - acc: 0.7641\n",
      "Epoch 61/100\n",
      "2141/2141 [==============================] - 0s 176us/step - loss: 0.9207 - acc: 0.7632\n",
      "Epoch 62/100\n",
      "2141/2141 [==============================] - 0s 181us/step - loss: 0.8184 - acc: 0.7772\n",
      "Epoch 63/100\n",
      "2141/2141 [==============================] - 0s 153us/step - loss: 0.8547 - acc: 0.7753\n",
      "Epoch 64/100\n",
      "2141/2141 [==============================] - 0s 176us/step - loss: 0.8238 - acc: 0.7926\n",
      "Epoch 65/100\n",
      "2141/2141 [==============================] - 0s 161us/step - loss: 0.7882 - acc: 0.7894\n",
      "Epoch 66/100\n",
      "2141/2141 [==============================] - 0s 155us/step - loss: 0.8203 - acc: 0.7833\n",
      "Epoch 67/100\n",
      "2141/2141 [==============================] - 0s 177us/step - loss: 0.8684 - acc: 0.7791\n",
      "Epoch 68/100\n",
      "2141/2141 [==============================] - 0s 164us/step - loss: 0.8190 - acc: 0.7870\n",
      "Epoch 69/100\n",
      "2141/2141 [==============================] - 0s 169us/step - loss: 0.8129 - acc: 0.7903\n",
      "Epoch 70/100\n",
      "2141/2141 [==============================] - 0s 171us/step - loss: 0.7913 - acc: 0.7954\n",
      "Epoch 71/100\n",
      "2141/2141 [==============================] - 0s 171us/step - loss: 0.7896 - acc: 0.7889\n",
      "Epoch 72/100\n",
      "2141/2141 [==============================] - 0s 157us/step - loss: 0.7929 - acc: 0.7987\n",
      "Epoch 73/100\n",
      "2141/2141 [==============================] - 0s 166us/step - loss: 0.8068 - acc: 0.7950\n",
      "Epoch 74/100\n",
      "2141/2141 [==============================] - 0s 171us/step - loss: 0.7209 - acc: 0.8057\n",
      "Epoch 75/100\n",
      "2141/2141 [==============================] - 0s 170us/step - loss: 0.7312 - acc: 0.8090\n",
      "Epoch 76/100\n",
      "2141/2141 [==============================] - 0s 163us/step - loss: 0.6986 - acc: 0.8118\n",
      "Epoch 77/100\n",
      "2141/2141 [==============================] - 0s 176us/step - loss: 0.7683 - acc: 0.8015\n",
      "Epoch 78/100\n",
      "2141/2141 [==============================] - 0s 176us/step - loss: 0.7453 - acc: 0.8132\n",
      "Epoch 79/100\n",
      "2141/2141 [==============================] - 0s 161us/step - loss: 0.7333 - acc: 0.8080\n",
      "Epoch 80/100\n",
      "2141/2141 [==============================] - 0s 172us/step - loss: 0.7709 - acc: 0.8085\n",
      "Epoch 81/100\n",
      "2141/2141 [==============================] - 0s 172us/step - loss: 0.6736 - acc: 0.8220\n",
      "Epoch 82/100\n",
      "2141/2141 [==============================] - 0s 174us/step - loss: 0.7111 - acc: 0.8141\n",
      "Epoch 83/100\n",
      "2141/2141 [==============================] - 0s 190us/step - loss: 0.7016 - acc: 0.8239\n",
      "Epoch 84/100\n",
      "2141/2141 [==============================] - 0s 160us/step - loss: 0.7051 - acc: 0.8202\n",
      "Epoch 85/100\n",
      "2141/2141 [==============================] - 0s 173us/step - loss: 0.7108 - acc: 0.8216\n",
      "Epoch 86/100\n",
      "2141/2141 [==============================] - 0s 177us/step - loss: 0.7398 - acc: 0.8174\n",
      "Epoch 87/100\n",
      "2141/2141 [==============================] - 0s 163us/step - loss: 0.7078 - acc: 0.8220\n",
      "Epoch 88/100\n",
      "2141/2141 [==============================] - 0s 178us/step - loss: 0.6110 - acc: 0.8421\n",
      "Epoch 89/100\n",
      "2141/2141 [==============================] - 0s 182us/step - loss: 0.6907 - acc: 0.8337\n",
      "Epoch 90/100\n",
      "2141/2141 [==============================] - 0s 163us/step - loss: 0.7051 - acc: 0.8253\n",
      "Epoch 91/100\n",
      "2141/2141 [==============================] - 0s 178us/step - loss: 0.6807 - acc: 0.8300\n",
      "Epoch 92/100\n",
      "2141/2141 [==============================] - 0s 156us/step - loss: 0.6776 - acc: 0.8309\n",
      "Epoch 93/100\n",
      "2141/2141 [==============================] - 0s 167us/step - loss: 0.7034 - acc: 0.8197\n",
      "Epoch 94/100\n",
      "2141/2141 [==============================] - 0s 174us/step - loss: 0.6501 - acc: 0.8286\n",
      "Epoch 95/100\n",
      "2141/2141 [==============================] - 0s 184us/step - loss: 0.6813 - acc: 0.8300\n",
      "Epoch 96/100\n",
      "2141/2141 [==============================] - 0s 169us/step - loss: 0.6208 - acc: 0.8431\n",
      "Epoch 97/100\n",
      "2141/2141 [==============================] - 0s 151us/step - loss: 0.6335 - acc: 0.8421\n",
      "Epoch 98/100\n",
      "2141/2141 [==============================] - 0s 164us/step - loss: 0.6163 - acc: 0.8417\n",
      "Epoch 99/100\n",
      "2141/2141 [==============================] - 0s 168us/step - loss: 0.5917 - acc: 0.8431\n",
      "Epoch 100/100\n",
      "2141/2141 [==============================] - 0s 189us/step - loss: 0.5932 - acc: 0.8454\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "test_preds = []\n",
    "for i in range(len(valsets)):\n",
    "    #split val and train\n",
    "    val_genes = genes.loc[valsets[i][\"sig_id\"]]\n",
    "    val_sigs = all_df.loc[valsets[i][\"sig_id\"]]\n",
    "    train_genes = genes.drop(valsets[i][\"sig_id\"])\n",
    "    train_sigs = all_df.drop(valsets[i][\"sig_id\"])\n",
    "    #one hot encode labels\n",
    "    encoded_Y_train = label_encoder.transform(train_sigs['moa_v1'])\n",
    "    encoded_Y_val = label_encoder.transform(val_sigs['moa_v1'])\n",
    "    train_y = np_utils.to_categorical(encoded_Y_train)\n",
    "    val_y = np_utils.to_categorical(encoded_Y_val)\n",
    "    # define the mlp model\n",
    "        \n",
    "    gene_input = Input(name = 'gene_input',shape = (978,), dtype = 'float32')\n",
    "\n",
    "    fc1 = Dense(model_params['dense_size'][0], activation = 'relu', kernel_initializer='random_normal')(gene_input)\n",
    "    fc1 = Dropout(model_params['dropout'][0])(fc1)\n",
    "    fc2 = Dense(model_params['dense_size'][1], activation = 'relu', kernel_initializer='random_normal')(fc1)\n",
    "    fc2 = Dropout(model_params['dropout'][1])(fc2)\n",
    "    fc3 = Dense(model_params['dense_size'][2], activation = 'relu', kernel_initializer='random_normal')(fc2)\n",
    "    fc3 = Dropout(model_params['dropout'][2])(fc3)\n",
    "\n",
    "    prediction = Dense(all_df['moa_v1'].nunique(), activation = 'softmax')(fc3)\n",
    "    adam = keras.optimizers.Adam(lr=model_params[\"lr\"], beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False)\n",
    "    mlp_model = Model(inputs = gene_input, outputs = prediction)\n",
    "    mlp_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    rlr = ReduceLROnPlateau(monitor='loss',factor=0.5, patience=4, verbose=1, min_lr=0.0000001)\n",
    "        \n",
    "    mlp_model.fit(np.array(train_genes),train_y, batch_size = 64, \n",
    "                    epochs = int(100), \n",
    "                    verbose = 1, shuffle = True, validation_data = None)\n",
    "       \n",
    "    pred = mlp_model.predict(np.array(val_genes))\n",
    "    test_pred = mlp_model.predict(np.array(test_genes))\n",
    "    test_preds.append(test_pred)\n",
    "    accs.append(accuracy_score(np.argmax(val_y,axis=1), np.argmax(pred,axis=1)))\n",
    "ave_acc = np.mean(accs,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6470588235294118,\n",
       " 0.5846153846153846,\n",
       " 0.6122448979591837,\n",
       " 0.6307692307692307]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_Y_test = label_encoder.transform(test_sigs['moa_v1'])\n",
    "test_y = np_utils.to_categorical(encoded_Y_test)\n",
    "test_predictions = np.argmax(np.mean(test_preds,axis = 0),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5042735042735043"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1), test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 31,  31,  31,  31,  50,  50,  50,  44,  44,  50,  44,  44,  44,\n",
       "        44,  50,  44,  44,  44,  44,  44,  50,  50,  50,  50,  50,  50,\n",
       "        50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  52,\n",
       "        52, 102,  52,  69, 102,  89,   4, 102,  52,  63,  63,  63,  63,\n",
       "        63,  10,  10,  10,  35,  78,  93,  93,  80,  80,  80,  80,  80,\n",
       "        93,  80,  80,  80,  80,  80,  80,  17,  14,  14,  58,  14,  14,\n",
       "       114,  14,  14,  77,  14,  14,  14,  14,  14,  14,  14,  78,  14,\n",
       "        14,  14,  58,  50,  14,  44,  50,  44,  44,  50,  44,  44,  44,\n",
       "        44,  44,  62,  45,  18,  46,  45, 102, 102, 102, 119, 104, 104],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sigs['predicted'] = label_encoder.inverse_transform(test_predictions.astype(int))\n",
    "drug_acc = per_drug_acc(test_sigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_drug_acc(df):\n",
    "    unique_drugs = df['rdkit'].unique()\n",
    "    s = 0\n",
    "    for drug in unique_drugs:\n",
    "        filt = df[df['rdkit']==drug]\n",
    "        score = accuracy_score(filt['moa_v1'], filt['predicted'])\n",
    "        nunique_moa = filt['predicted'].nunique()\n",
    "        if score >= (1/nunique_moa):\n",
    "            s = s + 1\n",
    "    return(s/len(unique_drugs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le.inverse_transform(pred.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
