{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torch.functional import F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "from torch_geometric.utils import from_networkx, to_networkx, degree\n",
    "from torch_geometric.nn import GATConv, GCNConv, global_add_pool, PNAConv, BatchNorm, CGConv, global_max_pool\n",
    "from torch_geometric.utils.metric import accuracy, precision, f1_score\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from models.graph_transformer.euclidean_graph_transformer import GraphTransformerEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score, precision_score\n",
    "\n",
    "from utils.data_gen import load_prot_embs, to_categorical\n",
    "\n",
    "dev = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_embs, global_dict = load_prot_embs(512, norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wcsv2graph(fname, global_dict, y):\n",
    "    \"\"\"\n",
    "    Weighted Graph Creator\n",
    "    \"\"\"\n",
    "    sample = pd.read_csv('../snac_data/' + fname)\n",
    "    \n",
    "    G = nx.from_pandas_edgelist(sample, source='node1', target='node2', \n",
    "                            edge_attr=['sign', 'weight'], create_using=nx.DiGraph())\n",
    "\n",
    "    n1a1d = sample[['node1','downact1']]\n",
    "    n1a1u = sample[['node1','upact1']]\n",
    "    n1a1d.columns = ['node','downact']\n",
    "    n1a1u.columns = ['node', 'upact']\n",
    "\n",
    "    n2a2d = sample[['node2','downact2']]\n",
    "    n2a2u = sample[['node2','upact2']]\n",
    "    n2a2d.columns = ['node','downact']\n",
    "    n2a2u.columns = ['node', 'upact']\n",
    "    \n",
    "    nad = pd.concat([n1a1d,n2a2d])\n",
    "    nad = nad.drop_duplicates('node')\n",
    "    nad = nad.set_index('node')\n",
    "    nad['downacts'] = nad[['downact']].apply(lambda x: np.hstack(x), axis=1)\n",
    "    nad = nad.drop(['downact'], axis=1)['downacts'].to_dict()\n",
    "    \n",
    "    nau = pd.concat([n1a1u,n2a2u])\n",
    "    nau = nau.drop_duplicates('node')\n",
    "    nau = nau.set_index('node')\n",
    "    nau['upacts'] = nau[['upact']].apply(lambda x: np.hstack(x), axis=1)\n",
    "    nau = nau.drop(['upact'], axis=1)['upacts'].to_dict()\n",
    "    \n",
    "    nx.set_node_attributes(G, global_dict,'global_idx')\n",
    "    nx.set_node_attributes(G, nad, 'downacts')\n",
    "    nx.set_node_attributes(G, nau, 'upacts')\n",
    "    \n",
    "    data = from_networkx(G)\n",
    "    \n",
    "    G = to_networkx(data)\n",
    "    data.weight = data.weight.float()\n",
    "    data.downacts, data.upacts = data.downacts.double(), data.upacts.float()\n",
    "    data.acts = torch.cat([data.downacts.float(), data.upacts.float()], dim=-1).double()\n",
    "    data.downacts = data.upacts = None\n",
    "    \n",
    "    data.sign[data.sign < 0] = 0\n",
    "    data.sign = data.sign.long()\n",
    "    data.sign = to_categorical(data.sign, 2).reshape(-1,2).float()\n",
    "    \n",
    "    data.y = torch.tensor(y)\n",
    "    data.label = torch.tensor(np.argmax(y)).view(-1).long()\n",
    "    \n",
    "    return data\n",
    "\n",
    "class SNLDataset(Dataset):\n",
    "    def __init__(self, fnames, y, global_dict):\n",
    "        super(SNLDataset, self).__init__()\n",
    "        self.fnames = fnames\n",
    "        self.gd = global_dict\n",
    "        self.y = y\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.fnames)\n",
    "        \n",
    "    def get(self, idx):\n",
    "        return wcsv2graph(self.fnames[idx], self.gd, self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_ugraphs = pd.read_csv('../snac_data/graph_classification_all.csv')\n",
    "weighted_df = pd.read_csv('../snac_data/file_info_weighted.csv')\n",
    "\n",
    "val_set_1 = pd.read_csv('../snac_data/splits/val_set_1.csv')\n",
    "val_set_2 = pd.read_csv('../snac_data/splits/val_set_2.csv')\n",
    "val_set_3 = pd.read_csv('../snac_data/splits/val_set_3.csv')\n",
    "val_set_4 = pd.read_csv('../snac_data/splits/val_set_4.csv')\n",
    "test_set = pd.read_csv('../snac_data/splits/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsample_path = weighted_df.files_weighted.to_numpy()[200]\n",
    "data = wcsv2graph(wsample_path, global_dict, [0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d94d896dc54f209588744e2b7786f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1031.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "usm = pd.DataFrame(labelled_ugraphs.groupby('sig_id').moa_v1.unique()).reset_index()\n",
    "usm_corr = np.array([np.array(i) for i in usm.moa_v1.to_numpy()]).reshape(-1)\n",
    "usm['moa_v1'] = usm_corr\n",
    "\n",
    "X_df = pd.merge(weighted_df, usm, on='sig_id')\n",
    "val_df =  pd.merge(X_df, val_set_4, on='sig_id')\n",
    "test_df = pd.merge(X_df, test_set, on='sig_id')\n",
    "\n",
    "for sig in tqdm(test_set.sig_id):\n",
    "    X_df = X_df[X_df['sig_id'] != sig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_df.files_weighted.to_numpy(), X_df.moa_v1.to_numpy()\n",
    "X_val, y_val = val_df.files_weighted.to_numpy(), val_df.moa_v1_x.to_numpy()\n",
    "X_test, y_test = test_df.files_weighted.to_numpy(), test_df.moa_v1_x.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = OneHotEncoder()\n",
    "y = np.concatenate([y_train, y_val, y_test])\n",
    "le = le.fit(y.reshape(-1,1))\n",
    "y_train = le.transform(y_train.reshape(len(y_train),-1)).toarray()\n",
    "y_val = le.transform(y_val.reshape(len(y_val),-1)).toarray()\n",
    "y_test = le.transform(y_test.reshape(len(y_test), -1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SNLDataset(X_train, y_train, global_dict)\n",
    "val_data = SNLDataset(X_val, y_val, global_dict)\n",
    "test_data = SNLDataset(X_test, y_test, global_dict)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=1, num_workers=12, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=1, num_workers=12, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute in-degree histogram over training data.\n",
    "def deg_distr():\n",
    "    deg = torch.zeros(58, dtype=torch.long)\n",
    "    for data in tqdm(train_data):\n",
    "        d = degree(data.edge_index[0], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "        deg += torch.bincount(d, minlength=58)\n",
    "    return deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(PostEncoding,self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.act_emb = nn.Linear(2, emb_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return self.act_emb(data.acts.float())\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_classes, pretrained_weights=None, train_embs=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.pretrained_weights = pretrained_weights\n",
    "        if pretrained_weights is not None:\n",
    "            self.n_prots, self.in_channels = pretrained_weights.shape\n",
    "        else:\n",
    "            self.n_prots = 919\n",
    "            self.in_channels = 512\n",
    "\n",
    "        self.node_emb = nn.Embedding(self.n_prots, self.in_channels, sparse=True)\n",
    "        self.node_emb.weight.requires_grad = train_embs\n",
    "        \n",
    "        self.edge_emb = nn.Embedding(2, 50)\n",
    "        self.pe = PostEncoding(self.in_channels)\n",
    "\n",
    "        aggregators = ['mean', 'min', 'max', 'std']\n",
    "        scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        for _ in range(2):\n",
    "            conv = PNAConv(in_channels=self.in_channels, out_channels=self.in_channels,\n",
    "                           aggregators=aggregators, scalers=scalers, deg=deg,\n",
    "                           edge_dim=50, towers=4, pre_layers=1, post_layers=1,\n",
    "                           divide_input=False)\n",
    "            self.convs.append(conv)\n",
    "            self.batch_norms.append(BatchNorm(self.in_channels))\n",
    "            \n",
    "        self.fc1 = nn.Linear(self.in_channels, 2 * self.in_channels)\n",
    "        self.fc_out = nn.Linear(2 * self.in_channels, n_classes)\n",
    "        self.act = nn.PReLU()    \n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        if self.pretrained_weights is not None:\n",
    "            initrange = 0.1\n",
    "            self.node_emb.weight.data.copy_(torch.from_numpy(self.pretrained_weights))\n",
    "            self.edge_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        else:\n",
    "            initrange = 0.1\n",
    "            emb_layer.weight.data.uniform_(-initrange, initrange)\n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                      m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.node_emb(data.global_idx)\n",
    "        edge_attr = self.edge_emb(data.sign)\n",
    "        act = self.pe(data)\n",
    "        x = torch.add(x, act)\n",
    "\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = F.relu(batch_norm(conv(x, data.edge_index, edge_attr)))\n",
    "\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        x = self.act(self.fc1(x))\n",
    "        return F.log_softmax(self.fc_out(x), dim=-1)\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        #pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "    \n",
    "class FF(nn.Module):\n",
    "    def __init__(self, encoder, in_channels, n_classes):\n",
    "        super(FF, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.fc_out = nn.Linear(in_channels, n_classes)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = self.encoder(data)\n",
    "        \n",
    "        x = global_max_pool(x, data.batch)\n",
    "        x = F.log_softmax(self.fc_out(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 255\n",
    "EMB_DIM = 512\n",
    "\n",
    "model = BigNet(N_CLASSES, pretrained_weights=prot_embs).to(dev)\n",
    "encoder = GraphTransformerEncoder(n_layers=2, n_heads=8, n_hid=512, \n",
    "                            pretrained_weights=prot_embs, summarizer=None).to(dev)\n",
    "model = FF(encoder, EMB_DIM, N_CLASSES).to(dev)\n",
    "ls = LabelSmoothingLoss(N_CLASSES, smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5.0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_train_acc = []\n",
    "    for tb in train_data_iterator:\n",
    "        tb = tb.to(dev)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(tb).view(tb.num_graphs, -1)\n",
    "        y = tb.y.reshape(tb.num_graphs, -1)\n",
    "        y_l = tb.label\n",
    "        \n",
    "        #loss = F.nll_loss(pred, y_l)\n",
    "        loss = ls(pred, y_l)\n",
    "        \n",
    "        acc_t = accuracy(torch.argmax(pred, dim=1), y_l)\n",
    "        roc_t = roc_auc_score(y.long().detach().cpu().numpy(), \n",
    "                                pred.detach().cpu().numpy(), average='samples')\n",
    "        \n",
    "        train_data_iterator.set_postfix(Epoch=epoch+1,\n",
    "                                        loss ='%.4f' % float(loss.item()),\n",
    "                                        acc = '%.4f' % float(acc_t),\n",
    "                                        roc= '%.4f' % float(roc_t))\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optimizer.step()\n",
    "        total_train_acc.append(acc_t)\n",
    "\n",
    "        del tb\n",
    "    \n",
    "    print(f'Total Training Accuracy: {np.mean(total_train_acc)}')\n",
    "    \n",
    "    val_data_iterator = tqdm(test_loader,leave=True,unit='batch',\n",
    "                        postfix={'Epoch': epoch+1,'val_loss': '%.4f' % 0.0, \n",
    "                                 'acc': '%.4f' % 0.0,\n",
    "                                 'roc': '%.4f' % 0.0})\n",
    "    total_val_acc = []\n",
    "    with torch.no_grad():\n",
    "        for tb in val_data_iterator:\n",
    "            tb = tb.to(dev)\n",
    "\n",
    "            pred = model(tb)\n",
    "            y = tb.y.reshape(tb.num_graphs, -1)\n",
    "            y_l = tb.label\n",
    "\n",
    "            #loss = F.nll_loss(pred, y_l)\n",
    "            loss = ls(pred, y_l)\n",
    "            \n",
    "            acc_v = accuracy(torch.argmax(pred, dim=1), y_l)\n",
    "            roc_v = roc_auc_score(y.long().detach().cpu().numpy(), \n",
    "                                        pred.detach().cpu().numpy(), average='samples')\n",
    "\n",
    "            val_data_iterator.set_postfix(Epoch=epoch+1,\n",
    "                                            val_loss ='%.4f' % float(loss.item()),\n",
    "                                            acc = '%.4f' % float(acc_v),\n",
    "                                            roc= '%.4f' % float(roc_v))\n",
    "            total_val_acc.append(acc_v)\n",
    "    \n",
    "        \n",
    "    print(f'Total Validation Accuracy: {np.mean(total_val_acc)}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = FF(encoder, EMB_DIM, N_CLASSES).to(dev)\n",
    "m2 = FF(encoder, EMB_DIM, N_CLASSES).to(dev)\n",
    "m3 = FF(encoder, EMB_DIM, N_CLASSES).to(dev)\n",
    "m4 = FF(encoder, EMB_DIM, N_CLASSES).to(dev)\n",
    "\n",
    "m1.load_state_dict(torch.load('../snac_data/checkpoints/val_set_1/moa_vs_1_val_acc_0.5059288537549407.pt'))\n",
    "m2.load_state_dict(torch.load('../snac_data/checkpoints/val_set_2/moa_vs_2_val_acc_0.2686084142394822.pt'))\n",
    "m3.load_state_dict(torch.load('../snac_data/checkpoints/val_set_3/moa_vs_3_val_acc_0.47396963123644253.pt'))\n",
    "m4.load_state_dict(torch.load('../snac_data/checkpoints/val_set_4/moa_vs_4_val_acc_0.3865546218487395.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(Ensemble, self).__init__()\n",
    "        \n",
    "        self.mlist = nn.ModuleList([*models])\n",
    "        self.fc1 = nn.Linear(2048, 255)\n",
    "        \n",
    "        \n",
    "        for m in self.mlist:\n",
    "            m.fc_out = nn.Identity()\n",
    "            for param in m.parameters():\n",
    "                param.requires_grad_(False)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        total_preds = []\n",
    "        m1 = self.mlist[0]\n",
    "        m2 = self.mlist[1]\n",
    "        m3 = self.mlist[2]\n",
    "        m4 = self.mlist[3]\n",
    "            \n",
    "        preds = torch.cat([m1(data), m2(data), m3(data), m4(data)]).reshape(-1)\n",
    "        preds = F.log_softmax(self.fc1(preds), dim=-1) \n",
    "        return preds\n",
    "    \n",
    "model = Ensemble([m1, m2, m3, m4]).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2598b591842c46b5946985ccc93ec0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2616.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Training Accuracy: 0.04128440366972477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c8bb4007a34adb8db41ce76a43c4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1031.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-1d4647196f33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0;34m'acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'%.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         'roc': '%.4f' % 0.0})\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-ca43c10f77ca>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#loss = F.nll_loss(pred, y_l)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0macc_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9930dc6f59c5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mtrue_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothing\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrue_dist\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "lr = 1.0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.98)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('-' * 100)\n",
    "    print('-' * 100)\n",
    "    train_data_iterator = tqdm(train_loader,leave=True,unit='batch',\n",
    "                        postfix={'Epoch': epoch+1,'loss': '%.4f' % 0.0,\n",
    "                        'loss': '%.4f' % 0.0,\n",
    "                        'acc': '%.4f' % 0.0,\n",
    "                        'roc': '%.4f' % 0.0})\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval(models, tloader):\n",
    "    preds = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for tb in tqdm(tloader):\n",
    "                tb = tb.to(dev)\n",
    "                y_l = tb.label\n",
    "                pred = model(tb).view(1,-1).detach().cpu().numpy()\n",
    "\n",
    "                preds.append(pred)\n",
    "            \n",
    "    return preds\n",
    "\n",
    "def per_drug_acc(df):\n",
    "    unique_drugs = df['rdkit.x'].unique()\n",
    "    s = 0\n",
    "    for drug in unique_drugs:\n",
    "        filt = df[df['rdkit.x']==drug]\n",
    "        score = accuracy_score(filt['true'], filt['predicted'])\n",
    "        nunique_moa = filt['predicted'].nunique()\n",
    "        if score >= (1/nunique_moa):\n",
    "            s = s + 1\n",
    "    return(s/len(unique_drugs))\n",
    "\n",
    "def per_sig_acc(df):\n",
    "    unique_sigs = df['sig_id'].unique()\n",
    "    s = 0\n",
    "    for sig in unique_sigs:\n",
    "        filt = df[df['sig_id']==sig]\n",
    "        score = accuracy_score(filt['true'], filt['predicted'])\n",
    "        nunique_moa = filt['predicted'].nunique()\n",
    "        if score >= (1/nunique_moa):\n",
    "            s = s + 1\n",
    "    return(s/len(unique_sigs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5457e3d6f548c3a41c897a0c361e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1031.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fcc588051d48998db80fd9eeb116ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1031.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d88b19416d4920b3c1624d4998b007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1031.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f075949748e341a8870bb7a434bdc85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1031.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = test_eval([m1, m2, m3, m4], test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(preds)\n",
    "p_reshaped = preds.reshape(4, 1031, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean = torch.mean(torch.tensor(p_reshaped), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = torch.argmax(pred_mean, dim=-1)\n",
    "true_labels = np.argmax(y_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['true'] = true_labels\n",
    "test_set['predicted'] = pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_drug_acc(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1794871794871795"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_sig_acc(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('torch': conda)",
   "language": "python",
   "name": "python37164bittorchconda400f3b5524f54409b045df0fcc1fa418"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
