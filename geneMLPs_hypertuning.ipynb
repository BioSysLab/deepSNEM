{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import re\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model, Model\n",
    "from tempfile import TemporaryFile\n",
    "from keras import layers\n",
    "from keras.callbacks import History, ReduceLROnPlateau\n",
    "from keras.layers import Input, BatchNormalization, Activation\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Dropout, Layer\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import functools\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, roc_auc_score, \\\n",
    "    auc, average_precision_score, pairwise_distances\n",
    "import scikitplot as skplt\n",
    "import numpy as np\n",
    "from numpy import inf, ndarray\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from copy import deepcopy\n",
    "from NGF.utils import filter_func_args, mol_shapes_to_dims\n",
    "import NGF.utils\n",
    "import NGF_layers.features\n",
    "import NGF_layers.graph_layers\n",
    "from NGF_layers.features import one_of_k_encoding, one_of_k_encoding_unk, atom_features, bond_features, num_atom_features, num_bond_features\n",
    "from NGF_layers.features import padaxis, tensorise_smiles, concat_mol_tensors\n",
    "from NGF_layers.graph_layers import temporal_padding, neighbour_lookup, NeuralGraphHidden, NeuralGraphOutput\n",
    "from math import ceil\n",
    "from argparse import Namespace\n",
    "import ast\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from scipy.spatial.distance import jensenshannon, cosine, pdist\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following GPU devices are available: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "# Check available GPU devices.\n",
    "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read gene features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_zscore=pd.read_csv(\"cmap_gene_scores.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the training,val,test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (7,8,10,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df_pairs=pd.read_csv(\"allpairs3_genes_new.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs.value=df_pairs.value/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.220000e+02, 4.141000e+03, 3.811400e+04, 3.262790e+05,\n",
       "        1.585490e+06, 1.802477e+06, 8.053150e+05, 2.237570e+05,\n",
       "        4.708700e+04, 2.354000e+03]),\n",
       " array([0.01712343, 0.09436539, 0.17160735, 0.2488493 , 0.32609126,\n",
       "        0.40333322, 0.48057518, 0.55781714, 0.6350591 , 0.71230106,\n",
       "        0.78954302]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_pairs.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "moa_df = pd.read_csv(\"moa_df.csv\",index_col=0,engine = \"python\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=pd.read_csv(\"../deepSNEM/data/graph_info_df/file_info_weighted.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1 = pd.read_csv(\"val_set_1_withoriginals.csv\",index_col=0).reset_index(drop=True)\n",
    "val2 = pd.read_csv(\"val_set_2_withoriginals.csv\",index_col=0).reset_index(drop=True)\n",
    "val3 = pd.read_csv(\"val_set_3_withoriginals.csv\",index_col=0).reset_index(drop=True)\n",
    "val4 = pd.read_csv(\"val_set_4_withoriginals.csv\",index_col=0).reset_index(drop=True)\n",
    "test=pd.read_csv(\"test_set_withoriginals.csv\",index_col=0).reset_index(drop=True)\n",
    "\n",
    "valsets = [val1,val2,val3,val4]\n",
    "label_id = ['label1','label2','label3','label4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigs=list(set(list(df_pairs['sig_id_original.x'])+list(df_pairs['sig_id_original.y'])))\n",
    "sigs_test=list(set(list(test['sig_id_original'])))\n",
    "\n",
    "sigs_val1=list(set(list(val1['sig_id_original'])))\n",
    "sigs_val2=list(set(list(val2['sig_id_original'])))\n",
    "sigs_val3=list(set(list(val3['sig_id_original'])))\n",
    "sigs_val4=list(set(list(val4['sig_id_original'])))\n",
    "sigs_valset=[sigs_val1,sigs_val2,sigs_val3,sigs_val4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesnorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_zscores(sig_list,gene_size,cmap):\n",
    "    import numpy as np\n",
    "    cols=list(cmap_zscore.columns)\n",
    "    n=len(sig_list)\n",
    "    X_genes=np.zeros((n,gene_size),dtype='float32')\n",
    "    for idx,sig in enumerate(sig_list):\n",
    "        X_genes[idx]=np.array(cmap_zscore[sig])\n",
    "    \n",
    "    return X_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_train=tensorize_zscores(sigs,978,cmap_zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_test=tensorize_zscores(sigs_test,978,cmap_zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_val1=tensorize_zscores(sigs_val1,978,cmap_zscore)\n",
    "gene_val2=tensorize_zscores(sigs_val2,978,cmap_zscore)\n",
    "gene_val3=tensorize_zscores(sigs_val3,978,cmap_zscore)\n",
    "gene_val4=tensorize_zscores(sigs_val4,978,cmap_zscore)\n",
    "gene_val_set=[gene_val1,gene_val2,gene_val3,gene_val4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only keep labels with more than 3 examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df_pairs['label1'][df_pairs[df_pairs[\"label\"]==1][df_pairs[df_pairs[\"label\"]==1]['count.x']<=3].index]=0\n",
    "df_pairs['label2'][df_pairs[df_pairs[\"label\"]==1][df_pairs[df_pairs[\"label\"]==1]['count.x']<=3].index]=0\n",
    "df_pairs['label3'][df_pairs[df_pairs[\"label\"]==1][df_pairs[df_pairs[\"label\"]==1]['count.x']<=3].index]=0\n",
    "df_pairs['label4'][df_pairs[df_pairs[\"label\"]==1][df_pairs[df_pairs[\"label\"]==1]['count.x']<=3].index]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen_random_moa(bs,df,df_labels,moa_df,sigs,gene_zscore):\n",
    "    import numpy as np\n",
    "    counter=int(0)\n",
    "    #Keep looping indefinetely\n",
    "    while True:\n",
    "        \n",
    "        #Initialize batches of inputs and outputs\n",
    "        ind1 = []\n",
    "        ind2 = []\n",
    "        \n",
    "        d=[]\n",
    "        label = []\n",
    "        #Keep looping until we reach batch size\n",
    "        while len(ind1)<=bs: #doesn't matter if it is smi1 or smi2 since they have the same len\n",
    "            \n",
    "            # check to see if you reached the end of the frame\n",
    "            if counter==len(df):\n",
    "                counter=int(0)\n",
    "                df = df.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            s1=df['sig_id_original.x'][counter]\n",
    "            s2=df['sig_id_original.y'][counter]\n",
    "            ind1.append(sigs.index(s1))\n",
    "            ind2.append(sigs.index(s2))\n",
    "            d.append(1-df.value[counter])\n",
    "            label.append(df.label1[counter])\n",
    "            counter+=1\n",
    "        \n",
    "        moa_sampled = moa_df.sample(25).reset_index(drop=True)\n",
    "        for k in range(len(moa_sampled)):\n",
    "            df_moa = df_labels[df_labels['moa_v1.x'] == moa_sampled['moa_v1.x'][k]].reset_index(drop=True)\n",
    "            df_sampled = df_moa.sample(1).reset_index(drop=True)\n",
    "            s1_extra=df_sampled['sig_id_original.x'][0]\n",
    "            s2_extra=df_sampled['sig_id_original.y'][0]\n",
    "            ind1.append(sigs.index(s1_extra))\n",
    "            ind2.append(sigs.index(s2_extra))\n",
    "            d.append(1-df_sampled.value[0])\n",
    "            label.append(df_sampled.label1[0])\n",
    "        \n",
    "        in_vec_1=gene_zscore[ind1]\n",
    "\n",
    "\n",
    "        in_vec_2=gene_zscore[ind2]\n",
    "\n",
    "        \n",
    "        # yield the batch to the calling function\n",
    "        yield ({'gene_inputs_1':in_vec_1,'gene_inputs_2':in_vec_2,'moa_inputs':np.array(label,dtype='float32')},np.array(d,dtype = 'float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "def get_cindex(y_true, y_pred):\n",
    "    g = tf.subtract(tf.expand_dims(y_pred, -1), y_pred)\n",
    "    g = tf.cast(g == 0.0, tf.float32) * 0.5 + tf.cast(g > 0.0, tf.float32)\n",
    "\n",
    "    f = tf.subtract(tf.expand_dims(y_true, -1), y_true) > 0.0\n",
    "    f = tf.matrix_band_part(tf.cast(f, tf.float32), -1, 0)\n",
    "\n",
    "    g = tf.reduce_sum(tf.multiply(g, f))\n",
    "    f = tf.reduce_sum(f)\n",
    "\n",
    "    return tf.where(tf.equal(g, 0), 0.0, g/f)\n",
    "\n",
    "def pearson_r(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x, axis=0)\n",
    "    my = K.mean(y, axis=0)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = K.sum(xm * ym)\n",
    "    x_square_sum = K.sum(xm * xm)\n",
    "    y_square_sum = K.sum(ym * ym)\n",
    "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
    "    r = r_num / r_den\n",
    "    return K.mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_2(labels):\n",
    "    def double_cosine(y_true, y_pred):\n",
    "        condition = tf.math.equal(labels,1)\n",
    "        indices = tf.where(condition)\n",
    "        sliced_sim = tf.gather_nd(y_pred,indices)\n",
    "        mean_sim = tf.reduce_mean(tf.subtract(1.0,sliced_sim))\n",
    "        return 0.5*tf.reduce_mean(K.square(y_pred - y_true), axis=-1) + 0.5*mean_sim\n",
    "    return double_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_cosine(labels):\n",
    "    def cosine(y_true, y_pred):\n",
    "        condition = tf.math.equal(labels,1)\n",
    "        indices = tf.where(condition)\n",
    "        sliced_sim = tf.gather_nd(y_pred,indices)\n",
    "        mean_sim = tf.reduce_mean(tf.subtract(1.0,sliced_sim))\n",
    "        return mean_sim\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_genes(params):\n",
    "        \n",
    "    ### encode genes\n",
    "    in_vec=Input(name='gene_inputs', shape=(978,),dtype = 'float32')\n",
    "    \n",
    "    fc1=keras.layers.Dense(params['dense_size_1'],activation = None,kernel_regularizer=None, kernel_initializer='glorot_normal')(in_vec)\n",
    "    fc1=keras.layers.Dropout(params['dropout_1'])(fc1)\n",
    "    fc2=keras.layers.Dense(params['dense_size_2'],activation = None,kernel_regularizer=None, kernel_initializer='glorot_normal')(fc1)\n",
    "    fc2=keras.layers.Dropout(params['dropout_2'])(fc2)\n",
    "    fc3=keras.layers.Dense(params['dense_size_3'],activation = None,kernel_regularizer=None, kernel_initializer='glorot_normal')(fc2)\n",
    "    #fc4=keras.layers.Dense(128,activation = None,kernel_regularizer=None, kernel_initializer='glorot_normal')(fc3)\n",
    "    #fc5=keras.layers.Dense(256,activation = None,kernel_regularizer=None, kernel_initializer='glorot_normal')(fc4)\n",
    "    #fc6=keras.layers.Dense(512,activation = None,kernel_regularizer=None, kernel_initializer='glorot_normal')(fc5)\n",
    "    embeddings = keras.layers.Lambda(lambda x: K.l2_normalize(x,axis=1))(fc3)\n",
    "\n",
    "\n",
    "    interactionModel = keras.Model(inputs=in_vec, outputs= embeddings)\n",
    "\n",
    "    print(interactionModel.summary())\n",
    "    return interactionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(params):\n",
    "    # Initialize encoder\n",
    "    encoder_nikos = enc_genes(params)\n",
    "\n",
    "    # Initialize model\n",
    "    in_vec_1=Input(name='gene_inputs_1', shape=(978,),dtype = 'float32')\n",
    "    in_vec_2=Input(name='gene_inputs_2', shape=(978,),dtype = 'float32')\n",
    "\n",
    "\n",
    "\n",
    "    label=Input(name='moa_inputs',shape=(1,),dtype='float32')\n",
    "\n",
    "    encoded_1 = encoder_nikos(in_vec_1)\n",
    "    encoded_2 = encoder_nikos(in_vec_2)\n",
    "\n",
    "    prediction = keras.layers.dot([encoded_1,encoded_2],axes = 1, normalize=True)\n",
    "\n",
    "    siamese_net = Model(inputs=[in_vec_1,in_vec_2,label],outputs=prediction)\n",
    "    print(siamese_net.summary())\n",
    "    adam = keras.optimizers.Adam(lr=params['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "    siamese_net.compile(optimizer= adam,loss= custom_loss_2(label),metrics=['mse', get_cindex, r_square, pearson_r,metric_cosine(label)])\n",
    "    \n",
    "    return([siamese_net,encoder_nikos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the space\n",
    "fspace = {\n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.0001, 0.01),\n",
    "    'epoches' : hp.quniform('epoches',1,10,1),\n",
    "    'dense_size_1': hp.quniform('dense_size_1',128,1024,128),\n",
    "    'dense_size_2': hp.quniform('dense_size_2',128,1024,128),\n",
    "    'dense_size_3': hp.quniform('dense_size_3',128,1024,128),\n",
    "    'dropout_1': hp.quniform('dropout_1', 0.1, 0.3, 0.05),\n",
    "    'dropout_2': hp.quniform('dropout_1', 0.1, 0.3, 0.05)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(fspace, df_pairs, gene_val_set, gene_train, valsets,moa_df, sigs,sigs_valset):\n",
    "    accs = []\n",
    "    model_params = {\n",
    "        \"learning_rate\" : fspace['learning_rate'],\n",
    "        \"epoches\" : int(fspace['epoches']),\n",
    "        \"dense_size_1\" : int(fspace['dense_size_1']),\n",
    "        \"dense_size_2\" : int(fspace['dense_size_2']),\n",
    "        \"dense_size_3\" : int(fspace['max_delta_step']),\n",
    "        \"dropout_1\" : fspace['dropout_1'],\n",
    "        \"dropout_2\" : fspace['dropout_2']\n",
    "        }\n",
    "    for i in range(len(valsets)):\n",
    "        df_labels = df_pairs[df_pairs[label_id[i]]==1].reset_index(drop=True)\n",
    "        def enc_genes(params):\n",
    "            ### encode genes\n",
    "            in_vec=Input(name='gene_inputs', shape=(978,),dtype = 'float32')\n",
    "    \n",
    "            fc1=keras.layers.Dense(params['dense_size_1'],activation = None,kernel_regularizer=None, kernel_initializer='glorot_normal')(in_vec)\n",
    "            fc1=keras.layers.Dropout(params['dropout_1'])(fc1)\n",
    "            fc2=keras.layers.Dense(params['dense_size_2'],activation = None,kernel_regularizer=None, kernel_initializer='glorot_normal')(fc1)\n",
    "            fc2=keras.layers.Dropout(params['dropout_2'])(fc2)\n",
    "            fc3=keras.layers.Dense(params['dense_size_3'],activation = None,kernel_regularizer=None, kernel_initializer='glorot_normal')(fc2)\n",
    "            embeddings = keras.layers.Lambda(lambda x: K.l2_normalize(x,axis=1))(fc3)\n",
    "\n",
    "\n",
    "            interactionModel = keras.Model(inputs=in_vec, outputs= embeddings)\n",
    "\n",
    "            #print(interactionModel.summary())\n",
    "            return interactionModel\n",
    "        \n",
    "        #siamese_net=siamese_model[0](model_params)\n",
    "        # Initialize encoder\n",
    "        encoder_nikos = enc_genes(params)\n",
    "\n",
    "        # Initialize model\n",
    "        in_vec_1=Input(name='gene_inputs_1', shape=(978,),dtype = 'float32')\n",
    "        in_vec_2=Input(name='gene_inputs_2', shape=(978,),dtype = 'float32')\n",
    "\n",
    "\n",
    "\n",
    "        label=Input(name='moa_inputs',shape=(1,),dtype='float32')\n",
    "\n",
    "        encoded_1 = encoder_nikos(in_vec_1)\n",
    "        encoded_2 = encoder_nikos(in_vec_2)\n",
    "\n",
    "        prediction = keras.layers.dot([encoded_1,encoded_2],axes = 1, normalize=True)\n",
    "\n",
    "        siamese_net = Model(inputs=[in_vec_1,in_vec_2,label],outputs=prediction)\n",
    "        #print(siamese_net.summary())\n",
    "        adam = keras.optimizers.Adam(lr=params['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "        siamese_net.compile(optimizer= adam,loss= custom_loss_2(label),metrics=['mse', get_cindex, r_square, pearson_r,metric_cosine(label)])\n",
    "        rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=1, min_lr=0.00001, verbose=1, min_delta=1e-5)\n",
    "        term=keras.callbacks.TerminateOnNaN()\n",
    "        bs=512\n",
    "        NUM_EPOCHS = model_params[\"epoches\"]\n",
    "        df_pairs = df_pairs.sample(frac=1).reset_index(drop=True)\n",
    "        #Set total number of training samples and tests samples\n",
    "        NUM_TRAIN = len(df_pairs)\n",
    "        trainGen=train_gen_random_moa(bs,df_pairs,df_labels,moa_df,sigs,gene_train)\n",
    "        history = siamese_net.fit_generator(trainGen,\n",
    "                                            steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                                            epochs=NUM_EPOCHS,\n",
    "                                            verbose = 1,\n",
    "                                            shuffle = True,\n",
    "                                            callbacks= [term, rlr])\n",
    "        \n",
    "        emb=encoder_nikos.predict(gene_train[i],batch_size=1024)\n",
    "        emb_id=sigs\n",
    "        output_embs_train={}\n",
    "        output_embs_train.update({'emb':emb_id})\n",
    "        for i in range(model_params['dense_size_3']):\n",
    "            output_embs_train.update({'x_%s'%i:emb[:,i]})\n",
    "        \n",
    "        emb=encoder_nikos.predict(gene_val_set[i],batch_size=1024)\n",
    "        emb_id=sigs_valset[i]\n",
    "        output_embs={}\n",
    "        output_embs.update({'emb':emb_id})\n",
    "        for i in range(model_params['dense_size_3']):\n",
    "            output_embs.update({'x_%s'%i:emb[:,i]})\n",
    "        \n",
    "        ###Write R code to evaluate embeddings and investigate k\n",
    "        \n",
    "        \n",
    "        pred = bst.predict(dtest)\n",
    "        accs.append(accuracy_score(val_sigs['moa_categorical'], pred))\n",
    "    ave_acc = np.mean(accs,axis = 0)\n",
    "    return {'loss': -ave_acc ,  'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective, df_pairs = df_pairs, gene_val_set = gene_val_set,gene_train=gene_train, valsets = valsets, moa_df=moa_df, sigs=sigs,sigs_valset=sigs_valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 1000  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"my_model.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"my_model.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 21 trials to 1021 (+1000) trials\n",
      "100%|█████████████████████████████████████████| 1021/1021 [8:14:38<00:00, 29.07s/trial, best loss: -0.6027980422938406]\n",
      "Best: {'colsample_bylevel': 0.35788804057451146, 'colsample_bytree': 0.1226530432425497, 'gamma': 0.1571640395272282, 'learning_rate': 0.8098321148828739, 'max_bin': 176.0, 'max_delta_step': 5.0, 'max_depth': 11.0, 'min_child_weight': 10.0, 'reg_alpha': 2.0041642106706092, 'reg_lambda': 69.63017517747672, 'subsample': 0.7429682334802664}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = trials.trials[index]['misc']['vals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = trials.trials[index]['misc']['vals']\n",
    "hyper_params = {\n",
    "        \"colsample_bylevel\" : 0.35788804057451146,\n",
    "        \"colsample_bytree\" : 0.1226530432425497,\n",
    "        \"gamma\" : 0.1571640395272282,\n",
    "        \"eta\" : 0.8098321148828739,\n",
    "        \"max_delta_step\" : 5,\n",
    "        \"max_depth\" : 11,\n",
    "        \"min_child_weight\" : 10,\n",
    "        \"alpha\" : 2.0041642106706092,\n",
    "        \"lambda\" : 69.63017517747672,\n",
    "        \"subsample\" : 0.7429682334802664,\n",
    "        \"objective\":'multi:softmax',\n",
    "        'num_class': all_df['moa_categorical'].nunique(),\n",
    "        \"booster\":'gbtree',\n",
    "        \"eval_metric\":'merror'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for i in range(len(valsets)):\n",
    "    val_genes = genes.loc[valsets[i][\"sig_id\"]]\n",
    "    val_sigs = all_df.loc[valsets[i][\"sig_id\"]]\n",
    "    train_genes = genes.drop(valsets[i][\"sig_id\"])\n",
    "    train_sigs = all_df.drop(valsets[i][\"sig_id\"])\n",
    "    dtrain = xgb.DMatrix(data=train_genes, label=train_sigs['moa_categorical'])\n",
    "    dtest = xgb.DMatrix(data=val_genes, label = val_sigs['moa_categorical'])\n",
    "    evalist = [(dtest,'eval'),(dtrain,'train')]\n",
    "    bst = xgb.train(hyper_params, dtrain, 100, evalist, early_stopping_rounds = 10, verbose_eval = False)\n",
    "    pred = bst.predict(dtest)\n",
    "    accs.append(accuracy_score(val_sigs['moa_categorical'], pred))\n",
    "ave_acc = np.mean(accs,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6554621848739496,\n",
       " 0.5076923076923077,\n",
       " 0.6326530612244898,\n",
       " 0.6153846153846154]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "val_genes = genes.loc[valsets[0][\"sig_id\"]]\n",
    "val_sigs = all_df.loc[valsets[0][\"sig_id\"]]\n",
    "train_genes = genes.drop(valsets[0][\"sig_id\"])\n",
    "train_sigs = all_df.drop(valsets[0][\"sig_id\"])\n",
    "dtrain = xgb.DMatrix(data=train_genes, label=train_sigs['moa_categorical'])\n",
    "dval = xgb.DMatrix(data=val_genes, label = val_sigs['moa_categorical'])\n",
    "dtest = xgb.DMatrix(data=test_genes)\n",
    "evalist = [(dval,'eval'),(dtrain,'train')]\n",
    "bst = xgb.train(hyper_params, dtrain, 100, evalist, early_stopping_rounds = 10, verbose_eval = False)\n",
    "pred = bst.predict(dtest)\n",
    "print(accuracy_score(test_sigs['moa_categorical'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le.inverse_transform(pred.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
