{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DziyDcDzUZ7S"
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from comet_ml import Experiment\n",
    "import numpy as np\n",
    "from numpy import inf, ndarray\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import re\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model, Model\n",
    "from tempfile import TemporaryFile\n",
    "from keras import layers\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors\n",
    "from matplotlib import pyplot as plt\n",
    "# matplotlib inline\n",
    "from keras.callbacks import History, ReduceLROnPlateau\n",
    "from keras.layers import Input, BatchNormalization, Activation\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Dropout, Layer\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.regularizers import l2\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from copy import deepcopy\n",
    "from NGF.utils import filter_func_args, mol_shapes_to_dims\n",
    "import NGF.utils\n",
    "import NGF_layers.features\n",
    "import NGF_layers.graph_layers\n",
    "from NGF_layers.features import one_of_k_encoding, one_of_k_encoding_unk, atom_features, bond_features, num_atom_features, num_bond_features\n",
    "from NGF_layers.features import padaxis, tensorise_smiles, concat_mol_tensors\n",
    "from NGF_layers.graph_layers import temporal_padding, neighbour_lookup, NeuralGraphHidden, NeuralGraphOutput\n",
    "from math import ceil\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import networkx as nx\n",
    "#from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from argparse import Namespace\n",
    "import ast\n",
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from scipy.spatial.distance import jensenshannon, cosine, pdist\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following GPU devices are available: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "# Check available GPU devices.\n",
    "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UGRAPHEMP Implementation using GED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_all=pd.read_csv(\"samples_all.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_path = 'lc_embeddings_raw.pkl'\n",
    "prot_dict = {}\n",
    "with open(prot_path, 'rb') as f:\n",
    "    prot_dict = pickle.load(f)\n",
    "    \n",
    "prot_dict['Perturbation'] = np.zeros((384,))\n",
    "prot_dict['IKBKAP'] = np.zeros((384,))\n",
    "prot_dict['WHSC1'] = np.zeros((384,))\n",
    "prot_dict['ADRBK1'] = np.zeros((384,))\n",
    "prot_dict['FIGF'] = np.zeros((384,))\n",
    "prot_dict['FAM175A'] = np.zeros((384,))\n",
    "prot_dict['PARK2'] = np.zeros((384,))\n",
    "prot_dict['NKX3~1'] = np.zeros((384,))\n",
    "prot_dict['ERBB2IP'] = np.zeros((384,))\n",
    "\n",
    "shapes_dict = {\n",
    "    'num_prot_features' : 385,\n",
    "    'num_edge_features' : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_go_enc=pd.read_csv(\"../deepSNEM/data/prot_embeddings/new_features/bp_features.csv\")\n",
    "shapes_dict = {\n",
    "    'num_prot_features' : 399,\n",
    "    'num_edge_features' : 1\n",
    "}\n",
    "prot_dict={}\n",
    "for i in range(len(prot_go_enc)):\n",
    "    prot_dict.update({list(prot_go_enc.iloc[i])[0]:np.array(list(prot_go_enc.iloc[i])[1:len(list(prot_go_enc.iloc[i]))])})\n",
    "prot_dict['Perturbation'] = np.zeros((398,))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_location='/home/biolab/Documents/deepsnem_drive/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_nx(df):\n",
    "    flag=False\n",
    "    net=nx.DiGraph()\n",
    "    nodes=list(set(list(df['node1']) + list(df['node2'])))\n",
    "    if \"Perturbation\" not in nodes:\n",
    "        nodes.append(\"Perturbation\")\n",
    "        flag=True\n",
    "    for i in range(len(nodes)):\n",
    "        if ((nodes[i]==\"Perturbation\") and flag):\n",
    "            net.add_node(nodes[i],act=0)\n",
    "        elif nodes[i] in list(df['node1']):\n",
    "            j=list(df['node1']).index(nodes[i])\n",
    "            a=int(list(df['activity1'])[j])\n",
    "            net.add_node(nodes[i],act=a)\n",
    "        else:\n",
    "            j=list(df['node2']).index(nodes[i])\n",
    "            a=int(list(df['activity2'])[j])\n",
    "            net.add_node(nodes[i],act=a)\n",
    "    for i in range(len(df)):\n",
    "        net.add_edge(df['node1'][i], df['node2'][i], weight=df['sign'][i])\n",
    "    \n",
    "    return(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FastGEDEstimate(G1, G2):\n",
    "    import networkx as nx\n",
    "    #from networkx.drawing.nx_agraph import graphviz_layout\n",
    "    import numpy as np\n",
    "    from numpy import inf, ndarray\n",
    "    import pandas as pd\n",
    "    ged=0\n",
    "    G_new=G1\n",
    "    G_source=G2\n",
    "    if len(list(G1.nodes()))>len(list(G2.nodes())):\n",
    "        G_new=G2\n",
    "        G_source=G1\n",
    "    node_list_new=list(G_new.nodes())\n",
    "    node_list_source=list(G_source.nodes())\n",
    "    tomod=[elem for elem in node_list_source if (elem not in node_list_new)]\n",
    "    toadd=[elem  for elem in node_list_new if (elem not in node_list_source)]\n",
    "    #if (len(tomod)>=len(toadd)):\n",
    "    ind=list(np.random.choice(len(tomod), len(toadd)))\n",
    "    ged=len(tomod)+ged\n",
    "    ordered_new=[x[1] for x in list(nx.bfs_tree(G_new, 'Perturbation').edges())]\n",
    "    for i in list(nx.bfs_tree(G_source, 'Perturbation').edges()):\n",
    "        if (i[1] in tomod):\n",
    "            add=[x for x in ordered_new if x in toadd]\n",
    "            if add!=[]:\n",
    "                toadd.remove(add[0])\n",
    "                tomod.remove(i[1])\n",
    "                mapping = {i[1]:add[0]} \n",
    "                G_source= nx.relabel_nodes(G_source, mapping)\n",
    "                G_source.nodes(data=True)[add[0]]['act']=G_new.nodes(data=True)[add[0]]['act']\n",
    "        if toadd==[]:\n",
    "            break\n",
    "    for i in tomod:\n",
    "        G_source.remove_node(i)\n",
    "    rem1=[]\n",
    "    rem2=[]\n",
    "    for n1,n2,attr in G_source.edges(data=True):\n",
    "        if (not G_new.has_edge(n1,n2)):\n",
    "            #if G_source[n1][n2]['weight']==G_new[n1][n2]['weight']:\n",
    "            ged=ged+1\n",
    "            rem1.append(n1)\n",
    "            rem2.append(n2)\n",
    "    for i in range(len(rem1)):\n",
    "        G_source.remove_edge(rem1[i], rem2[i])\n",
    "    add1=[]\n",
    "    add2=[]\n",
    "    for n1,n2,attr in G_new.edges(data=True):\n",
    "        if (not G_source.has_edge(n1,n2)):\n",
    "            #if G_source[n1][n2]['weight']==G_new[n1][n2]['weight']:\n",
    "            ged=ged+1\n",
    "            add1.append(n1)\n",
    "            add2.append(n2)\n",
    "    for i in range(len(add1)):\n",
    "        G_source.add_edge(add1[i], add2[i],weight=G_new[add1[i]][add2[i]]['weight'])\n",
    "    for n1,n2,attr in G_source.edges(data=True):\n",
    "        if ((G_source.nodes(data=True)[n1]!={}) and G_new.nodes(data=True)[n1]!={}):\n",
    "            if (abs((G_source.nodes(data=True)[n1]['act']-G_new.nodes(data=True)[n1]['act'])/G_new.nodes(data=True)[n1]['act'])*100>20):\n",
    "                ged=ged+1\n",
    "                G_source.nodes(data=True)[n1]['act']=G_new.nodes(data=True)[n1]['act']\n",
    "        if ((G_source.nodes(data=True)[n2]!={}) and G_new.nodes(data=True)[n2]!={}):\n",
    "            if (abs((G_source.nodes(data=True)[n2]['act']-G_new.nodes(data=True)[n2]['act'])/G_new.nodes(data=True)[n2]['act'])*100>20):\n",
    "                ged=ged+1\n",
    "                G_source.nodes(data=True)[n2]['act']=G_new.nodes(data=True)[n2]['act']\n",
    "        if (abs((G_source[n1][n2]['weight']-G_new[n1][n2]['weight'])/G_new[n1][n2]['weight'])*100>20):\n",
    "            ged=ged+1\n",
    "            G_source[n1][n2]['weight']=G_new[n1][n2]['weight']\n",
    "\n",
    "    #return([G_source,ged])\n",
    "    return(ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GEDgenerator(bs,df_pairs,graphs_location,init_count):\n",
    "    import numpy as np\n",
    "    #counter=int(0)\n",
    "    counter=int(init_count)\n",
    "    #Keep looping indefinetely\n",
    "    while counter<len(df_pairs):\n",
    "        ged=[]\n",
    "        while len(ged)<bs:\n",
    "            # check to see if you reached the end of the frame\n",
    "            if counter==len(df_pairs):\n",
    "                break\n",
    "            \n",
    "            p1=graphs_location+df_pairs['graph.x'][counter]\n",
    "            p2=graphs_location+df_pairs['graph.y'][counter]\n",
    "            graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "            graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "            ged.append(FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2)))\n",
    "            counter=counter+1\n",
    "    \n",
    "        \n",
    "        yield(ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df_pairs=pd.read_csv(\"df_weighted.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=10000\n",
    "pr_steps=int(len(df_pairs)/bs)\n",
    "lp=round(pr_steps/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1/1[-]\r"
     ]
    }
   ],
   "source": [
    "GedGen=GEDgenerator(bs,df_pairs,graphs_location,init_count=0)\n",
    "ged=[]\n",
    "cn=0\n",
    "for g in range(pr_steps):\n",
    "    d=list(next(GedGen))\n",
    "    ged=ged+d\n",
    "    if (g%lp==0):\n",
    "        cn+=1\n",
    "    print(\"Steps: \"+str(g+1)+\"/\"+str(pr_steps)+\" [\"+\"-\"*cn, end=\"\\r\", flush=True)\n",
    "    np.savetxt(\"weighted_ged.csv\", ged)    \n",
    "print(\"Steps: \"+str(g+1)+\"/\"+str(pr_steps)+\"[\"+\"-\"*cn+\"]\", end=\"\\r\", flush=True)\n",
    "np.savetxt(\"weighted_ged.csv\", ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=pd.read_csv(\"weighted_ged.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=list(ged['V1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "814"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=np.array(ged)\n",
    "np.save(\"weighted_ged.npy\", ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([149., 183.,  56.,  23.,  16.,  17.,  43., 112., 113., 102.]),\n",
       " array([  1. ,  16.6,  32.2,  47.8,  63.4,  79. ,  94.6, 110.2, 125.8,\n",
       "        141.4, 157. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=[]\n",
    "cn=0\n",
    "for i in range(len(df_pairs)):\n",
    "    p1=graphs_location+df_pairs['graph.x'][i]\n",
    "    p2=graphs_location+df_pairs['graph.y'][i]\n",
    "    graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "    graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "    ged.append(FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2)))\n",
    "    df_pairs['value'][i]=ged[i]\n",
    "    if (i%(0.1*len(df_pairs))==0):\n",
    "        cn+=1\n",
    "    print(\"Pairs: \"+str(i+1)+\"/\"+str(len(df_pairs))+\" [\"+\"-\"*(cn+1), end=\"\\r\", flush=True)\n",
    "    np.savetxt(\"samples_all_ged.csv\", ged)    \n",
    "print(\"Steps: \"+str(g+1)+\"/\"+str(pr_steps)+\"[\"+\"-\"*(g+1)+\"]\", end=\"\\r\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=np.array(ged)\n",
    "np.save(\"samples_all_ged.npy\", ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_loop(i,df_pairs):\n",
    "    p1=graphs_location+df_pairs['graph.x'][i]\n",
    "    p2=graphs_location+df_pairs['graph.y'][i]\n",
    "    graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "    graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "    g=FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "inputs = list(range(len(df_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_list = Parallel(n_jobs=num_cores)(delayed(one_loop)(i,df_pairs) for i in inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"samples_all_ged.csv\", processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"samples_all_ged.npy\", np.array(processed_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs['value']= processed_list \n",
    "df_pairs.to_csv(\"df_pairs2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028229713439941406\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "ged=[]\n",
    "p1=graphs_location+list(samples_all['path_list'])[0]\n",
    "p2=graphs_location+list(samples_all['path_list'])[1]\n",
    "graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "ged.append(FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2)))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.46539306640625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(end - start)*16000000/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pairs\n",
    "df_pairs=pd.read_csv(\"\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ged=[]\n",
    "for i in range(len(df_pairs)):\n",
    "    p1=graphs_location+df_pairs['path_list1'][i]\n",
    "    p2=graphs_location+df_pairs['path_list2'][i]\n",
    "    graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "    graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "    ged.append(FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2)))\n",
    "    if i%int(0.1*len(df_pairs))==0:\n",
    "        print('Running: %s'%(i/len(df_pairs)))\n",
    "np.save(\"samples_all_ged.npy\", np.array(ged))\n",
    "    #X_prots, X_edgeattr, X_edges=tensorize_signalnet(df_to_nx(graph_df),prot_dict,100,21,shapes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"samples_all_ged.csv\", ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GEDloop_fast(graphs_location,graphx,graphy):\n",
    "    p1=graphs_location+graphx\n",
    "    p2=graphs_location+graphy\n",
    "    graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "    graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "    ged=FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2))\n",
    "    return ged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "df_pairs['value']=df_pairs.apply(lambda row: GEDloop_fast(graphs_location,row['graph.x'],row['graph.y']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorize smiles and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_signalnet(netx_list,prot_dict,max_prots,max_degree,shapes_dict):\n",
    "    import numpy as np\n",
    "    n = len(netx_list)\n",
    "    X_prots= np.zeros((n,max_prots,shapes_dict['num_prot_features']),dtype='float32')\n",
    "    Χ_edgattr=np.zeros((n,max_prots,max_degree,shapes_dict['num_edge_features']),dtype='float32')\n",
    "    X_edges=np.ones((n,max_prots,max_degree),dtype='int32')*int(-1)\n",
    "    for idx,netx in enumerate(netx_list):\n",
    "        #print(idx)\n",
    "        for i,p in enumerate(list(netx.nodes())):\n",
    "            l=list(prot_dict[p])\n",
    "            l.append(netx.nodes[p]['act'])\n",
    "            X_prots[idx][i]=np.array(l)\n",
    "            inds=[list(netx.nodes()).index(x) for x in list(netx[p])[0:max_degree]]\n",
    "            attrs=[netx[p][x]['weight'] for x in list(netx[p])[0:max_degree]]\n",
    "            if len(inds)<max_degree:\n",
    "                for j in range(max_degree-len(inds)):\n",
    "                    inds.append(-1)\n",
    "                    attrs.append(0)\n",
    "            X_edges[idx][i]=np.array(inds)\n",
    "            Χ_edgattr[idx][i]=np.array(attrs).reshape(21,1)\n",
    "    \n",
    "    return X_prots,Χ_edgattr,X_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df_pairs=pd.read_csv(\"df_pairs.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs.value=df_pairs.value/max(df_pairs.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.547600e+04, 3.448500e+04, 1.154500e+04, 1.099750e+05,\n",
       "        6.134040e+05, 8.460250e+05, 1.771639e+06, 1.133279e+06,\n",
       "        1.276280e+05, 1.453000e+03]),\n",
       " array([0.00421941, 0.10379747, 0.20337553, 0.30295359, 0.40253165,\n",
       "        0.5021097 , 0.60168776, 0.70126582, 0.80084388, 0.90042194,\n",
       "        1.        ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_pairs.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets_paths=list(set(list(df_pairs['graph.x'])+list(df_pairs['graph.y'])))\n",
    "graph_df=[pd.read_csv(graphs_location+x,index_col=0).reset_index(drop=True) for x in nets_paths]\n",
    "nets=[df_to_nx(x) for x in graph_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_nx=tensorize_signalnet(nets,prot_dict,100,21,shapes_dict)\n",
    "#graphs_nx=tensorize_signalnet([df_to_nx(y) for y in [pd.read_csv(graphs_location+x,index_col=0).reset_index(drop=True) for x in nets_paths]],prot_dict,100,21,shapes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 31, 69, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1],\n",
       "       [-1, -1, -1, ..., -1, -1, -1]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs_nx[2][25470]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(bs,df,nets_paths,graphs_nx,shapes_dict,prot_dict,max_prots,max_degree):\n",
    "    import numpy as np\n",
    "    counter=int(0)\n",
    "    #Keep looping indefinetely\n",
    "    while True:\n",
    "        \n",
    "        #Initialize batches of inputs and outputs\n",
    "        ind1 = []\n",
    "        ind2 = []\n",
    "        \n",
    "        d=[]\n",
    "        \n",
    "        #Keep looping until we reach batch size\n",
    "        while len(ind1)<=bs: #doesn't matter if it is smi1 or smi2 since they have the same len\n",
    "            \n",
    "            # check to see if you reached the end of the frame\n",
    "            if counter==len(df):\n",
    "                counter=int(0)\n",
    "                df = df.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            p1=df['graph.x'][counter]\n",
    "            p2=df['graph.y'][counter]\n",
    "            ind1.append(nets_paths.index(p1))\n",
    "            ind2.append(nets_paths.index(p2))\n",
    "            d.append(df.value[counter])\n",
    "            counter+=1\n",
    "            \n",
    "        #l=[tensorize_signalnet(net,prot_dict,max_prots,max_degree,shapes_dict) for net in net1]\n",
    "        #X_prots= np.array([x[0] for x in l])\n",
    "        #X_edgeattr= np.array([x[1] for x in l])\n",
    "        #X_edges=np.array([x[2] for x in l])\n",
    "        #prot_1,edgattr_1,edge_1=tensorize_signalnet(net1,prot_dict,max_prots,max_degree,shapes_dict)\n",
    "        prot_1=np.array(graphs_nx[0][ind1],dtype = 'float32')\n",
    "        edgattr_1=np.array(graphs_nx[1][ind1],dtype = 'float32')\n",
    "        edge_1=np.array(graphs_nx[2][ind1],dtype = 'int32')\n",
    "        #l=[tensorize_signalnet(net,prot_dict,max_prots,max_degree,shapes_dict) for net in net2]\n",
    "        #X_prots= np.array([x[0] for x in l])\n",
    "        #X_edgeattr= np.array([x[1] for x in l])\n",
    "        #X_edges=np.array([x[2] for x in l])\n",
    "        #prot_2,edgattr_2,edge_2=tensorize_signalnet(net2,prot_dict,max_prots,max_degree,shapes_dict)\n",
    "        prot_2=np.array(graphs_nx[0][ind2],dtype = 'float32')\n",
    "        edgattr_2=np.array(graphs_nx[1][ind2],dtype = 'float32')\n",
    "        edge_2=np.array(graphs_nx[2][ind2],dtype = 'int32')\n",
    "        \n",
    "        # yield the batch to the calling function\n",
    "        yield ({'prot_inputs_1':prot_1,'edgattr_inputs_1':edgattr_1,'edge_inputs_1':edge_1,'prot_inputs_2':prot_2,\n",
    "                'edgattr_inputs_2':edgattr_2,'edge_inputs_2':edge_2},np.array(d,dtype = 'float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##not ready yet\n",
    "def preds_generator(bs,nets_paths,graphs_nx,shapes_dict,prot_dict,max_prots,max_degreemodel_net):\n",
    "    \n",
    "    import numpy as np\n",
    "    counter=int(0)\n",
    "    #Keep looping indefinetely\n",
    "    while counter<len(nets_paths):\n",
    "        \n",
    "        #Initialize batches of inputs and outputs\n",
    "        net = []\n",
    "        \n",
    "        \n",
    "        #Keep looping until we reach batch size\n",
    "        while len(net)<=bs: #doesn't matter if it is smi1 or smi2 since they have the same len\n",
    "            \n",
    "            # check to see if you reached the end of the frame\n",
    "            if counter==len(df_cold):\n",
    "                break\n",
    "                \n",
    "            p=graphs_location+df_emb['files_combined'][counter]\n",
    "            graph_df=pd.read_csv(p,index_col=0).reset_index(drop=True)\n",
    "            net.append(df_to_nx(graph_df1))\n",
    "            counter+=1\n",
    "    \n",
    "            \n",
    "        prot,edgattr,edge=tensorize_signalnet(net,prot_dict,max_prots,max_degree,shapes_dict)\n",
    "        \n",
    "        y_pred=model_net.predict([prot,edgattr,edge],batch_size=bs)\n",
    "        \n",
    "        yield(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prots = 100\n",
    "max_degree = 21\n",
    "num_prot_features = shapes_dict['num_prot_features']\n",
    "num_edge_features = shapes_dict['num_edge_features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary and custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "def get_cindex(y_true, y_pred):\n",
    "    g = tf.subtract(tf.expand_dims(y_pred, -1), y_pred)\n",
    "    g = tf.cast(g == 0.0, tf.float32) * 0.5 + tf.cast(g > 0.0, tf.float32)\n",
    "\n",
    "    f = tf.subtract(tf.expand_dims(y_true, -1), y_true) > 0.0\n",
    "    f = tf.matrix_band_part(tf.cast(f, tf.float32), -1, 0)\n",
    "\n",
    "    g = tf.reduce_sum(tf.multiply(g, f))\n",
    "    f = tf.reduce_sum(f)\n",
    "\n",
    "    return tf.where(tf.equal(g, 0), 0.0, g/f)\n",
    "\n",
    "def pearson_r(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x, axis=0)\n",
    "    my = K.mean(y, axis=0)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = K.sum(xm * ym)\n",
    "    x_square_sum = K.sum(xm * xm)\n",
    "    y_square_sum = K.sum(ym * ym)\n",
    "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
    "    r = r_num / r_den\n",
    "    return K.mean(r)\n",
    "\n",
    "def mse_sliced(y_true,y_pred):\n",
    "    condition = tf.math.less_equal(y_pred,0.2)\n",
    "    indices = tf.where(condition)\n",
    "    slice_true = tf.gather_nd(y_true,indices)\n",
    "    slice_pred = tf.gather_nd(y_pred,indices)\n",
    "    mse_sliced = K.mean(K.square(slice_pred - slice_true), axis=-1)\n",
    "    return mse_sliced\n",
    "\n",
    "def custom_accuracy(y_true,y_pred):\n",
    "    cat_true = tf.math.less_equal(y_true,0.2)\n",
    "    cat_pred = tf.math.less_equal(y_pred,0.2)\n",
    "    casted_true = K.cast(cat_true,\"int32\")\n",
    "    casted_pred = K.cast(cat_pred,\"int32\")\n",
    "    acc = K.mean(K.equal(casted_true, K.round(casted_pred)))\n",
    "    return acc\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Computes the precision over the whole batch using threshold_value.\n",
    "        \"\"\"\n",
    "        cat_true = tf.math.less_equal(y_true,0.2)\n",
    "        casted_true = K.cast(cat_true,\"float32\")\n",
    "        cat_pred = tf.math.less_equal(y_pred,0.2)\n",
    "        casted_pred = K.cast(cat_pred,\"float32\")\n",
    "        # Compute the number of true positives.\n",
    "        true_positives = K.sum(tf.math.multiply(casted_true , casted_pred ))\n",
    "        # count the predicted positives\n",
    "        predicted_positives = K.sum(casted_pred)\n",
    "        # Get the precision ratio\n",
    "        precision_ratio = tf.math.divide(true_positives , predicted_positives + K.epsilon() )\n",
    "        return precision_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(sigma):\n",
    "    def gaussian_loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(0.5*tf.log(sigma) + 0.5*tf.div(tf.square(y_true - y_pred), sigma)) + 1e-6\n",
    "    return gaussian_loss\n",
    "\n",
    "class GaussianLayer(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(GaussianLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.kernel_1 = self.add_weight(name='kernel_1', \n",
    "                                      shape=(int(input_shape[-1]), self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.kernel_2 = self.add_weight(name='kernel_2', \n",
    "                                      shape=(int(input_shape[-1]), self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.bias_1 = self.add_weight(name='bias_1',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        self.bias_2 = self.add_weight(name='bias_2',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        super(GaussianLayer, self).build(input_shape) \n",
    "    def call(self, x):\n",
    "        output_mu  = K.dot(x, self.kernel_1) + self.bias_1\n",
    "        output_sig = K.dot(x, self.kernel_2) + self.bias_2\n",
    "        output_sig_pos = K.log(1 + K.exp(output_sig)) + 1e-06  \n",
    "        return [output_mu, output_sig_pos]\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0], self.output_dim), (input_shape[0], self.output_dim)]\n",
    "\n",
    "class ConGaussianLayer(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(ConGaussianLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.kernel_1 = self.add_weight(name='kernel_1', \n",
    "                                      shape=(int(input_shape[-1]), self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.kernel_2 = self.add_weight(name='kernel_2', \n",
    "                                      shape=(int(input_shape[-1]), self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.bias_1 = self.add_weight(name='bias_1',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        self.bias_2 = self.add_weight(name='bias_2',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        super(ConGaussianLayer, self).build(input_shape) \n",
    "    def call(self, x):\n",
    "        output_mu  = K.dot(x, self.kernel_1) + self.bias_1\n",
    "        output_mu  = keras.layers.ReLU(max_value=1)(output_mu)\n",
    "        output_sig = K.dot(x, self.kernel_2) + self.bias_2\n",
    "        output_sig_pos = K.log(1 + K.exp(output_sig)) + 1e-06 \n",
    "        output_sig_pos  = keras.layers.ReLU(max_value=1)(output_sig_pos) + 1e-06 \n",
    "        return [output_mu, output_sig_pos]\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0], self.output_dim), (input_shape[0], self.output_dim)]\n",
    "\n",
    "\n",
    "def euclidean_distance(x,y):\n",
    "    #x, y = vects\n",
    "    #eucl=tf.sqrt(tf.math.reduce_sum(tf.square(tf.subtract(x,y),\"Square\"),\"Sqrt\"),axis=1,keepdims=True)\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "    #return eucl\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "class Attention_theta(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention_theta, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.theta = self.add_weight(name='theta', \n",
    "                                      shape=(int(input_shape[-1]), int(input_shape[-1])),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        super(Attention_theta, self).build(input_shape) \n",
    "    def call(self, x):\n",
    "        import numpy as np\n",
    "        multi1  = K.dot(tf.math.reduce_mean(x,axis=1),self.theta)\n",
    "        #multi1=tf.transpose(multi1)\n",
    "        #print(K.int_shape(multi1))\n",
    "        multi1=Activation('relu')(multi1)\n",
    "        multi2  = K.batch_dot(x,multi1,axes=(2,1))\n",
    "        multi2=Activation('sigmoid')(multi2)\n",
    "        #print(K.int_shape(multi2))\n",
    "        #print(K.int_shape(x))\n",
    "        output_emb = K.batch_dot(multi2,x,axes=1)\n",
    "        output_emb=keras.layers.Reshape((1,K.int_shape(output_emb)[1]))(output_emb)\n",
    "        #print(K.int_shape(output_emb))\n",
    "        return output_emb\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],1,int(input_shape[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {'lr': 0.0001,\n",
    "     'nfilters': int(32),\n",
    "     'size': int(8),\n",
    "     'conv_width' : 128,\n",
    "     'fp_length' : 256,\n",
    "     'size_drug_1' : 8,\n",
    "     'size_drug_2' : 4,\n",
    "     'size_protein_1' : 8,\n",
    "     'size_protein_2' : 16,\n",
    "     'size_protein_3' : 3,\n",
    "     'batch_size': int(128),\n",
    "     'dense_size': int(256),\n",
    "     'dense_size_2': 512,\n",
    "     'dropout': 0.25,\n",
    "     'l2reg': 0.00}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(y_pred,Y_cold,th,df_cold):\n",
    "    true = np.reshape(Y_cold,len(df_cold))\n",
    "    pred = np.reshape(y_pred,len(df_cold))\n",
    "    cor = np.corrcoef(true,pred)\n",
    "    mse_all = sklearn.metrics.mean_squared_error(true,pred)\n",
    "    # calculate mse of similars\n",
    "    mse_sims = sklearn.metrics.mean_squared_error(true[pred<=th],pred[pred<=th])\n",
    "    # turn to categorical to calculate precision and accuracy\n",
    "    true_cat = true <= th\n",
    "    pred_cat = pred <= th\n",
    "    pos = np.sum(pred_cat)\n",
    "    prec = precision_score(true_cat,pred_cat)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(true_cat,pred_cat)\n",
    "    result =pd.DataFrame({'cor' : cor[0,1], 'mse_all' : mse_all, 'mse_similars' : mse_sims,'precision': prec, 'accuracy': acc,\n",
    "                         'positives' : pos}, index=[0])\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_nikos(params, lr_value, conv_width, fp_length):\n",
    "        \n",
    "    ### encode smiles\n",
    "    \n",
    "    prots0 = Input(name='prot_inputs', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "    edgattrs = Input(name='edgattr_inputs', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "    edges = Input(name='edge_inputs', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "    #prosoxi arxika to kano 256 ton pinaka\n",
    "    g1 = NeuralGraphHidden(128 , activ = None, bias = True , init = 'glorot_normal')([prots0,edgattrs,edges])\n",
    "    g1 = BatchNormalization(momentum=0.6)(g1)\n",
    "    g1 = Activation('relu')(g1)\n",
    "    #g1=keras.layers.Dropout(0.4)(g1)\n",
    "\n",
    "    g2 = NeuralGraphHidden(128  , activ = None, bias = True , init = 'glorot_normal')([g1,edgattrs,edges])\n",
    "    g2 = BatchNormalization(momentum=0.6)(g2)\n",
    "    g2 = Activation('relu')(g2)\n",
    "    #g2=keras.layers.Dropout(0.4)(g2)\n",
    "\n",
    "    g3 = NeuralGraphHidden(128  , activ = None, bias = True , init = 'glorot_normal')([g2,edgattrs,edges])\n",
    "    g3 = BatchNormalization(momentum=0.6)(g3)\n",
    "    g3 = Activation('relu')(g3)\n",
    "\n",
    "\n",
    "\n",
    "    g4=keras.layers.Conv1D(512,37, activation=None, use_bias=False, kernel_initializer='glorot_uniform')(g3)\n",
    "    g4= BatchNormalization(momentum=0.6)(g4)\n",
    "    g4 = Activation('relu')(g4)\n",
    "    #g4=keras.layers.Dropout(0.3)(g4)\n",
    "    \n",
    "    #g5=keras.layers.Conv1D(512,33, activation=None, use_bias=False, kernel_initializer='glorot_uniform')(g4)\n",
    "    #g5= BatchNormalization(momentum=0.6)(g5)\n",
    "    #g5 = Activation('relu')(g5)\n",
    "    #g5=keras.layers.Dropout(0.3)(g5)\n",
    "    \n",
    "    #g6=keras.layers.Conv1D(128,32, activation=None, use_bias=False, kernel_initializer='glorot_uniform')(g5)\n",
    "    #g6= BatchNormalization(momentum=0.6)(g6)\n",
    "    #g6 = Activation('relu')(g6)\n",
    "    #g6=keras.layers.Dropout(0.3)(g6)\n",
    "    #g6=keras.layers.Flatten()(g6)\n",
    "    g6=keras.layers.AveragePooling1D(pool_size=64)(g4)\n",
    "    g6=keras.layers.Flatten()(g6)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    interactionModel = keras.Model(inputs=[prots0, edgattrs, edges], outputs= g6)\n",
    "\n",
    "    print(interactionModel.summary())\n",
    "    return interactionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0508 22:24:52.643865 140015761385280 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0508 22:24:52.646544 140015761385280 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0508 22:24:52.647185 140015761385280 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0508 22:24:52.927547 140015761385280 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0508 22:24:53.495020 140015761385280 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0508 22:24:53.555204 140015761385280 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "prot_inputs (InputLayer)        (None, 100, 399)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edgattr_inputs (InputLayer)     (None, 100, 21, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs (InputLayer)        (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "neural_graph_hidden_1 (NeuralGr (None, 100, 128)     1077888     prot_inputs[0][0]                \n",
      "                                                                 edgattr_inputs[0][0]             \n",
      "                                                                 edge_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100, 128)     512         neural_graph_hidden_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100, 128)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "neural_graph_hidden_2 (NeuralGr (None, 100, 128)     349440      activation_1[0][0]               \n",
      "                                                                 edgattr_inputs[0][0]             \n",
      "                                                                 edge_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100, 128)     512         neural_graph_hidden_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 100, 128)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "neural_graph_hidden_3 (NeuralGr (None, 100, 128)     349440      activation_2[0][0]               \n",
      "                                                                 edgattr_inputs[0][0]             \n",
      "                                                                 edge_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100, 128)     512         neural_graph_hidden_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100, 128)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 64, 512)      2424832     activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 512)      2048        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 512)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePoo (None, 1, 512)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling1d_1[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 4,205,184\n",
      "Trainable params: 4,203,392\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0508 22:24:54.628710 140015761385280 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0508 22:24:54.632120 140015761385280 deprecation.py:323] From <ipython-input-21-9273130b0431>:3: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0508 22:24:54.653497 140015761385280 deprecation.py:323] From <ipython-input-20-1beb98e2bdc0>:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "prot_inputs_1 (InputLayer)      (None, 100, 399)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edgattr_inputs_1 (InputLayer)   (None, 100, 21, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_1 (InputLayer)      (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "prot_inputs_2 (InputLayer)      (None, 100, 399)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edgattr_inputs_2 (InputLayer)   (None, 100, 21, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_2 (InputLayer)      (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 512)          4205184     prot_inputs_1[0][0]              \n",
      "                                                                 edgattr_inputs_1[0][0]           \n",
      "                                                                 edge_inputs_1[0][0]              \n",
      "                                                                 prot_inputs_2[0][0]              \n",
      "                                                                 edgattr_inputs_2[0][0]           \n",
      "                                                                 edge_inputs_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 512)          0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (ConGaussianLayer)  [(None, 1), (None, 1 1026        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,206,210\n",
      "Trainable params: 4,204,418\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initialize encoder\n",
    "encoder_nikos = enc_nikos(p, 0.001, 256, 256)\n",
    "\n",
    "# Initialize model\n",
    "prots0_1 = Input(name='prot_inputs_1', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "edgattr_1 = Input(name='edgattr_inputs_1', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "edges_1 = Input(name='edge_inputs_1', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "prots0_2 = Input(name='prot_inputs_2', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "edgattr_2 = Input(name='edgattr_inputs_2', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "edges_2 =Input(name='edge_inputs_2', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "encoded_1 = encoder_nikos([prots0_1,edgattr_1,edges_1])\n",
    "#encoded_1 =keras.layers.Flatten()(encoded_1)\n",
    "encoded_2 = encoder_nikos([prots0_2,edgattr_2,edges_2])\n",
    "#encoded_2 = keras.layers.Flatten()(encoded_2)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_1, encoded_2])\n",
    "\n",
    "mu, sigma = ConGaussianLayer(1, name='main_output')(L1_distance)\n",
    "\n",
    "siamese_net = Model(inputs=[prots0_1,edgattr_1,edges_1,prots0_2,edgattr_2,edges_2],outputs=mu)\n",
    "print(siamese_net.summary())\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "siamese_net.compile(optimizer= adam,loss= custom_loss(sigma),metrics=['mse', get_cindex, r_square, pearson_r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0508 18:00:20.249168 140271167338304 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0508 18:00:20.252101 140271167338304 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0508 18:00:20.252803 140271167338304 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0508 18:00:20.538065 140271167338304 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0508 18:00:21.127068 140271167338304 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0508 18:00:21.187420 140271167338304 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "prot_inputs (InputLayer)        (None, 100, 399)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edgattr_inputs (InputLayer)     (None, 100, 21, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs (InputLayer)        (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "neural_graph_hidden_1 (NeuralGr (None, 100, 128)     1077888     prot_inputs[0][0]                \n",
      "                                                                 edgattr_inputs[0][0]             \n",
      "                                                                 edge_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100, 128)     512         neural_graph_hidden_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100, 128)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "neural_graph_hidden_2 (NeuralGr (None, 100, 128)     349440      activation_1[0][0]               \n",
      "                                                                 edgattr_inputs[0][0]             \n",
      "                                                                 edge_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100, 128)     512         neural_graph_hidden_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 100, 128)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "neural_graph_hidden_3 (NeuralGr (None, 100, 128)     349440      activation_2[0][0]               \n",
      "                                                                 edgattr_inputs[0][0]             \n",
      "                                                                 edge_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 100, 128)     512         neural_graph_hidden_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100, 128)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 64, 512)      2424832     activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 512)      2048        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 512)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePoo (None, 1, 512)       0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling1d_1[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 4,205,184\n",
      "Trainable params: 4,203,392\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0508 18:00:22.309803 140271167338304 deprecation_wrapper.py:119] From /home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0508 18:00:22.313133 140271167338304 deprecation.py:323] From <ipython-input-19-9273130b0431>:3: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0508 18:00:22.335592 140271167338304 deprecation.py:323] From <ipython-input-18-1beb98e2bdc0>:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "prot_inputs_1 (InputLayer)      (None, 100, 399)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edgattr_inputs_1 (InputLayer)   (None, 100, 21, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_1 (InputLayer)      (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "prot_inputs_2 (InputLayer)      (None, 100, 399)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edgattr_inputs_2 (InputLayer)   (None, 100, 21, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_2 (InputLayer)      (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 512)          4205184     prot_inputs_1[0][0]              \n",
      "                                                                 edgattr_inputs_1[0][0]           \n",
      "                                                                 edge_inputs_1[0][0]              \n",
      "                                                                 prot_inputs_2[0][0]              \n",
      "                                                                 edgattr_inputs_2[0][0]           \n",
      "                                                                 edge_inputs_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 512)          0           model_1[1][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (ConGaussianLayer)  [(None, 1), (None, 1 1026        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,206,210\n",
      "Trainable params: 4,204,418\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "9209/9209 [==============================] - 7209s 783ms/step - loss: 0.0061 - mean_squared_error: 0.0119 - get_cindex: 0.7933 - r_square: 0.3240 - pearson_r: nan\n",
      "Epoch 2/2\n",
      "9209/9209 [==============================] - 7207s 783ms/step - loss: 7.4406e-04 - mean_squared_error: 0.0015 - get_cindex: 0.8913 - r_square: 0.9162 - pearson_r: 0.9584\n"
     ]
    }
   ],
   "source": [
    "# Initialize encoder\n",
    "encoder_nikos = enc_nikos(p, 0.001, 256, 256)\n",
    "\n",
    "# Initialize model\n",
    "prots0_1 = Input(name='prot_inputs_1', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "edgattr_1 = Input(name='edgattr_inputs_1', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "edges_1 = Input(name='edge_inputs_1', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "prots0_2 = Input(name='prot_inputs_2', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "edgattr_2 = Input(name='edgattr_inputs_2', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "edges_2 =Input(name='edge_inputs_2', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "encoded_1 = encoder_nikos([prots0_1,edgattr_1,edges_1])\n",
    "#encoded_1 =keras.layers.Flatten()(encoded_1)\n",
    "encoded_2 = encoder_nikos([prots0_2,edgattr_2,edges_2])\n",
    "#encoded_2 = keras.layers.Flatten()(encoded_2)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_1, encoded_2])\n",
    "\n",
    "mu, sigma = ConGaussianLayer(1, name='main_output')(L1_distance)\n",
    "\n",
    "siamese_net = Model(inputs=[prots0_1,edgattr_1,edges_1,prots0_2,edgattr_2,edges_2],outputs=mu)\n",
    "print(siamese_net.summary())\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "siamese_net.compile(optimizer= adam,loss= custom_loss(sigma),metrics=['mse', get_cindex, r_square, pearson_r])\n",
    "\n",
    "# Train with fitgen\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=2, min_lr=0.00001, verbose=1, min_delta=1e-5)\n",
    "term=keras.callbacks.TerminateOnNaN()\n",
    "bs=512\n",
    "NUM_EPOCHS = 2\n",
    "df_pairs = df_pairs.sample(frac=1).reset_index(drop=True)\n",
    "#Set total number of training samples and tests samples\n",
    "NUM_TRAIN = len(df_pairs)\n",
    "trainGen=train_generator(bs,df_pairs,nets_paths,graphs_nx,shapes_dict,prot_dict,max_prots,max_degree)\n",
    "history = siamese_net.fit_generator(trainGen,\n",
    "                                    steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                                    epochs=NUM_EPOCHS,\n",
    "                                    verbose = 1,\n",
    "                                    shuffle = True,\n",
    "                                    callbacks= [term, rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.save_weights(\"Ugraphemp/models/siamese_model512_2.h5\")\n",
    "encoder_nikos.save_weights(\"Ugraphemp/models/encoder_model512_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_nikos.load_weights(\"Ugraphemp/models/encoder_model512_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=encoder_nikos.predict([graphs_nx[0],graphs_nx[1],graphs_nx[2]],batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_id=[]\n",
    "for f in nets_paths:\n",
    "    emb_id.append(f.strip(\".csv\").split(\"/\")[-2] + \"_emb_\" + str(f.strip(\".csv\").split(\"_\")[-1]))\n",
    "output_embs={}\n",
    "output_embs.update({'emb':emb_id})\n",
    "for i in range(512):\n",
    "    output_embs.update({'x_%s'%i:emb[:,i]})\n",
    "pd.DataFrame(output_embs).to_csv(\"Ugraphemp/embeddings/ged_embs512_seen_2ep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Later to embeed unseen\n",
    "nets_paths=list(samples_all['path_list'])\n",
    "graph_df=[pd.read_csv(graphs_location+x,index_col=0).reset_index(drop=True) for x in nets_paths]\n",
    "nets=[df_to_nx(x) for x in graph_df]\n",
    "graphs_nx=tensorize_signalnet(nets,prot_dict,100,21,shapes_dict)\n",
    "emb=encoder_nikos.predict([graphs_nx[0],graphs_nx[1],graphs_nx[2]],batch_size=512)\n",
    "emb_id=[]\n",
    "for f in nets_paths:\n",
    "    emb_id.append(f.strip(\".csv\").split(\"/\")[-2] + \"_emb_\" + str(f.strip(\".csv\").split(\"_\")[-1]))\n",
    "output_embs={}\n",
    "output_embs.update({'emb':emb_id})\n",
    "for i in range(512):\n",
    "    output_embs.update({'x_%s'%i:emb[:,i]})\n",
    "pd.DataFrame(output_embs).to_csv(\"Ugraphemp/embeddings/ged_embs512_samples_all_2ep.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.load_weights(\"Ugraphemp/models/siamese_model512_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "9209/9209 [==============================] - 6541s 710ms/step - loss: -2.5374 - mean_squared_error: 0.0058 - get_cindex: 0.8620 - r_square: 0.6774 - pearson_r: 0.8328\n",
      "Epoch 2/2\n",
      "9209/9209 [==============================] - 6515s 707ms/step - loss: -2.6491 - mean_squared_error: 0.0050 - get_cindex: 0.8777 - r_square: 0.7207 - pearson_r: 0.8593\n"
     ]
    }
   ],
   "source": [
    "# Train with fitgen\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=2, min_lr=0.00001, verbose=1, min_delta=1e-5)\n",
    "term=keras.callbacks.TerminateOnNaN()\n",
    "bs=512\n",
    "NUM_EPOCHS = 2\n",
    "df_pairs = df_pairs.sample(frac=1).reset_index(drop=True)\n",
    "#Set total number of training samples and tests samples\n",
    "NUM_TRAIN = len(df_pairs)\n",
    "trainGen=train_generator(bs,df_pairs,nets_paths,graphs_nx,shapes_dict,prot_dict,max_prots,max_degree)\n",
    "history = siamese_net.fit_generator(trainGen,\n",
    "                                    steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                                    epochs=NUM_EPOCHS,\n",
    "                                    verbose = 1,\n",
    "                                    shuffle = True,\n",
    "                                    callbacks= [term, rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.save_weights(\"Ugraphemp/models/siamese_model512_4.h5\")\n",
    "encoder_nikos.save_weights(\"Ugraphemp/models/encoder_model512_4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=encoder_nikos.predict([graphs_nx[0],graphs_nx[1],graphs_nx[2]],batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_id=[]\n",
    "for f in nets_paths:\n",
    "    emb_id.append(f.strip(\".csv\").split(\"/\")[-2] + \"_emb_\" + str(f.strip(\".csv\").split(\"_\")[-1]))\n",
    "output_embs={}\n",
    "output_embs.update({'emb':emb_id})\n",
    "for i in range(128):\n",
    "    output_embs.update({'x_%s'%i:emb[:,i]})\n",
    "pd.DataFrame(output_embs).to_csv(\"Ugraphemp/embeddings/ged_embs512_seen_4ep.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Later to embeed unseen\n",
    "nets_paths=list(samples_all['path_list'])\n",
    "graph_df=[pd.read_csv(graphs_location+x,index_col=0).reset_index(drop=True) for x in nets_paths]\n",
    "nets=[df_to_nx(x) for x in graph_df]\n",
    "graphs_nx=tensorize_signalnet(nets,prot_dict,100,21,shapes_dict)\n",
    "emb=encoder_nikos.predict([graphs_nx[0],graphs_nx[1],graphs_nx[2]],batch_size=512)\n",
    "emb_id=[]\n",
    "for f in nets_paths:\n",
    "    emb_id.append(f.strip(\".csv\").split(\"/\")[-2] + \"_emb_\" + str(f.strip(\".csv\").split(\"_\")[-1]))\n",
    "output_embs={}\n",
    "output_embs.update({'emb':emb_id})\n",
    "for i in range(128):\n",
    "    output_embs.update({'x_%s'%i:emb[:,i]})\n",
    "pd.DataFrame(output_embs).to_csv(\"Ugraphemp/embeddings/ged_embs512_samples_all_4ep.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for pravious projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [4]:\n",
    "    \n",
    "    #Load data\n",
    "    df = pd.read_csv(\"a375/\" + \"fold_%s/train_%s.csv\" %(i+1,i+1),index_col=0).reset_index(drop=True)\n",
    "    smiles=list(set(list(df['rdkit.x'])+list(df['rdkit.y'])))\n",
    "    X_atoms, X_bonds, X_edges = tensorise_smiles(smiles, max_degree, max_atoms)\n",
    "    df_cold = pd.read_csv(\"a375/\" + \"fold_%s/val_%s.csv\" %(i+1,i+1),index_col=0).reset_index(drop=True)\n",
    "    smiles_cold = list(set(list(df_cold['rdkit.x'])+list(df_cold['rdkit.y'])))\n",
    "    X_atoms_cold, X_bonds_cold, X_edges_cold = tensorise_smiles(smiles_cold, max_degree=5, max_atoms = 60)\n",
    "    Y_cold = df_cold.value/2\n",
    "    \n",
    "    n=8\n",
    "    cold_preds_mus = []\n",
    "    cold_preds_sigmas = []\n",
    "    while n<10:\n",
    "        \n",
    "        # Initialize encoder\n",
    "        encoder_nikos = enc_nikos(p, 0.001, 128, 256)\n",
    "    \n",
    "        # Initialize model\n",
    "        prots0_1 = Input(name='atom_inputs_1', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "        edgattr_1 = Input(name='edgattr_inputs_1', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "        edges_1 = Input(name='edge_inputs_1', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "        prots0_2 = Input(name='atom_inputs_2', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "        edgattr_2 = Input(name='edgattr_inputs_2', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "        edges_2 =Input(name='edge_inputs_2', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "        encoded_1 = encoder_nikos([prots0_1,edgattr_1,edges_1])\n",
    "        #encoded_1 =keras.layers.Flatten()(encoded_1)\n",
    "        encoded_2 = encoder_nikos([prots0_2,edgattr_2,edges_2])\n",
    "        #encoded_2 = keras.layers.Flatten()(encoded_2)\n",
    "\n",
    "        #K dot\n",
    "        #product_layer=keras.layers.Lambda(lambda tensors:tf.linalg.trace(K.batch_dot(tensors[0],tensors[1],axes=2)))\n",
    "        #product_layer=keras.layers.Lambda(lambda tensors:tf.reduce_sum(K.batch_dot(tensors[0],tensors[1],axes=2),[1, 2]))\n",
    "        product_layer=keras.layers.Lambda(lambda tensors:K.batch_dot(tensors[0],tensors[1],axes=2))\n",
    "        product=product_layer([encoded_1, encoded_2])\n",
    "\n",
    "        c1=keras.layers.Conv1D(128, 32, activation=None, use_bias=False, kernel_initializer='glorot_uniform')(product)\n",
    "        c1= BatchNormalization(momentum=0.6)(c1)\n",
    "        c1= Activation('relu')(c1)\n",
    "        c1=keras.layers.Dropout(0.25)(c1)\n",
    "        c1=keras.layers.Flatten()(c1)\n",
    "\n",
    "\n",
    "        mu, sigma = GaussianLayer(1, name='main_output')(c1)\n",
    "\n",
    "\n",
    "        siamese_net = Model(inputs=[prots0_1,edgattr_1,edges_1,prots0_2,edgattr_2,edges_2],outputs=mu)\n",
    "        print(siamese_net.summary())\n",
    "        adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "        siamese_net.compile(optimizer= adam,loss= custom_loss(sigma),metrics=['mse', get_cindex, r_square, pearson_r])\n",
    "\n",
    "        # Train with fitgen\n",
    "        rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=2, min_lr=0.00001, verbose=1, min_delta=1e-5)\n",
    "        term=keras.callbacks.TerminateOnNaN()\n",
    "        bs=128\n",
    "        NUM_EPOCHS = 20\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        #Set total number of training samples and tests samples\n",
    "        NUM_TRAIN = len(df)\n",
    "        trainGen=train_generator(bs,df,smiles,X_atoms, X_bonds, X_edges)\n",
    "        history = siamese_net.fit_generator(trainGen,\n",
    "                                            steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                                            epochs=NUM_EPOCHS,\n",
    "                                            verbose = 1,\n",
    "                                            shuffle = True,\n",
    "                                            callbacks= [term, rlr])\n",
    "        \n",
    "        if history.history[\"r_square\"][len(history.history[\"r_square\"])-1] < 0.7:\n",
    "            history = siamese_net.fit_generator(trainGen,\n",
    "                                             steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                                             epochs = 10,\n",
    "                                             verbose = 1,\n",
    "                                             shuffle = True,\n",
    "                                             callbacks= [term, rlr])\n",
    "        if history.history[\"r_square\"][len(history.history[\"r_square\"])-1] >= 0.7:\n",
    "            siamese_net.save_weights(\"a375/results/\" + \"fold_%s/models/\"%(i+1) + \"model_%s.h5\"%n)\n",
    "            gaussian = keras.Model(siamese_net.inputs, siamese_net.get_layer('main_output').output)\n",
    "            pr_steps=ceil(len(df_cold)/2048)\n",
    "            PredGen=preds_generator(2048,df_cold,smiles_cold,X_atoms_cold, X_bonds_cold, X_edges_cold,gaussian)\n",
    "            y_pred1=[]\n",
    "            y_pred2=[]\n",
    "            for g in range(pr_steps):\n",
    "                cold_pred=list(next(PredGen))\n",
    "                y_pred1=y_pred1+list(cold_pred[0])\n",
    "                y_pred2=y_pred2+list(cold_pred[1])\n",
    "            y_pred1=np.array(y_pred1)\n",
    "            y_pred2=np.array(y_pred2)\n",
    "            if (len(y_pred1[np.where(y_pred1 <= 0.2)])>0):\n",
    "                get = model_evaluate(y_pred1,Y_cold,0.2,df_cold)\n",
    "                get.to_csv(\"a375/results/\" + \"fold_%s/performance/\"%(i+1) + \"model_%s.csv\"%n)\n",
    "            cold_preds_mus.append(y_pred1)\n",
    "            np.save(\"a375/results/\"+ \"fold_%s/cold/mu/\"%(i+1) + \"cold_mu_%s.npy\"%n, y_pred1)\n",
    "            cold_preds_sigmas.append(y_pred2)\n",
    "            np.save(\"a375/results/\" + \"fold_%s/cold/sigma/\"%(i+1) + \"cold_sigma_%s.npy\"%n, y_pred2)\n",
    "            n=n+1\n",
    "    \n",
    "    mu_star=np.mean(cold_preds_mus,axis=0)\n",
    "    sigma_star = np.sqrt(np.mean(cold_preds_sigmas + np.square(cold_preds_mus), axis = 0) - np.square(mu_star))\n",
    "    cv_star = sigma_star/mu_star\n",
    "    if (len(mu_star[np.where(mu_star <= 0.2)])>0):\n",
    "        get_fold = model_evaluate(mu_star,Y_cold,0.2,df_cold)\n",
    "        get_fold.to_csv(\"a375/results/\" + \"fold_%s/ensemble_performance.csv\"%(i+1))\n",
    "    df_cold['mu'] = mu_star\n",
    "    df_cold['cv'] = cv_star\n",
    "    df_cold.to_csv(\"a375/results/\" + \"fold_%s/ensemble_preds_dataframe.csv\"%(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_preds_mus = []\n",
    "cold_preds_sigmas = []\n",
    "for n in range(10):\n",
    "    cold_preds_mus.append(np.load(\"a375/results/fold_5/cold/mu/cold_mu_%s.npy\"%n))\n",
    "    cold_preds_sigmas.append(np.load(\"a375/results/fold_5/cold/sigma/cold_sigma_%s.npy\"%n))\n",
    "\n",
    "mu_star=np.mean(cold_preds_mus,axis=0)\n",
    "sigma_star = np.sqrt(np.mean(cold_preds_sigmas + np.square(cold_preds_mus), axis = 0) - np.square(mu_star))\n",
    "cv_star = sigma_star/mu_star\n",
    "if (len(mu_star[np.where(mu_star <= 0.2)])>0):\n",
    "    get_fold = model_evaluate(mu_star,Y_cold,0.2,df_cold)\n",
    "    get_fold.to_csv(\"a375/results/fold_5/ensemble_performance.csv\")\n",
    "    df_cold['mu'] = mu_star\n",
    "    df_cold['cv'] = cv_star\n",
    "    df_cold.to_csv(\"a375/results/fold_5/ensemble_preds_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fold_all={'cor' : [], 'mse_all' : [], 'mse_similars' : [],'precision': [], 'accuracy': [],'positives' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    get=pd.read_csv(\"a375/results/fold_%s/ensemble_performance.csv\"%i,index_col=0)\n",
    "    get_fold_all['cor'].append(get['cor'][0])\n",
    "    get_fold_all['mse_all'].append(get['mse_all'][0])\n",
    "    get_fold_all['mse_similars'].append(get['mse_similars'][0])\n",
    "    get_fold_all['precision'].append(get['precision'][0])\n",
    "    get_fold_all['accuracy'].append(get['accuracy'][0])\n",
    "    get_fold_all['positives'].append(get['positives'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fold_all=pd.DataFrame(get_fold_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fold_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'cor' : get_fold_all['cor'].mean(), 'mse_all' : get_fold_all['mse_all'].mean(), 'mse_similars' : get_fold_all['mse_similars'].mean(),\n",
    "              'precision': get_fold_all['precision'].mean(), 'accuracy': get_fold_all['accuracy'].mean(),'positives' : ceil(get_fold_all['positives'].mean())}, index=[0]).to_csv(\"a375/results/cross_ensemble_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'cor' : get_fold_all['cor'].mean(), 'mse_all' : get_fold_all['mse_all'].mean(), 'mse_similars' : get_fold_all['mse_similars'].mean(),\n",
    "              'precision': get_fold_all['precision'].mean(), 'accuracy': get_fold_all['accuracy'].mean(),'positives' : ceil(get_fold_all['positives'].mean())}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net2json(df):\n",
    "    nodes=list(set(list(df['node1']) + list(df['node2'])))\n",
    "    #read pickle in the folder you are\n",
    "    #import pickle\n",
    "    #filep=open('ReSimNet-Dataset.pkl','rb')\n",
    "    #prot_enc=pickle.load(filep)\n",
    "    #filep.close()\n",
    "    if \"Perturbation\" in nodes:\n",
    "        nodes.remove(\"Perturbation\")\n",
    "    #nodes[nodes.index(\"Perturbation\")]=nodes[0]\n",
    "    #nodes[0]=\"Perturbation\"\n",
    "    feats={}\n",
    "    for j,x in enumerate(nodes):\n",
    "        if (x in list(df['node1'])):\n",
    "            activity=list(df['activity1'])[list(df['node1']).index(x)]\n",
    "        else:\n",
    "            activity=list(df['activity2'])[list(df['node2']).index(x)]\n",
    "        sign = lambda a: '-' if (int(a)==-1) else '+'\n",
    "        feats.update({\"%s\"%j:x+sign(activity)})\n",
    "    net={}\n",
    "    net.update({\"edges\":[]})\n",
    "    net.update({\"features\":feats})\n",
    "    for j in range(1,len(df)):\n",
    "        if ((df['node1'][j]!=\"Perturbation\") and (df['node2'][j]!=\"Perturbation\")):\n",
    "            ind1=nodes.index(df['node1'][j])\n",
    "            ind2=nodes.index(df['node2'][j])\n",
    "            net[\"edges\"].append([ind1,ind2])\n",
    "    return(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=pd.read_csv(\"file_info.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8UWqfPHWCGO"
   },
   "outputs": [],
   "source": [
    "#find how to copy file in python\n",
    "#from shutil import copyfile\n",
    "import json\n",
    "path_list=[]\n",
    "embed_id=[]\n",
    "prev_sig=[]\n",
    "counter=0\n",
    "for (i in range(len(info))):\n",
    "    if (info['sig_id'][i] not in prev_sig):\n",
    "        #copyfile(info['files_combined'][i], \"drive/My Drive/Computational_projects/deepsnem/graph2vec\")\n",
    "        path_list.append(info['files_combined'][i])\n",
    "        embed_id.append(info['emb'][i])\n",
    "        prev_sig.append(info['sig_id'][i])\n",
    "        counter+=1\n",
    "    if counter==7788:\n",
    "        break\n",
    "for (i,x in enumerate(path_list)):\n",
    "    df=pd.read_csv(x,index_col=0)\n",
    "    net=net2json(df)\n",
    "    with open('carn_data/'+embed_id[i]+'.json', 'w') as fp:\n",
    "        json.dump(net, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Xs9gqqI1p98c",
    "outputId": "635ef07b-df19-4ef1-ee9e-1be8f874ac6d"
   },
   "outputs": [],
   "source": [
    "def getMCS(G_source, G_new):\n",
    "    matching_graph=nx.Graph()\n",
    "\n",
    "    for n1,n2,attr in G_new.edges(data=True):\n",
    "        if G_source.has_edge(n1,n2) :\n",
    "            if G_source[n1][n2]['weight']==G_new[n1][n2]['weight']:\n",
    "                matching_graph.add_edge(n1,n2,weight=attr)\n",
    "\n",
    "    graphs = list(nx.connected_components(matching_graph))\n",
    "\n",
    "    mcs_length = 0\n",
    "    mcs_graph = nx.Graph()\n",
    "    for i, graph in enumerate(graphs):\n",
    "\n",
    "        if len(graph) > mcs_length:\n",
    "            mcs_length = len(graph)\n",
    "            mcs_graph = graph\n",
    "\n",
    "    return [mcs_length,mcs_graph]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "JeF02HWukEue",
    "kKckpFxZkEuh",
    "JRq-sQBqkEul",
    "Hvzz7L1dkEup",
    "-3edzyo7lrYF",
    "AMKuY0P_kEvD"
   ],
   "name": "UGRAPHEMP_encoder_att.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
