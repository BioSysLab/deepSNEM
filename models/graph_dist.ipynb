{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DziyDcDzUZ7S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "from comet_ml import Experiment\n",
    "import numpy as np\n",
    "from numpy import inf, ndarray\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import re\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model, Model\n",
    "from tempfile import TemporaryFile\n",
    "from keras import layers\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors\n",
    "from matplotlib import pyplot as plt\n",
    "# matplotlib inline\n",
    "from keras.callbacks import History, ReduceLROnPlateau\n",
    "from keras.layers import Input, BatchNormalization, Activation\n",
    "from keras.layers import CuDNNLSTM, Dense, Bidirectional, Dropout, Layer\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.regularizers import l2\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count, Pool\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from copy import deepcopy\n",
    "from NGF.utils import filter_func_args, mol_shapes_to_dims\n",
    "import NGF.utils\n",
    "import NGF_layers.features\n",
    "import NGF_layers.graph_layers\n",
    "from NGF_layers.features import one_of_k_encoding, one_of_k_encoding_unk, atom_features, bond_features, num_atom_features, num_bond_features\n",
    "from NGF_layers.features import padaxis, tensorise_smiles, concat_mol_tensors\n",
    "from NGF_layers.graph_layers import temporal_padding, neighbour_lookup, NeuralGraphHidden, NeuralGraphOutput\n",
    "from math import ceil\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import networkx as nx\n",
    "#from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from argparse import Namespace\n",
    "import ast\n",
    "import pickle\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from scipy.spatial.distance import jensenshannon, cosine, pdist\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following GPU devices are available: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "# Check available GPU devices.\n",
    "print(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UGRAPHEMP Implementation using GED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_all=pd.read_csv(\"samples_all.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_path = 'lc_embeddings_raw.pkl'\n",
    "prot_dict = {}\n",
    "with open(prot_path, 'rb') as f:\n",
    "    prot_dict = pickle.load(f)\n",
    "    \n",
    "prot_dict['Perturbation'] = np.zeros((384,))\n",
    "prot_dict['IKBKAP'] = np.zeros((384,))\n",
    "prot_dict['WHSC1'] = np.zeros((384,))\n",
    "prot_dict['ADRBK1'] = np.zeros((384,))\n",
    "prot_dict['FIGF'] = np.zeros((384,))\n",
    "prot_dict['FAM175A'] = np.zeros((384,))\n",
    "prot_dict['PARK2'] = np.zeros((384,))\n",
    "prot_dict['NKX3~1'] = np.zeros((384,))\n",
    "prot_dict['ERBB2IP'] = np.zeros((384,))\n",
    "\n",
    "shapes_dict = {\n",
    "    'num_prot_features' : 385,\n",
    "    'num_edge_features' : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_go_enc=pd.read_csv(\"../deepSNEM/data/prot_embeddings/bp_features.csv\")\n",
    "shapes_dict = {\n",
    "    'num_prot_features' : 1040,\n",
    "    'num_edge_features' : 1\n",
    "}\n",
    "prot_dict={}\n",
    "for i in range(len(prot_go_enc)):\n",
    "    prot_dict.update({list(prot_go_enc.iloc[i])[0]:np.array(list(prot_go_enc.iloc[i])[1:len(list(prot_go_enc.iloc[i]))])})\n",
    "prot_dict['Perturbation'] = np.zeros((1039,))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_location='/home/biolab/Documents/deepsnem_drive/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_nx(df):\n",
    "    flag=False\n",
    "    net=nx.DiGraph()\n",
    "    nodes=list(set(list(df['node1']) + list(df['node2'])))\n",
    "    if \"Perturbation\" not in nodes:\n",
    "        nodes.append(\"Perturbation\")\n",
    "        flag=True\n",
    "    for i in range(len(nodes)):\n",
    "        if ((nodes[i]==\"Perturbation\") and flag):\n",
    "            net.add_node(nodes[i],act=0)\n",
    "        elif nodes[i] in list(df['node1']):\n",
    "            j=list(df['node1']).index(nodes[i])\n",
    "            a=int(list(df['activity1'])[j])\n",
    "            net.add_node(nodes[i],act=a)\n",
    "        else:\n",
    "            j=list(df['node2']).index(nodes[i])\n",
    "            a=int(list(df['activity2'])[j])\n",
    "            net.add_node(nodes[i],act=a)\n",
    "    for i in range(len(df)):\n",
    "        net.add_edge(df['node1'][i], df['node2'][i], weight=df['sign'][i])\n",
    "    \n",
    "    return(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FastGEDEstimate(G1, G2):\n",
    "    import networkx as nx\n",
    "    #from networkx.drawing.nx_agraph import graphviz_layout\n",
    "    import numpy as np\n",
    "    from numpy import inf, ndarray\n",
    "    import pandas as pd\n",
    "    ged=0\n",
    "    G_new=G1\n",
    "    G_source=G2\n",
    "    if len(list(G1.nodes()))>len(list(G2.nodes())):\n",
    "        G_new=G2\n",
    "        G_source=G1\n",
    "    node_list_new=list(G_new.nodes())\n",
    "    node_list_source=list(G_source.nodes())\n",
    "    tomod=[elem for elem in node_list_source if (elem not in node_list_new)]\n",
    "    toadd=[elem  for elem in node_list_new if (elem not in node_list_source)]\n",
    "    #if (len(tomod)>=len(toadd)):\n",
    "    ind=list(np.random.choice(len(tomod), len(toadd)))\n",
    "    ged=len(tomod)+ged\n",
    "    ordered_new=[x[1] for x in list(nx.bfs_tree(G_new, 'Perturbation').edges())]\n",
    "    for i in list(nx.bfs_tree(G_source, 'Perturbation').edges()):\n",
    "        if (i[1] in tomod):\n",
    "            add=[x for x in ordered_new if x in toadd]\n",
    "            if add!=[]:\n",
    "                toadd.remove(add[0])\n",
    "                tomod.remove(i[1])\n",
    "                mapping = {i[1]:add[0]} \n",
    "                G_source= nx.relabel_nodes(G_source, mapping)\n",
    "                G_source.nodes(data=True)[add[0]]['act']=G_new.nodes(data=True)[add[0]]['act']\n",
    "        if toadd==[]:\n",
    "            break\n",
    "    for i in tomod:\n",
    "        G_source.remove_node(i)\n",
    "    rem1=[]\n",
    "    rem2=[]\n",
    "    for n1,n2,attr in G_source.edges(data=True):\n",
    "        if (not G_new.has_edge(n1,n2)):\n",
    "            #if G_source[n1][n2]['weight']==G_new[n1][n2]['weight']:\n",
    "            ged=ged+1\n",
    "            rem1.append(n1)\n",
    "            rem2.append(n2)\n",
    "    for i in range(len(rem1)):\n",
    "        G_source.remove_edge(rem1[i], rem2[i])\n",
    "    add1=[]\n",
    "    add2=[]\n",
    "    for n1,n2,attr in G_new.edges(data=True):\n",
    "        if (not G_source.has_edge(n1,n2)):\n",
    "            #if G_source[n1][n2]['weight']==G_new[n1][n2]['weight']:\n",
    "            ged=ged+1\n",
    "            add1.append(n1)\n",
    "            add2.append(n2)\n",
    "    for i in range(len(add1)):\n",
    "        G_source.add_edge(add1[i], add2[i],weight=G_new[add1[i]][add2[i]]['weight'])\n",
    "    for n1,n2,attr in G_source.edges(data=True):\n",
    "        if ((G_source.nodes(data=True)[n1]!={}) and G_new.nodes(data=True)[n1]!={}):\n",
    "            if (abs((G_source.nodes(data=True)[n1]['act']-G_new.nodes(data=True)[n1]['act'])/G_new.nodes(data=True)[n1]['act'])*100>20):\n",
    "                ged=ged+1\n",
    "                G_source.nodes(data=True)[n1]['act']=G_new.nodes(data=True)[n1]['act']\n",
    "        if ((G_source.nodes(data=True)[n2]!={}) and G_new.nodes(data=True)[n2]!={}):\n",
    "            if (abs((G_source.nodes(data=True)[n2]['act']-G_new.nodes(data=True)[n2]['act'])/G_new.nodes(data=True)[n2]['act'])*100>20):\n",
    "                ged=ged+1\n",
    "                G_source.nodes(data=True)[n2]['act']=G_new.nodes(data=True)[n2]['act']\n",
    "        if (abs((G_source[n1][n2]['weight']-G_new[n1][n2]['weight'])/G_new[n1][n2]['weight'])*100>20):\n",
    "            ged=ged+1\n",
    "            G_source[n1][n2]['weight']=G_new[n1][n2]['weight']\n",
    "\n",
    "    #return([G_source,ged])\n",
    "    return(ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GEDgenerator(bs,df_pairs,graphs_location,init_count):\n",
    "    import numpy as np\n",
    "    #counter=int(0)\n",
    "    counter=int(init_count)\n",
    "    #Keep looping indefinetely\n",
    "    while counter<len(df_pairs):\n",
    "        ged=[]\n",
    "        while len(ged)<bs:\n",
    "            # check to see if you reached the end of the frame\n",
    "            if counter==len(df_pairs):\n",
    "                break\n",
    "            \n",
    "            p1=graphs_location+df_pairs['graph.x'][counter]\n",
    "            p2=graphs_location+df_pairs['graph.y'][counter]\n",
    "            graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "            graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "            ged.append(FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2)))\n",
    "            counter=counter+1\n",
    "    \n",
    "        \n",
    "        yield(ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs=pd.read_csv(\"df_weighted.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=10000\n",
    "pr_steps=int(len(df_pairs)/bs)\n",
    "lp=round(pr_steps/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3087"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1/1[--]\r"
     ]
    }
   ],
   "source": [
    "GedGen=GEDgenerator(bs,df_pairs,graphs_location,init_count=0)\n",
    "ged=[]\n",
    "cn=0\n",
    "for g in range(pr_steps):\n",
    "    d=list(next(GedGen))\n",
    "    ged=ged+d\n",
    "    if (g%lp==0):\n",
    "        cn+=1\n",
    "    print(\"Steps: \"+str(g+1)+\"/\"+str(pr_steps)+\" [\"+\"-\"*cn, end=\"\\r\", flush=True)\n",
    "    np.savetxt(\"weighted_ged.csv\", ged)    \n",
    "print(\"Steps: \"+str(g+1)+\"/\"+str(pr_steps)+\"[\"+\"-\"*cn+\"]\", end=\"\\r\", flush=True)\n",
    "np.savetxt(\"weighted_ged.csv\", ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=pd.read_csv(\"weighted_ged.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=list(ged['V1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4604095"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=np.array(ged)\n",
    "np.save(\"weighted_ged.npy\", ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.700000e+01, 6.400000e+01, 2.988000e+03, 1.081180e+05,\n",
       "        6.130870e+05, 8.458340e+05, 1.771547e+06, 1.133279e+06,\n",
       "        1.276280e+05, 1.453000e+03]),\n",
       " array([  1. ,  24.6,  48.2,  71.8,  95.4, 119. , 142.6, 166.2, 189.8,\n",
       "        213.4, 237. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAD7CAYAAACmJ9mYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAATDUlEQVR4nO3db4xcdb3H8fduK+2GbvmzjChKqVfp14TARYFcovzJfWBufFDxCuFPLAZ5oBUSH1xMIEZvfGLSaEkQSynRmHiFa0JyI380SsKDBhuiUbSWYu73FqXtiuSybElor7Rid++DPavT2m53Zn9zZvfM+5VMdub3nd+c8/vN2fnsOTNzdmh6ehpJkkoa7vcKSJKax3CRJBVnuEiSijNcJEnFGS6SpOKW93sFFokVwBXAK8DRPq+LJC0Vy4B3Ar8AjrQXDJcZVwA/7fdKSNISdTWwo73BcJnxCsDrr/8fU1Odfe9nbGwVk5OHerJSS8Ggjx+cg0EfPwzuHAwPD3HWWadD9RraznCZcRRgamq643CZ7TfIBn384BwM+vhh4Ofg795O8A19SVJxhoskqTjDRZJUnOEiSSrOcJEkFWe4SJKKm9dHkSNiM3A9sBa4ODN3R8Ra4LG2u50JrM7Ms6s+e4HD1QXg7sx8qqpdCTwEjAB7gQ2Z+WqvapKkes33ey6PAd+g7VvsmbkXuHT2dkTcd4LHuyEzd7c3RMQQ8DBwW2buiIgvAZuA23tRm+f4JJ3A6OoRVq449ctEqzVadLmHj/yFg2+8WfQxVa95hUtm7gCIiBPWI+I04JPAv8zj4S4HDs8+JrCNmT2N23tUk9SllSuWs/6ux2tf7pP3XsfB2peqkkq95/Ix4OXM/NVx7Y9ExK6I2BoRZ1Zta4B9s3fIzNeA4Yg4u0c1SVLNSp3+5XbgO8e1XZ2Z4xGxArgP2AJsKLS8nhgbW9VVv9KHBJaaQR8/OAe9sNTmdKmtb68tOFwi4jzgWuDW9vbMHK9+HomIrcATVWk/cEFb/3OA6cw8EBHFa52MZXLyUMfnB2q1RpmYGNwd+EEfPzR7Dvr5grmU5rTJ28BchoeHTvpHeYnDYrcBP8rMydmGiDg9Is6org8BNwM7q/JzwEhEXFXd3gg82sOaJKlm8wqXiLg/Iv4AvBt4OiJeaCvfxt8fEjsX2B4Ru4DdwDrgDoDMnGJmL+fBiNjDzF7PPb2qSZLqNzQ9PdCniZ61FnjJw2KdG/TxQ7PnoNUa7dunxZbSnDZ5G5hL22Gx9zDzCd2/1fqxQpKkZjNcJEnFGS6SpOIMF0lScYaLJKk4w0WSVJzhIkkqznCRJBVnuEiSijNcJEnFGS6SpOIMF0lScYaLJKk4w0WSVJzhIkkqznCRJBVnuEiSijNcJEnFGS6SpOIMF0lSccvnc6eI2AxcD6wFLs7M3VX7XuBwdQG4OzOfqmpXAg8BI8BeYENmvtqPmiSpXvPdc3kMuAbYd4LaDZl5aXWZDZYh4GHgzsxcBzwDbOpHTZJUv3mFS2buyMzxDh73cuBwZu6obm8DbuxTTZJUsxLvuTwSEbsiYmtEnFm1raFtLyczXwOGI+LsPtQkSTWb13suc7g6M8cjYgVwH7AF2LDw1eqPsbFVXfVrtUYLr8nSMujjB+egF5banC619e21BYXL7KGyzDwSEVuBJ6rSfuCC2ftFxDnAdGYeiIhaa52MZ3LyEFNT0510odUaZWLiYEd9mmTQxw/NnoN+vmAupTlt8jYwl+HhoZP+Ud71YbGIOD0izqiuDwE3Azur8nPASERcVd3eCDzap5okqWbzCpeIuD8i/gC8G3g6Il4AzgW2R8QuYDewDrgDIDOngFuBByNiD3AtcE8/apKk+g1NT3d2GKih1gIveVisc4M+fmj2HLRao6y/6/Hal/vkvdctqTlt8jYwl7bDYu9h5vuFf6v1Y4UkSc1muEiSijNcJEnFGS6SpOIMF0lScYaLJKk4w0WSVJzhIkkqznCRJBVnuEiSijNcJEnFGS6SpOIMF0lScYaLJKk4w0WSVJzhIkkqznCRJBVnuEiSijNcJEnFGS6SpOIMF0lSccvnc6eI2AxcD6wFLs7M3RExBnwPeC9wBHgR+GxmTlR9poHnganqYW7NzOer2nrg69XynwM+nZl/6lVNklSv+e65PAZcA+xra5sGvpaZkZmXAL8DNh3X70OZeWl1mQ2WVcC3gPWZ+T7gIPCFXtUkSfWbV7hk5o7MHD+u7UBmbm9r+hlwwTwe7qPALzNzT3V7G3BTD2uSpJrN67DYqUTEMPA54InjStsjYjnwY+ArmXkEWMOxe0D7gfOr672ozdvY2KpOuwDQao121a8pBn384Bz0wlKb06W2vr1WJFyAbwKHgC1tbWsyczwiVjPz3syXgS8VWl5PTE4eYmpquqM+rdYoExMHe7RGi9+gjx+aPQf9fMFcSnPa5G1gLsPDQyf9o3zBnxar3uy/ELgpM2ffvGf2MFpmvgF8G/hwVdrPsYfP1gDjPaxJkmq2oHCJiK8ClwEfrw55zbafFREj1fXlwA3Azqr8E+CKiLiwur0ReLSHNUlSzeYVLhFxf0T8AXg38HREvBARFwFfBM4Dno2InRHxg6rL+4GfR8RvgF3AW8wcFiMzDwKfAX4YES8CZwCbe1WTJNVvaHq6s/cYGmot8JLvuXRu0McPzZ6DVmuU9Xc9Xvtyn7z3uiU1p03eBubS9p7Le4C9x9T6sUKSpGYzXCRJxRkukqTiDBdJUnGGiySpOMNFklSc4SJJKs5wkSQVV+rElZJUzJ/fOtqXk2YePvIXDr7xZu3LbSLDRdKic9rblvXtzACD9z373vCwmCSpOMNFklSc4SJJKs5wkSQVZ7hIkoozXCRJxRkukqTiDBdJUnGGiySpOMNFklTcKU//EhGbgeuBtcDFmbm7al8HfBcYAyaBT2XmnsVWkyTVbz57Lo8B1wD7jmvfBjyQmeuAB4CHFmlNklSzU+65ZOYOgIj4a1tEvB34IPCRqun7wJaIaAFDi6WWmRPzmwZJUkndnhX5fODlzDwKkJlHI+KPVfvQIqp1FC5jY6u6mox+nBp8MRn08YNz0CTdPpduA8fylPttJicPMTU13VGfVmuUiYnBPUn3oI8fmj0Hg/iC2c1z2eRtYC7Dw0Mn/aO820+LjQPviohlANXP86r2xVSTJPVBV+GSma8CO4FbqqZbgF9n5sRiqnUzNknSws3no8j3A58A3gE8HRGTmXkRsBH4bkT8O/A68Km2boupJkmq2Xw+LfZ54PMnaP9v4J9O0mfR1CRJ9fMb+pKk4gwXSVJxhoskqTjDRZJUnOEiSSrOcJEkFWe4SJKKM1wkScUZLpKk4gwXSVJxhoskqTjDRZJUnP8sTFrkRlePsHKFv6paWtxipUVu5YrlrL/r8b4s+8l7r+vLcrX0eVhMklSc4SJJKs5wkSQVZ7hIkoozXCRJxRkukqTiFvRR5IhYCzzW1nQmsDozz46IvcDh6gJwd2Y+VfW7EngIGAH2Ahsy89Ve1SRJ9VrQnktm7s3MS2cvzATNf7bd5Ya2+mywDAEPA3dm5jrgGWBTr2qSpPoVOywWEacBnwS+c4q7Xg4czswd1e1twI09rEmSalbyPZePAS9n5q/a2h6JiF0RsTUizqza1gD7Zu+Qma8BwxFxdo9qkqSalTz9y+0cu9dydWaOR8QK4D5gC7Ch4PKKGxtb1VW/Vmu08JosLYM+fnAOmqTb59Jt4FhFwiUizgOuBW6dbcvM8ernkYjYCjxRlfYDF7T1PQeYzswDEVG81sk4JicPMTU13UkXWq1RJiYOdtSnSQZ9/ND7OfBFq17dPJeD+nswPDx00j/KSx0Wuw34UWZOAkTE6RFxRnV9CLgZ2Fnd9zlgJCKuqm5vBB7tYU2SVLOS4dJ+SOxcYHtE7AJ2A+uAOwAyc4qZPZwHI2IPM3s89/SqJkmqX5HDYtXHf9tv/x74wBz3fxa4uK6aJKlefkNfklSc4SJJKs5wkSQVZ7hIkoozXCRJxRkukqTiDBdJUnGGiySpOMNFklSc4SJJKs5wkSQVV/L/uUiNNrp6hJUrTvwr42nxpWMZLtI8rVyxnPV3PV77cp+897ralyktlIfFJEnFGS6SpOIMF0lScYaLJKk4w0WSVJzhIkkqznCRJBW34O+5RMRe4HB1Abg7M5+KiCuBh4ARYC+wITNfrfrUWpMk1avUnssNmXlpdXkqIoaAh4E7M3Md8AywCaDumiSpfr06LHY5cDgzd1S3twE39qkmSapZqXB5JCJ2RcTWiDgTWAPsmy1m5mvAcESc3YeaJKlmJc4tdnVmjkfECuA+YAvwgwKPW7uxsVVd9Rv0kxYO+vjVLN1uz/4eHGvB4ZKZ49XPIxGxFXgC+AZwwex9IuIcYDozD0TE/jprnYxlcvIQU1PTnXSh1RplYuJgR32aZJDG74vHYOhmex6k34N2w8NDJ/2jfEGHxSLi9Ig4o7o+BNwM7ASeA0Yi4qrqrhuBR6vrddckSTVb6Hsu5wLbI2IXsBtYB9yRmVPArcCDEbEHuBa4B6DumiSpfgs6LJaZvwc+cJLas8DFi6EmSaqX39CXJBVnuEiSijNcJEnFGS6SpOIMF0lScYaLJKk4w0WSVJzhIkkqznCRJBVnuEiSijNcJEnFGS6SpOIMF0lScYaLJKk4w0WSVJzhIkkqznCRJBVnuEiSijNcJEnFGS6SpOKWL6RzRIwB3wPeCxwBXgQ+m5kTETENPA9MVXe/NTOfr/qtB75eLf854NOZ+ade1SRJ9Vronss08LXMjMy8BPgdsKmt/qHMvLS6zAbLKuBbwPrMfB9wEPhCr2qSpPotKFwy80Bmbm9r+hlwwSm6fRT4ZWbuqW5vA27qYU2SVLMFHRZrFxHDwOeAJ9qat0fEcuDHwFcy8wiwBtjXdp/9wPnV9V7UJEk1KxYuwDeBQ8CW6vaazByPiNXMvC/zZeBLBZdX3NjYqq76tVqjhddkaRn08atZut2e/T04VpFwiYjNwIXMvOcxBZCZ49XPNyLi28C/VXffD/xzW/c1wHgPa/M2OXmIqanpjvq0WqNMTBzsdFGNMUjj98VjMHSzPQ/S70G74eGhk/5RvuCPIkfEV4HLgI9Xh72IiLMiYqS6vhy4AdhZdfkJcEVEXFjd3gg82sOaJKlmCwqXiLgI+CJwHvBsROyMiB8A7wd+HhG/AXYBbzFzWIzMPAh8BvhhRLwInAFs7lVNklS/BR0Wy8wXgKGTlC+Zo9/jwON11SRJ9fIb+pKk4gwXSVJxhoskqTjDRZJUnOEiSSrOcJEkFWe4SJKKM1wkScUZLpKk4kqeFVmSlrQ/v3W0b2dFPnzkLxx8480FPcZiYrhIUuW0ty1j/V39OYvUk/deR5POq+xhMUlSce65aEkZXT3CyhVuttJi52+plpSVK5b39bCFpPnxsJgkqTjDRZJUnOEiSSrOcJEkFWe4SJKKM1wkScUZLpKk4hr1PZeIWAd8FxgDJoFPZeae/q6VJA2epu25bAMeyMx1wAPAQ31eH0kaSI3Zc4mItwMfBD5SNX0f2BIRrcycOEX3ZQDDw0NdLbvbfk1R9/jfftZIrctbDMt2zM1fLiy915K29V12fG1oenq63rXpkYi4DPiPzLyore23wIbM/NUpul8F/LSX6ydJDXY1sKO9oTF7Lgv0C2Ym5xXgaJ/XRZKWimXAO5l5DT1Gk8JlHHhXRCzLzKMRsQw4r2o/lSMcl7qSpHn53YkaG/OGfma+CuwEbqmabgF+PY/3WyRJhTXmPReAiHg/Mx9FPgt4nZmPImd/10qSBk+jwkWStDg05rCYJGnxMFwkScUZLpKk4gwXSVJxTfqeS60G8SSZEbEXOFxdAO7OzKci4kpmzuM2Auxl5qwIr/ZjHUuKiM3A9cBa4OLM3F21n/S5b9p2Mccc7OUE20JVa8z2EBFjwPeA9zLzfbgXgc9m5sRc42zSHHTLPZfuDepJMm/IzEury1MRMQQ8DNxZzcUzwKb+rmIxjwHXAPuOa5/ruW/adnGyOYDjtgWABm4P08DXMjMy8xJmvjC4aa5xNnAOumK4dKHtJJnfr5q+D3wwIlr9W6u+uRw4nJmzZzjYBtzYx/UpJjN3ZOYxZ3iY67lv4nZxojk4hUZtD5l5IDO3tzX9DLiAucfZqDnoluHSnfOBlzPzKED1849Ve9M9EhG7ImJrRJwJrKHtr9rMfA0Yjoiz+7aGvTXXcz9o28Xx2wI0eHuIiGHgc8ATzD3Oxs5BJwwXdeLqzPxH4ApgCNjS5/VR/wzitvBN4BCDMdYFM1y689eTZAJ0eJLMJWv28EhmHgG2Ah8G9jNzmACAiDgHmM7MA31Zyd6b67kfmO3iJNsCNHR7qD7YcCFwU2ZOMfc4GzkHnTJcujCIJ8mMiNMj4ozq+hBwMzNz8BwwEhFXVXfdCDzan7Xsvbme+0HZLubYFqCB20NEfBW4DPh4FaYw9zgbNwfd8NxiXRq0k2RGxD8A/8XM/29YBvwW+HxmvhIRH2LmU1Er+dvHLv+3X+taSkTcD3wCeAfwGjCZmRfN9dw3bbs40RwA6znJtlD1acz2EBEXAbuB/wHerJpfysx/nWucTZqDbhkukqTiPCwmSSrOcJEkFWe4SJKKM1wkScUZLpKk4gwXSVJxhoskqTjDRZJU3P8DbP/eS/l1VCAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=[]\n",
    "cn=0\n",
    "for i in range(len(df_pairs)):\n",
    "    p1=graphs_location+df_pairs['graph.x'][i]\n",
    "    p2=graphs_location+df_pairs['graph.y'][i]\n",
    "    graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "    graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "    ged.append(FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2)))\n",
    "    df_pairs['value'][i]=ged[i]\n",
    "    if (i%(0.1*len(df_pairs))==0):\n",
    "        cn+=1\n",
    "    print(\"Pairs: \"+str(i+1)+\"/\"+str(len(df_pairs))+\" [\"+\"-\"*(cn+1), end=\"\\r\", flush=True)\n",
    "    np.savetxt(\"samples_all_ged.csv\", ged)    \n",
    "print(\"Steps: \"+str(g+1)+\"/\"+str(pr_steps)+\"[\"+\"-\"*(g+1)+\"]\", end=\"\\r\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ged=np.array(ged)\n",
    "np.save(\"samples_all_ged.npy\", ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_loop(i,df_pairs):\n",
    "    p1=graphs_location+df_pairs['graph.x'][i]\n",
    "    p2=graphs_location+df_pairs['graph.y'][i]\n",
    "    graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "    graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "    g=FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "inputs = list(range(len(df_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_list = Parallel(n_jobs=num_cores)(delayed(one_loop)(i,df_pairs) for i in inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"samples_all_ged.csv\", processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"samples_all_ged.npy\", np.array(processed_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pairs['value']= processed_list \n",
    "df_pairs.to_csv(\"df_pairs2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028229713439941406\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "ged=[]\n",
    "p1=graphs_location+list(samples_all['path_list'])[0]\n",
    "p2=graphs_location+list(samples_all['path_list'])[1]\n",
    "graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "ged.append(FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2)))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.46539306640625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(end - start)*16000000/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pairs\n",
    "df_pairs=pd.read_csv(\"\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ged=[]\n",
    "for i in range(len(df_pairs)):\n",
    "    p1=graphs_location+df_pairs['path_list1'][i]\n",
    "    p2=graphs_location+df_pairs['path_list2'][i]\n",
    "    graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "    graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "    ged.append(FastGEDEstimate(df_to_nx(graph_df1),df_to_nx(graph_df2)))\n",
    "    if i%int(0.1*len(df_pairs))==0:\n",
    "        print('Running: %s'%(i/len(df_pairs)))\n",
    "np.save(\"samples_all_ged.npy\", np.array(ged))\n",
    "    #X_prots, X_edgeattr, X_edges=tensorize_signalnet(df_to_nx(graph_df),prot_dict,100,21,shapes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"samples_all_ged.csv\", ged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_signalnet(netx_list,prot_dict,max_prots,max_degree,shapes_dict):\n",
    "    import numpy as np\n",
    "    n = len(netx_list)\n",
    "    X_prots= np.zeros((n,max_prots,shapes_dict['num_prot_features']),dtype='float32')\n",
    "    Χ_edgattr=np.zeros((n,max_prots,max_degree,shapes_dict['num_edge_features']),dtype='float32')\n",
    "    X_edges=np.ones((n,max_prots,max_degree),dtype='int32')*int(-1)\n",
    "    for idx,netx in enumerate(netx_list):\n",
    "        for i,p in enumerate(list(netx.nodes())):\n",
    "            l=list(prot_dict[p])\n",
    "            l.append(netx.nodes[p]['act'])\n",
    "            X_prots[idx][i]=np.array(l)\n",
    "            inds=[list(netx.nodes()).index(x) for x in list(netx[p])[0:max_degree]]\n",
    "            attrs=[netx[p][x]['weight'] for x in list(netx[p])[0:max_degree]]\n",
    "            if len(inds)<max_degree:\n",
    "                for j in range(max_degree-len(inds)):\n",
    "                    inds.append(-1)\n",
    "                    attrs.append(0)\n",
    "            X_edges[idx][i]=np.array(inds)\n",
    "            Χ_edgattr[idx][i]=np.array(attrs).reshape(21,1)\n",
    "    \n",
    "    return X_prots,Χ_edgattr,X_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/biolab/miniconda3/envs/tf1/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df_pairs=pd.read_csv(\"df_pairs.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[graphs_location+x for x in list(df_pairs['graph.x'])[0:10]]\n",
    "graph_df=[pd.read_csv(x,index_col=0).reset_index(drop=True) for x in p]\n",
    "gg=[df_to_nx(x) for x in graph_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prots, X_edgeattr, X_edges= tensorize_signalnet(gg,prot_dict,100,21,shapes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prots=100\n",
    "pax_degree=21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net1=[]\n",
    "for i in range(10):\n",
    "    p1=graphs_location+samples_all['path_list'][i]\n",
    "    graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "    net1.append(df_to_nx(graph_df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo=[tensorize_signalnet(net,prot_dict,max_prots,pax_degree,shapes_dict) for net in net1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prots= np.array([x[0] for x in yolo])\n",
    "X_edgeattr= np.array([x[1] for x in yolo])\n",
    "X_edges=[x[2] for x in yolo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prots=np.array(X_prots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100, 385)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(bs,df,graphs_location,shapes_dict,prot_dict,max_prots,max_degree):\n",
    "    import numpy as np\n",
    "    counter=int(0)\n",
    "    #Keep looping indefinetely\n",
    "    while True:\n",
    "        \n",
    "        #Initialize batches of inputs and outputs\n",
    "        net1 = []\n",
    "        net2 = []\n",
    "        \n",
    "        d=[]\n",
    "        \n",
    "        #Keep looping until we reach batch size\n",
    "        while len(net1)<=bs: #doesn't matter if it is smi1 or smi2 since they have the same len\n",
    "            \n",
    "            # check to see if you reached the end of the frame\n",
    "            if counter==len(df):\n",
    "                counter=int(0)\n",
    "                df = df.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            p1=graphs_location+df['graph.x'][counter]\n",
    "            p2=graphs_location+df['graph.y'][counter]\n",
    "            graph_df1=pd.read_csv(p1,index_col=0).reset_index(drop=True)\n",
    "            graph_df2=pd.read_csv(p2,index_col=0).reset_index(drop=True)\n",
    "            net1.append(df_to_nx(graph_df1))\n",
    "            net2.append(df_to_nx(graph_df2))\n",
    "            \n",
    "            d.append(df.value[counter])\n",
    "            counter+=1\n",
    "            \n",
    "        #l=[tensorize_signalnet(net,prot_dict,max_prots,max_degree,shapes_dict) for net in net1]\n",
    "        #X_prots= np.array([x[0] for x in l])\n",
    "        #X_edgeattr= np.array([x[1] for x in l])\n",
    "        #X_edges=np.array([x[2] for x in l])\n",
    "        prot_1,edgattr_1,edge_1=tensorize_signalnet(net1,prot_dict,max_prots,max_degree,shapes_dict)\n",
    "        #prot_1=np.array(X_prots,dtype = 'float32')\n",
    "        #edgattr_1=np.array(X_edgeattr[ind1],dtype = 'float32')\n",
    "        #edge_1=np.array(X_edges,dtype = 'int32')\n",
    "        #l=[tensorize_signalnet(net,prot_dict,max_prots,max_degree,shapes_dict) for net in net2]\n",
    "        #X_prots= np.array([x[0] for x in l])\n",
    "        #X_edgeattr= np.array([x[1] for x in l])\n",
    "        #X_edges=np.array([x[2] for x in l])\n",
    "        prot_2,edgattr_2,edge_2=tensorize_signalnet(net2,prot_dict,max_prots,max_degree,shapes_dict)\n",
    "        #prot_2=np.array(X_prots,dtype = 'float32')\n",
    "        #edgattr_2=np.array(X_edgeattr,dtype = 'float32')\n",
    "        #edge_2=np.array(X_edges,dtype = 'int32')\n",
    "        \n",
    "        # yield the batch to the calling function\n",
    "        yield ({'prot_inputs_1':prot_1,'edgattr_inputs_1':edgattr_1,'edge_inputs_1':edge_1,'prot_inputs_2':prot_2,\n",
    "                'edgattr_inputs_2':edgattr_2,'edge_inputs_2':edge_2},np.array(d,dtype = 'float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##not ready yet\n",
    "def preds_generator(bs,df_emb,prot_dict,max_prots,max_degree,shapes_dict,model_net):\n",
    "    \n",
    "    import numpy as np\n",
    "    counter=int(0)\n",
    "    #Keep looping indefinetely\n",
    "    while counter<len(df_emb):\n",
    "        \n",
    "        #Initialize batches of inputs and outputs\n",
    "        net = []\n",
    "        \n",
    "        \n",
    "        #Keep looping until we reach batch size\n",
    "        while len(net)<=bs: #doesn't matter if it is smi1 or smi2 since they have the same len\n",
    "            \n",
    "            # check to see if you reached the end of the frame\n",
    "            if counter==len(df_cold):\n",
    "                break\n",
    "                \n",
    "            p=graphs_location+df_emb['files_combined'][counter]\n",
    "            graph_df=pd.read_csv(p,index_col=0).reset_index(drop=True)\n",
    "            net.append(df_to_nx(graph_df1))\n",
    "            counter+=1\n",
    "    \n",
    "            \n",
    "        prot,edgattr,edge=tensorize_signalnet(net,prot_dict,max_prots,max_degree,shapes_dict)\n",
    "        \n",
    "        y_pred=model_net.predict([prot,edgattr,edge],batch_size=bs)\n",
    "        \n",
    "        yield(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prots = 100\n",
    "max_degree = 21\n",
    "num_prot_features = shapes_dict['num_prot_features']\n",
    "num_edge_features = shapes_dict['num_edge_features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary and custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "def get_cindex(y_true, y_pred):\n",
    "    g = tf.subtract(tf.expand_dims(y_pred, -1), y_pred)\n",
    "    g = tf.cast(g == 0.0, tf.float32) * 0.5 + tf.cast(g > 0.0, tf.float32)\n",
    "\n",
    "    f = tf.subtract(tf.expand_dims(y_true, -1), y_true) > 0.0\n",
    "    f = tf.matrix_band_part(tf.cast(f, tf.float32), -1, 0)\n",
    "\n",
    "    g = tf.reduce_sum(tf.multiply(g, f))\n",
    "    f = tf.reduce_sum(f)\n",
    "\n",
    "    return tf.where(tf.equal(g, 0), 0.0, g/f)\n",
    "\n",
    "def pearson_r(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x, axis=0)\n",
    "    my = K.mean(y, axis=0)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = K.sum(xm * ym)\n",
    "    x_square_sum = K.sum(xm * xm)\n",
    "    y_square_sum = K.sum(ym * ym)\n",
    "    r_den = K.sqrt(x_square_sum * y_square_sum)\n",
    "    r = r_num / r_den\n",
    "    return K.mean(r)\n",
    "\n",
    "def mse_sliced(y_true,y_pred):\n",
    "    condition = tf.math.less_equal(y_pred,0.2)\n",
    "    indices = tf.where(condition)\n",
    "    slice_true = tf.gather_nd(y_true,indices)\n",
    "    slice_pred = tf.gather_nd(y_pred,indices)\n",
    "    mse_sliced = K.mean(K.square(slice_pred - slice_true), axis=-1)\n",
    "    return mse_sliced\n",
    "\n",
    "def custom_accuracy(y_true,y_pred):\n",
    "    cat_true = tf.math.less_equal(y_true,0.2)\n",
    "    cat_pred = tf.math.less_equal(y_pred,0.2)\n",
    "    casted_true = K.cast(cat_true,\"int32\")\n",
    "    casted_pred = K.cast(cat_pred,\"int32\")\n",
    "    acc = K.mean(K.equal(casted_true, K.round(casted_pred)))\n",
    "    return acc\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Computes the precision over the whole batch using threshold_value.\n",
    "        \"\"\"\n",
    "        cat_true = tf.math.less_equal(y_true,0.2)\n",
    "        casted_true = K.cast(cat_true,\"float32\")\n",
    "        cat_pred = tf.math.less_equal(y_pred,0.2)\n",
    "        casted_pred = K.cast(cat_pred,\"float32\")\n",
    "        # Compute the number of true positives.\n",
    "        true_positives = K.sum(tf.math.multiply(casted_true , casted_pred ))\n",
    "        # count the predicted positives\n",
    "        predicted_positives = K.sum(casted_pred)\n",
    "        # Get the precision ratio\n",
    "        precision_ratio = tf.math.divide(true_positives , predicted_positives + K.epsilon() )\n",
    "        return precision_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(sigma):\n",
    "    def gaussian_loss(y_true, y_pred):\n",
    "        return tf.reduce_mean(0.5*tf.log(sigma) + 0.5*tf.div(tf.square(y_true - y_pred), sigma)) + 1e-6\n",
    "    return gaussian_loss\n",
    "\n",
    "class GaussianLayer(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(GaussianLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.kernel_1 = self.add_weight(name='kernel_1', \n",
    "                                      shape=(int(input_shape[-1]), self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.kernel_2 = self.add_weight(name='kernel_2', \n",
    "                                      shape=(int(input_shape[-1]), self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.bias_1 = self.add_weight(name='bias_1',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        self.bias_2 = self.add_weight(name='bias_2',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        super(GaussianLayer, self).build(input_shape) \n",
    "    def call(self, x):\n",
    "        output_mu  = K.dot(x, self.kernel_1) + self.bias_1\n",
    "        output_sig = K.dot(x, self.kernel_2) + self.bias_2\n",
    "        output_sig_pos = K.log(1 + K.exp(output_sig)) + 1e-06  \n",
    "        return [output_mu, output_sig_pos]\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0], self.output_dim), (input_shape[0], self.output_dim)]\n",
    "\n",
    "class ConGaussianLayer(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(ConGaussianLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.kernel_1 = self.add_weight(name='kernel_1', \n",
    "                                      shape=(int(input_shape[-1]), self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.kernel_2 = self.add_weight(name='kernel_2', \n",
    "                                      shape=(int(input_shape[-1]), self.output_dim),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        self.bias_1 = self.add_weight(name='bias_1',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        self.bias_2 = self.add_weight(name='bias_2',\n",
    "                                    shape=(self.output_dim, ),\n",
    "                                    initializer=glorot_normal(),\n",
    "                                    trainable=True)\n",
    "        super(ConGaussianLayer, self).build(input_shape) \n",
    "    def call(self, x):\n",
    "        output_mu  = K.dot(x, self.kernel_1) + self.bias_1\n",
    "        output_mu  = keras.layers.ReLU(max_value=1)(output_mu)\n",
    "        output_sig = K.dot(x, self.kernel_2) + self.bias_2\n",
    "        output_sig_pos = K.log(1 + K.exp(output_sig)) + 1e-06 \n",
    "        output_sig_pos  = keras.layers.ReLU(max_value=1)(output_sig_pos) + 1e-06 \n",
    "        return [output_mu, output_sig_pos]\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0], self.output_dim), (input_shape[0], self.output_dim)]\n",
    "\n",
    "\n",
    "def euclidean_distance(x,y):\n",
    "    #x, y = vects\n",
    "    #eucl=tf.sqrt(tf.math.reduce_sum(tf.square(tf.subtract(x,y),\"Square\"),\"Sqrt\"),axis=1,keepdims=True)\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "    #return eucl\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "class Attention_theta(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention_theta, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.theta = self.add_weight(name='theta', \n",
    "                                      shape=(int(input_shape[-1]), int(input_shape[-1])),\n",
    "                                      initializer=glorot_normal(),\n",
    "                                      trainable=True)\n",
    "        super(Attention_theta, self).build(input_shape) \n",
    "    def call(self, x):\n",
    "        import numpy as np\n",
    "        multi1  = K.dot(tf.math.reduce_mean(x,axis=1),self.theta)\n",
    "        #multi1=tf.transpose(multi1)\n",
    "        #print(K.int_shape(multi1))\n",
    "        multi1=Activation('relu')(multi1)\n",
    "        multi2  = K.batch_dot(x,multi1,axes=(2,1))\n",
    "        multi2=Activation('sigmoid')(multi2)\n",
    "        #print(K.int_shape(multi2))\n",
    "        #print(K.int_shape(x))\n",
    "        output_emb = K.batch_dot(multi2,x,axes=1)\n",
    "        output_emb=keras.layers.Reshape((1,K.int_shape(output_emb)[1]))(output_emb)\n",
    "        #print(K.int_shape(output_emb))\n",
    "        return output_emb\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],1,int(input_shape[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {'lr': 0.0001,\n",
    "     'nfilters': int(32),\n",
    "     'size': int(8),\n",
    "     'conv_width' : 128,\n",
    "     'fp_length' : 256,\n",
    "     'size_drug_1' : 8,\n",
    "     'size_drug_2' : 4,\n",
    "     'size_protein_1' : 8,\n",
    "     'size_protein_2' : 16,\n",
    "     'size_protein_3' : 3,\n",
    "     'batch_size': int(128),\n",
    "     'dense_size': int(256),\n",
    "     'dense_size_2': 512,\n",
    "     'dropout': 0.25,\n",
    "     'l2reg': 0.01}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(y_pred,Y_cold,th,df_cold):\n",
    "    true = np.reshape(Y_cold,len(df_cold))\n",
    "    pred = np.reshape(y_pred,len(df_cold))\n",
    "    cor = np.corrcoef(true,pred)\n",
    "    mse_all = sklearn.metrics.mean_squared_error(true,pred)\n",
    "    # calculate mse of similars\n",
    "    mse_sims = sklearn.metrics.mean_squared_error(true[pred<=th],pred[pred<=th])\n",
    "    # turn to categorical to calculate precision and accuracy\n",
    "    true_cat = true <= th\n",
    "    pred_cat = pred <= th\n",
    "    pos = np.sum(pred_cat)\n",
    "    prec = precision_score(true_cat,pred_cat)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(true_cat,pred_cat)\n",
    "    result =pd.DataFrame({'cor' : cor[0,1], 'mse_all' : mse_all, 'mse_similars' : mse_sims,'precision': prec, 'accuracy': acc,\n",
    "                         'positives' : pos}, index=[0])\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_nikos(params, lr_value, conv_width, fp_length):\n",
    "        \n",
    "    ### encode smiles\n",
    "    \n",
    "    prots0 = Input(name='prot_inputs', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "    edgattrs = Input(name='edgattr_inputs', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "    edges = Input(name='edge_inputs', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "    #prosoxi arxika to kano 256 ton pinaka\n",
    "    g1 = NeuralGraphHidden(conv_width , activ = None, bias = True , init = 'glorot_normal')([prots0,edgattrs,edges])\n",
    "    g1 = BatchNormalization(momentum=0.6)(g1)\n",
    "    g1 = Activation('relu')(g1)\n",
    "    g1=keras.layers.Dropout(0.4)(g1)\n",
    "\n",
    "    g2 = NeuralGraphHidden(int(conv_width/2) , activ = None, bias = True , init = 'glorot_normal')([g1,edgattrs,edges])\n",
    "    g2 = BatchNormalization(momentum=0.6)(g2)\n",
    "    g2 = Activation('relu')(g2)\n",
    "    g2=keras.layers.Dropout(0.4)(g2)\n",
    "\n",
    "    g3 = NeuralGraphHidden(int(conv_width/2) , activ = None, bias = True , init = 'glorot_normal')([g2,edgattrs,edges])\n",
    "    g3 = BatchNormalization(momentum=0.6)(g3)\n",
    "    g3 = Activation('relu')(g3)\n",
    "\n",
    "\n",
    "\n",
    "    g4=keras.layers.Conv1D(128,37, activation=None, use_bias=False, kernel_initializer='glorot_uniform')(g3)\n",
    "    g4= BatchNormalization(momentum=0.6)(g4)\n",
    "    g4 = Activation('relu')(g4)\n",
    "    g4=keras.layers.Dropout(0.3)(g4)\n",
    "    \n",
    "    g5=keras.layers.Conv1D(128,33, activation=None, use_bias=False, kernel_initializer='glorot_uniform')(g4)\n",
    "    g5= BatchNormalization(momentum=0.6)(g5)\n",
    "    g5 = Activation('relu')(g5)\n",
    "    g5=keras.layers.Dropout(0.3)(g5)\n",
    "    \n",
    "    g6=keras.layers.Conv1D(128,32, activation=None, use_bias=False, kernel_initializer='glorot_uniform')(g5)\n",
    "    g6= BatchNormalization(momentum=0.6)(g6)\n",
    "    g6 = Activation('relu')(g6)\n",
    "    g6=keras.layers.Dropout(0.3)(g6)\n",
    "    g6=keras.layers.Flatten()(g6)\n",
    "\n",
    "\n",
    "    interactionModel = keras.Model(inputs=[prots0, edgattrs, edges], outputs= g6)\n",
    "\n",
    "    print(interactionModel.summary())\n",
    "    return interactionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "prot_inputs (InputLayer)        (None, 100, 1040)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edgattr_inputs (InputLayer)     (None, 100, 21, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs (InputLayer)        (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "neural_graph_hidden_21 (NeuralG (None, 100, 256)     5601792     prot_inputs[0][0]                \n",
      "                                                                 edgattr_inputs[0][0]             \n",
      "                                                                 edge_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 100, 256)     1024        neural_graph_hidden_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 100, 256)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 100, 256)     0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "neural_graph_hidden_22 (NeuralG (None, 100, 128)     693504      dropout_25[0][0]                 \n",
      "                                                                 edgattr_inputs[0][0]             \n",
      "                                                                 edge_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 100, 128)     512         neural_graph_hidden_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 100, 128)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 100, 128)     0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "neural_graph_hidden_23 (NeuralG (None, 100, 128)     349440      dropout_26[0][0]                 \n",
      "                                                                 edgattr_inputs[0][0]             \n",
      "                                                                 edge_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 100, 128)     512         neural_graph_hidden_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 100, 128)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 64, 128)      606208      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 64, 128)      512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 64, 128)      0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 64, 128)      0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 32, 128)      540672      dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 32, 128)      512         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 32, 128)      0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 32, 128)      0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1, 128)       524288      dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 1, 128)       512         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 1, 128)       0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 1, 128)       0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 128)          0           dropout_29[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 8,319,488\n",
      "Trainable params: 8,317,696\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "atom_inputs_1 (InputLayer)      (None, 100, 1040)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edgattr_inputs_1 (InputLayer)   (None, 100, 21, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_1 (InputLayer)      (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "atom_inputs_2 (InputLayer)      (None, 100, 1040)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edgattr_inputs_2 (InputLayer)   (None, 100, 21, 1)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "edge_inputs_2 (InputLayer)      (None, 100, 21)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Model)                 (None, 128)          8319488     atom_inputs_1[0][0]              \n",
      "                                                                 edgattr_inputs_1[0][0]           \n",
      "                                                                 edge_inputs_1[0][0]              \n",
      "                                                                 atom_inputs_2[0][0]              \n",
      "                                                                 edgattr_inputs_2[0][0]           \n",
      "                                                                 edge_inputs_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 128)          0           model_9[1][0]                    \n",
      "                                                                 model_9[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (ConGaussianLayer)  [(None, 1), (None, 1 258         lambda_5[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,319,746\n",
      "Trainable params: 8,317,954\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'IKBKAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-acf6abddeea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                     \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                                     callbacks= [term, rlr])\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     )\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf1/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \"\"\"\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-d60ef4637a3a>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(bs, df, graphs_location, shapes_dict, prot_dict, max_prots, max_degree)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#X_edgeattr= np.array([x[1] for x in l])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#X_edges=np.array([x[2] for x in l])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mprot_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medgattr_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorize_signalnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprot_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_prots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_degree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshapes_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m#prot_1=np.array(X_prots,dtype = 'float32')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#edgattr_1=np.array(X_edgeattr[ind1],dtype = 'float32')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-62af45b9f066>\u001b[0m in \u001b[0;36mtensorize_signalnet\u001b[0;34m(netx_list, prot_dict, max_prots, max_degree, shapes_dict)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprot_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mX_prots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'IKBKAP'"
     ]
    }
   ],
   "source": [
    "# Initialize encoder\n",
    "encoder_nikos = enc_nikos(p, 0.001, 256, 256)\n",
    "\n",
    "# Initialize model\n",
    "prots0_1 = Input(name='atom_inputs_1', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "edgattr_1 = Input(name='edgattr_inputs_1', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "edges_1 = Input(name='edge_inputs_1', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "prots0_2 = Input(name='atom_inputs_2', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "edgattr_2 = Input(name='edgattr_inputs_2', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "edges_2 =Input(name='edge_inputs_2', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "encoded_1 = encoder_nikos([prots0_1,edgattr_1,edges_1])\n",
    "#encoded_1 =keras.layers.Flatten()(encoded_1)\n",
    "encoded_2 = encoder_nikos([prots0_2,edgattr_2,edges_2])\n",
    "#encoded_2 = keras.layers.Flatten()(encoded_2)\n",
    "\n",
    "L1_layer = keras.layers.Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "L1_distance = L1_layer([encoded_1, encoded_2])\n",
    "\n",
    "mu, sigma = ConGaussianLayer(1, name='main_output')(L1_distance)\n",
    "\n",
    "siamese_net = Model(inputs=[prots0_1,edgattr_1,edges_1,prots0_2,edgattr_2,edges_2],outputs=mu)\n",
    "print(siamese_net.summary())\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "siamese_net.compile(optimizer= adam,loss= custom_loss(sigma),metrics=['mse', get_cindex, r_square, pearson_r])\n",
    "\n",
    "# Train with fitgen\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=2, min_lr=0.00001, verbose=1, min_delta=1e-5)\n",
    "term=keras.callbacks.TerminateOnNaN()\n",
    "bs=512\n",
    "NUM_EPOCHS = 5\n",
    "df_pairs = df_pairs.sample(frac=1).reset_index(drop=True)\n",
    "#Set total number of training samples and tests samples\n",
    "NUM_TRAIN = len(df_pairs)\n",
    "trainGen=train_generator(bs,df_pairs,graphs_location,shapes_dict,prot_dict,max_prots,max_degree)\n",
    "history = siamese_net.fit_generator(trainGen,\n",
    "                                    steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                                    epochs=NUM_EPOCHS,\n",
    "                                    verbose = 1,\n",
    "                                    shuffle = True,\n",
    "                                    callbacks= [term, rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with fitgen\n",
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=2, min_lr=0.00001, verbose=1, min_delta=1e-5)\n",
    "term=keras.callbacks.TerminateOnNaN()\n",
    "bs=512\n",
    "NUM_EPOCHS = 5\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "#Set total number of training samples and tests samples\n",
    "NUM_TRAIN = len(df)\n",
    "trainGen=train_generator(bs,df,smiles,X_atoms, X_bonds, X_edges)\n",
    "history = siamese_net.fit_generator(trainGen,\n",
    "                                    steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                                    epochs=NUM_EPOCHS,\n",
    "                                    verbose = 1,\n",
    "                                    shuffle = True,\n",
    "                                    callbacks= [term, rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_to_emb=pd.read_csv(\"samples_seen.csv\",index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.save_weights(\"Ugraphemp/models/siamese_model.h5\")\n",
    "encoder_nikos.save_weights(\"Ugraphemp/models/encoder_model.h5\")\n",
    "#gaussian = keras.Model(siamese_net.inputs, siamese_net.get_layer('main_output').output)\n",
    "pr_steps=ceil(len(graphs_to_emb)/2048)\n",
    "PredGen=preds_generator(2048,graphs_to_emb,prot_dict,max_prots,max_degree,shapes_dict,encoder_nikos)\n",
    "y_pred=[]\n",
    "for g in range(pr_steps):\n",
    "    y_pred=y_pred+list(next(PredGen))\n",
    "y_pred=np.array(y_pred)\n",
    "np.save(\"Ugraphemp/models/ugraph_seen_emb1.npy\",y_pred)\n",
    "np.savetxt(\"Ugraphemp/models/ugraph_seen_emb1.csv\",y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.save_weights(\"Ugraphemp/models/siamese_model.h5\")\n",
    "encoder_nikos.save_weights(\"Ugraphemp/models/encoder_model.h5\")\n",
    "#gaussian = keras.Model(siamese_net.inputs, siamese_net.get_layer('main_output').output)\n",
    "pr_steps=ceil(len(samples_all)/2048)\n",
    "PredGen=preds_generator(2048,samples_all,prot_dict,max_prots,max_degree,shapes_dict,encoder_nikos)\n",
    "y_pred=[]\n",
    "for g in range(pr_steps):\n",
    "    y_pred=y_pred+list(next(PredGen))\n",
    "y_pred=np.array(y_pred)\n",
    "np.save(\"Ugraphemp/models/ugraph_samples_all_emb1.npy\",y_pred)\n",
    "np.savetxt(\"Ugraphemp/models/ugraph_samples_all_emb1.csv\",y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for pravious projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [4]:\n",
    "    \n",
    "    #Load data\n",
    "    df = pd.read_csv(\"a375/\" + \"fold_%s/train_%s.csv\" %(i+1,i+1),index_col=0).reset_index(drop=True)\n",
    "    smiles=list(set(list(df['rdkit.x'])+list(df['rdkit.y'])))\n",
    "    X_atoms, X_bonds, X_edges = tensorise_smiles(smiles, max_degree, max_atoms)\n",
    "    df_cold = pd.read_csv(\"a375/\" + \"fold_%s/val_%s.csv\" %(i+1,i+1),index_col=0).reset_index(drop=True)\n",
    "    smiles_cold = list(set(list(df_cold['rdkit.x'])+list(df_cold['rdkit.y'])))\n",
    "    X_atoms_cold, X_bonds_cold, X_edges_cold = tensorise_smiles(smiles_cold, max_degree=5, max_atoms = 60)\n",
    "    Y_cold = df_cold.value/2\n",
    "    \n",
    "    n=8\n",
    "    cold_preds_mus = []\n",
    "    cold_preds_sigmas = []\n",
    "    while n<10:\n",
    "        \n",
    "        # Initialize encoder\n",
    "        encoder_nikos = enc_nikos(p, 0.001, 128, 256)\n",
    "    \n",
    "        # Initialize model\n",
    "        prots0_1 = Input(name='atom_inputs_1', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "        edgattr_1 = Input(name='edgattr_inputs_1', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "        edges_1 = Input(name='edge_inputs_1', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "        prots0_2 = Input(name='atom_inputs_2', shape=(max_prots, num_prot_features),dtype = 'float32')\n",
    "        edgattr_2 = Input(name='edgattr_inputs_2', shape=(max_prots, max_degree, num_edge_features),dtype = 'float32')\n",
    "        edges_2 =Input(name='edge_inputs_2', shape=(max_prots, max_degree), dtype='int32')\n",
    "\n",
    "        encoded_1 = encoder_nikos([prots0_1,edgattr_1,edges_1])\n",
    "        #encoded_1 =keras.layers.Flatten()(encoded_1)\n",
    "        encoded_2 = encoder_nikos([prots0_2,edgattr_2,edges_2])\n",
    "        #encoded_2 = keras.layers.Flatten()(encoded_2)\n",
    "\n",
    "        #K dot\n",
    "        #product_layer=keras.layers.Lambda(lambda tensors:tf.linalg.trace(K.batch_dot(tensors[0],tensors[1],axes=2)))\n",
    "        #product_layer=keras.layers.Lambda(lambda tensors:tf.reduce_sum(K.batch_dot(tensors[0],tensors[1],axes=2),[1, 2]))\n",
    "        product_layer=keras.layers.Lambda(lambda tensors:K.batch_dot(tensors[0],tensors[1],axes=2))\n",
    "        product=product_layer([encoded_1, encoded_2])\n",
    "\n",
    "        c1=keras.layers.Conv1D(128, 32, activation=None, use_bias=False, kernel_initializer='glorot_uniform')(product)\n",
    "        c1= BatchNormalization(momentum=0.6)(c1)\n",
    "        c1= Activation('relu')(c1)\n",
    "        c1=keras.layers.Dropout(0.25)(c1)\n",
    "        c1=keras.layers.Flatten()(c1)\n",
    "\n",
    "\n",
    "        mu, sigma = GaussianLayer(1, name='main_output')(c1)\n",
    "\n",
    "\n",
    "        siamese_net = Model(inputs=[prots0_1,edgattr_1,edges_1,prots0_2,edgattr_2,edges_2],outputs=mu)\n",
    "        print(siamese_net.summary())\n",
    "        adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "        siamese_net.compile(optimizer= adam,loss= custom_loss(sigma),metrics=['mse', get_cindex, r_square, pearson_r])\n",
    "\n",
    "        # Train with fitgen\n",
    "        rlr = ReduceLROnPlateau(monitor='loss', factor=0.5,patience=2, min_lr=0.00001, verbose=1, min_delta=1e-5)\n",
    "        term=keras.callbacks.TerminateOnNaN()\n",
    "        bs=128\n",
    "        NUM_EPOCHS = 20\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        #Set total number of training samples and tests samples\n",
    "        NUM_TRAIN = len(df)\n",
    "        trainGen=train_generator(bs,df,smiles,X_atoms, X_bonds, X_edges)\n",
    "        history = siamese_net.fit_generator(trainGen,\n",
    "                                            steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                                            epochs=NUM_EPOCHS,\n",
    "                                            verbose = 1,\n",
    "                                            shuffle = True,\n",
    "                                            callbacks= [term, rlr])\n",
    "        \n",
    "        if history.history[\"r_square\"][len(history.history[\"r_square\"])-1] < 0.7:\n",
    "            history = siamese_net.fit_generator(trainGen,\n",
    "                                             steps_per_epoch= ceil(NUM_TRAIN/bs),\n",
    "                                             epochs = 10,\n",
    "                                             verbose = 1,\n",
    "                                             shuffle = True,\n",
    "                                             callbacks= [term, rlr])\n",
    "        if history.history[\"r_square\"][len(history.history[\"r_square\"])-1] >= 0.7:\n",
    "            siamese_net.save_weights(\"a375/results/\" + \"fold_%s/models/\"%(i+1) + \"model_%s.h5\"%n)\n",
    "            gaussian = keras.Model(siamese_net.inputs, siamese_net.get_layer('main_output').output)\n",
    "            pr_steps=ceil(len(df_cold)/2048)\n",
    "            PredGen=preds_generator(2048,df_cold,smiles_cold,X_atoms_cold, X_bonds_cold, X_edges_cold,gaussian)\n",
    "            y_pred1=[]\n",
    "            y_pred2=[]\n",
    "            for g in range(pr_steps):\n",
    "                cold_pred=list(next(PredGen))\n",
    "                y_pred1=y_pred1+list(cold_pred[0])\n",
    "                y_pred2=y_pred2+list(cold_pred[1])\n",
    "            y_pred1=np.array(y_pred1)\n",
    "            y_pred2=np.array(y_pred2)\n",
    "            if (len(y_pred1[np.where(y_pred1 <= 0.2)])>0):\n",
    "                get = model_evaluate(y_pred1,Y_cold,0.2,df_cold)\n",
    "                get.to_csv(\"a375/results/\" + \"fold_%s/performance/\"%(i+1) + \"model_%s.csv\"%n)\n",
    "            cold_preds_mus.append(y_pred1)\n",
    "            np.save(\"a375/results/\"+ \"fold_%s/cold/mu/\"%(i+1) + \"cold_mu_%s.npy\"%n, y_pred1)\n",
    "            cold_preds_sigmas.append(y_pred2)\n",
    "            np.save(\"a375/results/\" + \"fold_%s/cold/sigma/\"%(i+1) + \"cold_sigma_%s.npy\"%n, y_pred2)\n",
    "            n=n+1\n",
    "    \n",
    "    mu_star=np.mean(cold_preds_mus,axis=0)\n",
    "    sigma_star = np.sqrt(np.mean(cold_preds_sigmas + np.square(cold_preds_mus), axis = 0) - np.square(mu_star))\n",
    "    cv_star = sigma_star/mu_star\n",
    "    if (len(mu_star[np.where(mu_star <= 0.2)])>0):\n",
    "        get_fold = model_evaluate(mu_star,Y_cold,0.2,df_cold)\n",
    "        get_fold.to_csv(\"a375/results/\" + \"fold_%s/ensemble_performance.csv\"%(i+1))\n",
    "    df_cold['mu'] = mu_star\n",
    "    df_cold['cv'] = cv_star\n",
    "    df_cold.to_csv(\"a375/results/\" + \"fold_%s/ensemble_preds_dataframe.csv\"%(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_preds_mus = []\n",
    "cold_preds_sigmas = []\n",
    "for n in range(10):\n",
    "    cold_preds_mus.append(np.load(\"a375/results/fold_5/cold/mu/cold_mu_%s.npy\"%n))\n",
    "    cold_preds_sigmas.append(np.load(\"a375/results/fold_5/cold/sigma/cold_sigma_%s.npy\"%n))\n",
    "\n",
    "mu_star=np.mean(cold_preds_mus,axis=0)\n",
    "sigma_star = np.sqrt(np.mean(cold_preds_sigmas + np.square(cold_preds_mus), axis = 0) - np.square(mu_star))\n",
    "cv_star = sigma_star/mu_star\n",
    "if (len(mu_star[np.where(mu_star <= 0.2)])>0):\n",
    "    get_fold = model_evaluate(mu_star,Y_cold,0.2,df_cold)\n",
    "    get_fold.to_csv(\"a375/results/fold_5/ensemble_performance.csv\")\n",
    "    df_cold['mu'] = mu_star\n",
    "    df_cold['cv'] = cv_star\n",
    "    df_cold.to_csv(\"a375/results/fold_5/ensemble_preds_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fold_all={'cor' : [], 'mse_all' : [], 'mse_similars' : [],'precision': [], 'accuracy': [],'positives' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    get=pd.read_csv(\"a375/results/fold_%s/ensemble_performance.csv\"%i,index_col=0)\n",
    "    get_fold_all['cor'].append(get['cor'][0])\n",
    "    get_fold_all['mse_all'].append(get['mse_all'][0])\n",
    "    get_fold_all['mse_similars'].append(get['mse_similars'][0])\n",
    "    get_fold_all['precision'].append(get['precision'][0])\n",
    "    get_fold_all['accuracy'].append(get['accuracy'][0])\n",
    "    get_fold_all['positives'].append(get['positives'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fold_all=pd.DataFrame(get_fold_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fold_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'cor' : get_fold_all['cor'].mean(), 'mse_all' : get_fold_all['mse_all'].mean(), 'mse_similars' : get_fold_all['mse_similars'].mean(),\n",
    "              'precision': get_fold_all['precision'].mean(), 'accuracy': get_fold_all['accuracy'].mean(),'positives' : ceil(get_fold_all['positives'].mean())}, index=[0]).to_csv(\"a375/results/cross_ensemble_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'cor' : get_fold_all['cor'].mean(), 'mse_all' : get_fold_all['mse_all'].mean(), 'mse_similars' : get_fold_all['mse_similars'].mean(),\n",
    "              'precision': get_fold_all['precision'].mean(), 'accuracy': get_fold_all['accuracy'].mean(),'positives' : ceil(get_fold_all['positives'].mean())}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net2json(df):\n",
    "    nodes=list(set(list(df['node1']) + list(df['node2'])))\n",
    "    #read pickle in the folder you are\n",
    "    #import pickle\n",
    "    #filep=open('ReSimNet-Dataset.pkl','rb')\n",
    "    #prot_enc=pickle.load(filep)\n",
    "    #filep.close()\n",
    "    if \"Perturbation\" in nodes:\n",
    "        nodes.remove(\"Perturbation\")\n",
    "    #nodes[nodes.index(\"Perturbation\")]=nodes[0]\n",
    "    #nodes[0]=\"Perturbation\"\n",
    "    feats={}\n",
    "    for j,x in enumerate(nodes):\n",
    "        if (x in list(df['node1'])):\n",
    "            activity=list(df['activity1'])[list(df['node1']).index(x)]\n",
    "        else:\n",
    "            activity=list(df['activity2'])[list(df['node2']).index(x)]\n",
    "        sign = lambda a: '-' if (int(a)==-1) else '+'\n",
    "        feats.update({\"%s\"%j:x+sign(activity)})\n",
    "    net={}\n",
    "    net.update({\"edges\":[]})\n",
    "    net.update({\"features\":feats})\n",
    "    for j in range(1,len(df)):\n",
    "        if ((df['node1'][j]!=\"Perturbation\") and (df['node2'][j]!=\"Perturbation\")):\n",
    "            ind1=nodes.index(df['node1'][j])\n",
    "            ind2=nodes.index(df['node2'][j])\n",
    "            net[\"edges\"].append([ind1,ind2])\n",
    "    return(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=pd.read_csv(\"file_info.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8UWqfPHWCGO"
   },
   "outputs": [],
   "source": [
    "#find how to copy file in python\n",
    "#from shutil import copyfile\n",
    "import json\n",
    "path_list=[]\n",
    "embed_id=[]\n",
    "prev_sig=[]\n",
    "counter=0\n",
    "for (i in range(len(info))):\n",
    "    if (info['sig_id'][i] not in prev_sig):\n",
    "        #copyfile(info['files_combined'][i], \"drive/My Drive/Computational_projects/deepsnem/graph2vec\")\n",
    "        path_list.append(info['files_combined'][i])\n",
    "        embed_id.append(info['emb'][i])\n",
    "        prev_sig.append(info['sig_id'][i])\n",
    "        counter+=1\n",
    "    if counter==7788:\n",
    "        break\n",
    "for (i,x in enumerate(path_list)):\n",
    "    df=pd.read_csv(x,index_col=0)\n",
    "    net=net2json(df)\n",
    "    with open('carn_data/'+embed_id[i]+'.json', 'w') as fp:\n",
    "        json.dump(net, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Xs9gqqI1p98c",
    "outputId": "635ef07b-df19-4ef1-ee9e-1be8f874ac6d"
   },
   "outputs": [],
   "source": [
    "def getMCS(G_source, G_new):\n",
    "    matching_graph=nx.Graph()\n",
    "\n",
    "    for n1,n2,attr in G_new.edges(data=True):\n",
    "        if G_source.has_edge(n1,n2) :\n",
    "            if G_source[n1][n2]['weight']==G_new[n1][n2]['weight']:\n",
    "                matching_graph.add_edge(n1,n2,weight=attr)\n",
    "\n",
    "    graphs = list(nx.connected_components(matching_graph))\n",
    "\n",
    "    mcs_length = 0\n",
    "    mcs_graph = nx.Graph()\n",
    "    for i, graph in enumerate(graphs):\n",
    "\n",
    "        if len(graph) > mcs_length:\n",
    "            mcs_length = len(graph)\n",
    "            mcs_graph = graph\n",
    "\n",
    "    return [mcs_length,mcs_graph]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "JeF02HWukEue",
    "kKckpFxZkEuh",
    "JRq-sQBqkEul",
    "Hvzz7L1dkEup",
    "-3edzyo7lrYF",
    "AMKuY0P_kEvD"
   ],
   "name": "UGRAPHEMP_encoder_att.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
