{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import functools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, roc_auc_score, \\\n",
    "    auc, average_precision_score, pairwise_distances\n",
    "import scikitplot as skplt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgb_hyper import objective\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import dill\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read ged embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = pd.read_csv(\"data/ged_unsupervised/ged_unsupervised_embeddings.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the training,val,test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"data/ged_unsupervised/sig_emb_df_ged.csv\",index_col=0)\n",
    "val1_df = pd.read_csv(\"data/gene_data/splits/csv/val_set_1.csv\",index_col=0)\n",
    "val2_df = pd.read_csv(\"data/gene_data/splits/csv/val_set_2.csv\",index_col=0)\n",
    "val3_df = pd.read_csv(\"data/gene_data/splits/csv/val_set_3.csv\",index_col=0)\n",
    "val4_df = pd.read_csv(\"data/gene_data/splits/csv/val_set_4.csv\",index_col=0)\n",
    "test_df = pd.read_csv(\"data/gene_data/splits/csv/test_set.csv\",index_col=0)\n",
    "\n",
    "valsets = [val1_df,val2_df,val3_df,val4_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only keep labels with more than 3 examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df[all_df[\"moa_count\"]>3]\n",
    "embs = embs.loc[all_df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# turn labels to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(all_df.moa_v1)\n",
    "all_df['moa_categorical'] = le.transform(all_df.moa_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first remove the test set from all_df and all genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embs = embs.loc[test_df[\"sig_id\"]]\n",
    "test_sigs = all_df.loc[test_df[\"sig_id\"]]\n",
    "all_df = all_df.drop(test_df[\"sig_id\"])\n",
    "embs = embs.drop(test_df[\"sig_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the space\n",
    "fspace = {\n",
    "    'colsample_bylevel' : hp.uniform('colsample_bylevel', 0.1, 1), #+\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.1, 1), #+\n",
    "    'gamma' : hp.uniform('gamma', 0.1, 1), #+\n",
    "    'learning_rate' : hp.uniform('learning_rate', 0.1, 1),\n",
    "    'max_delta_step' : hp.quniform('max_delta_step',1,10,1),\n",
    "    'max_depth' : hp.quniform('max_depth',6, 12, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight',10 ,500 ,5),\n",
    "    'reg_alpha' : hp.uniform('reg_alpha',0.1,100),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda',0.1,100),\n",
    "    'subsample' : hp.uniform('subsample',0.1,1.0),\n",
    "    'max_bin' : hp.quniform('max_bin',16,256,16)\n",
    "    # add sampling method,max bin,predicto,monotone_constraints,interaction_constraints,single_precision_histogram\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(fspace, all_df, embs, valsets):\n",
    "    accs = []\n",
    "    model_params = {\n",
    "        \"colsample_bylevel\" : fspace['colsample_bylevel'],\n",
    "        \"colsample_bytree\" : fspace['colsample_bytree'],\n",
    "        \"gamma\" : fspace['gamma'],\n",
    "        \"eta\" : fspace['learning_rate'],\n",
    "        \"max_delta_step\" : int(fspace['max_delta_step']),\n",
    "        \"max_depth\" : int(fspace['max_depth']),\n",
    "        \"min_child_weight\" : int(fspace['min_child_weight']),\n",
    "        \"alpha\" : fspace['reg_alpha'],\n",
    "        \"lambda\" : fspace['reg_lambda'],\n",
    "        \"subsample\" : fspace['subsample'],\n",
    "        \"objective\":'multi:softmax',\n",
    "        'num_class': all_df['moa_categorical'].nunique(),\n",
    "        \"booster\":'gbtree',\n",
    "        \"eval_metric\":'merror'\n",
    "        }\n",
    "    for i in range(len(valsets)):\n",
    "        val_embs = embs.loc[valsets[i][\"sig_id\"]]\n",
    "        val_sigs = all_df.loc[valsets[i][\"sig_id\"]]\n",
    "        train_embs = embs.drop(valsets[i][\"sig_id\"])\n",
    "        train_sigs = all_df.drop(valsets[i][\"sig_id\"])\n",
    "        dtrain = xgb.DMatrix(data=train_embs, label=train_sigs['moa_categorical'])\n",
    "        dtest = xgb.DMatrix(data=val_embs, label = val_sigs['moa_categorical'])\n",
    "        evalist = [(dtest,'eval'),(dtrain,'train')]\n",
    "        bst = xgb.train(model_params, dtrain, 100, evalist, early_stopping_rounds = 10, verbose_eval = False)\n",
    "        pred = bst.predict(dtest)\n",
    "        accs.append(accuracy_score(val_sigs['moa_categorical'], pred))\n",
    "    ave_acc = np.mean(accs,axis = 0)\n",
    "    return {'loss': -ave_acc ,  'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin_objective = partial(objective, all_df = all_df, embs=embs, valsets = valsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 1000  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"my_model.hyperopt\", \"rb\"))\n",
    "        print(\"Found saved Trials! Loading...\")\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn = fmin_objective, space = fspace, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    \n",
    "    # save the trials object\n",
    "    with open(\"my_model.hyperopt\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    return(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved Trials! Loading...\n",
      "Rerunning from 1 trials to 1001 (+1000) trials\n",
      "100%|█████████████████████████████████████████| 1001/1001 [8:05:20<00:00, 29.09s/trial, best loss: -0.3766691291901376]\n",
      "Best: {'colsample_bylevel': 0.7509667377368692, 'colsample_bytree': 0.6003359106806985, 'gamma': 0.5522788057237071, 'learning_rate': 0.34320925623500753, 'max_bin': 128.0, 'max_delta_step': 9.0, 'max_depth': 8.0, 'min_child_weight': 25.0, 'reg_alpha': 2.222253612124258, 'reg_lambda': 38.23268490863382, 'subsample': 0.9216804323381075}\n"
     ]
    }
   ],
   "source": [
    "trials = run_trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params = trials.trials[index]['misc']['vals']\n",
    "hyper_params = {\n",
    "        \"colsample_bylevel\" : 0.7509667377368692,\n",
    "        \"colsample_bytree\" : 0.6003359106806985,\n",
    "        \"gamma\" : 0.5522788057237071,\n",
    "        \"eta\" : 0.34320925623500753,\n",
    "        \"max_delta_step\" : 9,\n",
    "        \"max_depth\" : 8,\n",
    "        \"min_child_weight\" : 25,\n",
    "        \"alpha\" : 2.222253612124258,\n",
    "        \"lambda\" : 38.23268490863382,\n",
    "        \"subsample\" : 0.9216804323381075,\n",
    "        \"objective\":'multi:softmax',\n",
    "        'num_class': all_df['moa_categorical'].nunique(),\n",
    "        \"booster\":'gbtree',\n",
    "        \"eval_metric\":'merror'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for i in range(len(valsets)):\n",
    "    val_embs = embs.loc[valsets[i][\"sig_id\"]]\n",
    "    val_sigs = all_df.loc[valsets[i][\"sig_id\"]]\n",
    "    train_embs = embs.drop(valsets[i][\"sig_id\"])\n",
    "    train_sigs = all_df.drop(valsets[i][\"sig_id\"])\n",
    "    dtrain = xgb.DMatrix(data=train_embs, label=train_sigs['moa_categorical'])\n",
    "    dtest = xgb.DMatrix(data=val_embs, label = val_sigs['moa_categorical'])\n",
    "    evalist = [(dtest,'eval'),(dtrain,'train')]\n",
    "    bst = xgb.train(hyper_params, dtrain, 100, evalist, early_stopping_rounds = 10, verbose_eval = False)\n",
    "    pred = bst.predict(dtest)\n",
    "    accs.append(accuracy_score(val_sigs['moa_categorical'], pred))\n",
    "ave_acc = np.mean(accs,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4369747899159664,\n",
       " 0.2923076923076923,\n",
       " 0.40816326530612246,\n",
       " 0.36923076923076925]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2905982905982906\n"
     ]
    }
   ],
   "source": [
    "val_embs = embs.loc[valsets[0][\"sig_id\"]]\n",
    "val_sigs = all_df.loc[valsets[0][\"sig_id\"]]\n",
    "train_embs = embs.drop(valsets[0][\"sig_id\"])\n",
    "train_sigs = all_df.drop(valsets[0][\"sig_id\"])\n",
    "dtrain = xgb.DMatrix(data=train_embs, label=train_sigs['moa_categorical'])\n",
    "dval = xgb.DMatrix(data=val_embs, label = val_sigs['moa_categorical'])\n",
    "dtest = xgb.DMatrix(data=test_embs)\n",
    "evalist = [(dval,'eval'),(dtrain,'train')]\n",
    "bst = xgb.train(hyper_params, dtrain, 100, evalist, early_stopping_rounds = 10, verbose_eval = False)\n",
    "pred = bst.predict(dtest)\n",
    "print(accuracy_score(test_sigs['moa_categorical'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le.inverse_transform(pred.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_drug_acc(df):\n",
    "    unique_drugs = df['rdkit'].unique()\n",
    "    s = 0\n",
    "    for drug in unique_drugs:\n",
    "        filt = df[df['rdkit']==drug]\n",
    "        score = accuracy_score(filt['moa_v1'], filt['predicted'])\n",
    "        nunique_moa = filt['predicted'].nunique()\n",
    "        if score >= (1/nunique_moa):\n",
    "            s = s + 1\n",
    "    return(s/len(unique_drugs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sigs['predicted'] = le.inverse_transform(pred.astype(int))\n",
    "drug_acc = per_drug_acc(test_sigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
